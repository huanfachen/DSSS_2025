[
  {
    "objectID": "sessions/weekX_practical.html",
    "href": "sessions/weekX_practical.html",
    "title": "Practical XXX: XXX",
    "section": "",
    "text": "This week is focussed on XXX."
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-a-callout-note",
    "href": "sessions/weekX_practical.html#to-add-a-callout-note",
    "title": "Practical XXX: XXX",
    "section": "To add a callout-note",
    "text": "To add a callout-note\n\n\n\n\n\n\nNote\n\n\n\nSuggestions for a Better Learning Experience:\n\nXXX"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-python-code-without-running-them",
    "href": "sessions/weekX_practical.html#to-add-python-code-without-running-them",
    "title": "Practical XXX: XXX",
    "section": "To add Python code without running them …",
    "text": "To add Python code without running them …\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-and-run-python-code",
    "href": "sessions/weekX_practical.html#to-add-and-run-python-code",
    "title": "Practical XXX: XXX",
    "section": "To add and run Python code",
    "text": "To add and run Python code\n\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())\n\n   Area code          Area name Area type  Population 2011  Population 2021  \\\n0  K04000001  England and Wales  National       56075912.0       59597542.0   \n1  E92000001            England   Country       53012456.0       56490048.0   \n2  W92000004              Wales   Country        3063456.0        3107494.0   \n3  E12000001         North East    Region        2596886.0        2647013.0   \n4  E12000002         North West    Region        7052177.0        7417397.0   \n\n   Percentage change  \n0                6.3  \n1                6.6  \n2                1.4  \n3                1.9  \n4                5.2"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-a-photo---replace-the-path.-using-relative-path-is-also-okay.",
    "href": "sessions/weekX_practical.html#to-add-a-photo---replace-the-path.-using-relative-path-is-also-okay.",
    "title": "Practical XXX: XXX",
    "section": "To add a photo - replace the path. Using relative path is also okay.",
    "text": "To add a photo - replace the path. Using relative path is also okay."
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-some-questions",
    "href": "sessions/weekX_practical.html#to-add-some-questions",
    "title": "Practical XXX: XXX",
    "section": "To add some “questions”",
    "text": "To add some “questions”\nThe qmd file will be rendered as two files in sessions folder, including a html and ipynb format. The html file will contain both question and answer, while the ipynb file will contain only the question.\nFor the effect, please check HTML and ipynb.\n\nQuestionAnswerAnswer\n\n\nif ??\n    ??\nelse:\n    ??\n\n\n\n\n\nif 'Moscow' in ['Moscow', 'Beijing']:\n    print(\"Moscow is in the cities list.\")\nelse:\n    print(\"Moscow is not in the cities list.\")\nMoscow is in the cities list."
  },
  {
    "objectID": "sessions/weekX_practical.html#youre-done",
    "href": "sessions/weekX_practical.html#youre-done",
    "title": "Practical XXX: XXX",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the first QM practical session! If you are still working on it, take you time.\nDon’t worry about understanding every detail of the Python code — what matters most is knowing which functions to use for a specific task, like checking minimum and maximum values or generating boxplots, and knowing how to debug when it goes wrong. Remember, practice makes perfect."
  },
  {
    "objectID": "sessions/weekX.html",
    "href": "sessions/weekX.html",
    "title": "Week XXX",
    "section": "",
    "text": "This week will introduce XXX."
  },
  {
    "objectID": "sessions/weekX.html#introduction",
    "href": "sessions/weekX.html#introduction",
    "title": "Week XXX",
    "section": "",
    "text": "This week will introduce XXX."
  },
  {
    "objectID": "sessions/weekX.html#learning-objectives",
    "href": "sessions/weekX.html#learning-objectives",
    "title": "Week XXX",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nXXX\nXXX\nXXX"
  },
  {
    "objectID": "sessions/weekX.html#lecture",
    "href": "sessions/weekX.html#lecture",
    "title": "Week XXX",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture"
  },
  {
    "objectID": "sessions/weekX.html#quiz",
    "href": "sessions/weekX.html#quiz",
    "title": "Week XXX",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page."
  },
  {
    "objectID": "sessions/weekX.html#practical",
    "href": "sessions/weekX.html#practical",
    "title": "Week XXX",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload"
  },
  {
    "objectID": "sessions/weekX.html#further-resources",
    "href": "sessions/weekX.html#further-resources",
    "title": "Week XXX",
    "section": "Further resources",
    "text": "Further resources"
  },
  {
    "objectID": "sessions/week6.html",
    "href": "sessions/week6.html",
    "title": "Week 5",
    "section": "",
    "text": "This week, we’ll explore how to tell if several groups are truly different and how to measure the strength and direction of the relationship between two variables.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#introduction",
    "href": "sessions/week6.html#introduction",
    "title": "Week 5",
    "section": "",
    "text": "This week, we’ll explore how to tell if several groups are truly different and how to measure the strength and direction of the relationship between two variables.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#learning-objectives",
    "href": "sessions/week6.html#learning-objectives",
    "title": "Week 5",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand ANOVA.\nUnderstand correlation between two variables.\nUnderstand the difference of Pearson and Spearman correlation.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#lecture",
    "href": "sessions/week6.html#lecture",
    "title": "Week 5",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#quiz",
    "href": "sessions/week6.html#quiz",
    "title": "Week 5",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#practical",
    "href": "sessions/week6.html#practical",
    "title": "Week 5",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week6.html#further-resources",
    "href": "sessions/week6.html#further-resources",
    "title": "Week 5",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "6. Introduction to Regression"
    ]
  },
  {
    "objectID": "sessions/week4.html",
    "href": "sessions/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "This week will introduce neural networks.",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#introduction",
    "href": "sessions/week4.html#introduction",
    "title": "Week 4",
    "section": "",
    "text": "This week will introduce neural networks.",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#learning-objectives",
    "href": "sessions/week4.html#learning-objectives",
    "title": "Week 4",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the principle of neural networks.\nLearn how to construct and train a neural network.\nCan apply neural networks to supervised learning tasks.",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#lecture",
    "href": "sessions/week4.html#lecture",
    "title": "Week 4",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#quiz",
    "href": "sessions/week4.html#quiz",
    "title": "Week 4",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#practical",
    "href": "sessions/week4.html#practical",
    "title": "Week 4",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week4.html#further-resources",
    "href": "sessions/week4.html#further-resources",
    "title": "Week 4",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Basics",
      "4. Introduction to Linear Algebra"
    ]
  },
  {
    "objectID": "sessions/week2.html",
    "href": "sessions/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "This week will introduce framework and evaluation metrics of supervised learning.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#introduction",
    "href": "sessions/week2.html#introduction",
    "title": "Week 2",
    "section": "",
    "text": "This week will introduce framework and evaluation metrics of supervised learning.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#learning-objectives",
    "href": "sessions/week2.html#learning-objectives",
    "title": "Week 2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the framework of supervised learning.\nUnderstand the common metrics for regression and classification.\nAppreciate the accuracy paradox and its implications.\nKnow how to pick the right metric for your problem.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#lecture",
    "href": "sessions/week2.html#lecture",
    "title": "Week 2",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#quiz",
    "href": "sessions/week2.html#quiz",
    "title": "Week 2",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#practical",
    "href": "sessions/week2.html#practical",
    "title": "Week 2",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week2.html#further-resources",
    "href": "sessions/week2.html#further-resources",
    "title": "Week 2",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Basics",
      "2. Explanatory Data Analysis #2"
    ]
  },
  {
    "objectID": "sessions/week1_lecture.html#what-we-learnt-in-term-1",
    "href": "sessions/week1_lecture.html#what-we-learnt-in-term-1",
    "title": "Introduction to machine learning",
    "section": "What we learnt in Term 1",
    "text": "What we learnt in Term 1\n\nPython programming\nData types, visualisation\nRegression (Ordinary Least Square; Linear Mixed Effects; multicollinearity)\nDimensionality reduction\nClustering"
  },
  {
    "objectID": "sessions/week1_lecture.html#learning-objectives",
    "href": "sessions/week1_lecture.html#learning-objectives",
    "title": "Introduction to machine learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand the basics and classifications of machine learning\nUnderstand the differences between statistical methods and machine learning (estimation vs. prediction)\nAppreciate GIGO theorems in machine learning and data science"
  },
  {
    "objectID": "sessions/week1_lecture.html#ml-as-subset-of-ai",
    "href": "sessions/week1_lecture.html#ml-as-subset-of-ai",
    "title": "Introduction to machine learning",
    "section": "ML as subset of AI",
    "text": "ML as subset of AI\n\n\n\nMachine learning (decision tree, random forest, k-means, etc.)\n\nDeep learning (deep neural networks)\n\nOther AI tools: graphical models, symbolic AI\n\nNote: we don’t distinguish ML/DL and consider NN as part of ML\n\n\n\n\n\nImage Credit: Lecture slide (ML is a subset of AI)"
  },
  {
    "objectID": "sessions/week1_lecture.html#definition-of-machine-learning",
    "href": "sessions/week1_lecture.html#definition-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Definition of machine learning",
    "text": "Definition of machine learning\n\nArthur Samuel (1959): (Machine learning is the) field of study that gives computers the ability to learn without being explicitly programmed.\n\n\nTom Mitchell (1997): A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."
  },
  {
    "objectID": "sessions/week1_lecture.html#what-is-machine-learning",
    "href": "sessions/week1_lecture.html#what-is-machine-learning",
    "title": "Introduction to machine learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nExtracting knowledge from data\nRelying on data and algorithms\nClosely related to statistics but distinct from linear models\nFocus on prediction rather than estimating relationships or interpretation"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-facebook",
    "href": "sessions/week1_lecture.html#examples-facebook",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook",
    "text": "Examples: Facebook\n\n\n\n\nMachine learning in news feed ranking\nContent selection and targeting\nAds and recommendations"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-facebook-cont.",
    "href": "sessions/week1_lecture.html#examples-facebook-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nFace detection and recognition\nPhoto organization"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-facebook-cont.-1",
    "href": "sessions/week1_lecture.html#examples-facebook-cont.-1",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nPhoto selection and layout"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-amazon",
    "href": "sessions/week1_lecture.html#examples-amazon",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon",
    "text": "Examples: Amazon\n\n\n\n\nProduct ranking\nPersonalised recommendations\nAds selection"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-amazon-cont.",
    "href": "sessions/week1_lecture.html#examples-amazon-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon (cont.)",
    "text": "Examples: Amazon (cont.)\n\n\n\n\nSeller selection\nDefault choices\nRelated products"
  },
  {
    "objectID": "sessions/week1_lecture.html#science-applications",
    "href": "sessions/week1_lecture.html#science-applications",
    "title": "Introduction to machine learning",
    "section": "Science Applications",
    "text": "Science Applications\n\n\n\n\nPersonalised cancer treatment\nMedical diagnosis\nDrug discovery\nHiggs boson discovery\nExoplanet detection"
  },
  {
    "objectID": "sessions/week1_lecture.html#types-of-machine-learning",
    "href": "sessions/week1_lecture.html#types-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nSupervised Learning: Learn from input-output pairs (with labelled data)\nUnsupervised Learning: Discover structure in data (without labelled data)\nReinforcement Learning: Learn through interaction with environment (with an actual/simulated environment)"
  },
  {
    "objectID": "sessions/week1_lecture.html#supervised-learning",
    "href": "sessions/week1_lecture.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in \\mathbb{R} \\\\\nf(x_i) &\\approx y_i\n\\end{aligned}\n\\]\nLearn a function \\(f\\) from input-output pairs to predict on new data."
  },
  {
    "objectID": "sessions/week1_lecture.html#generalisation-to-unseen-data",
    "href": "sessions/week1_lecture.html#generalisation-to-unseen-data",
    "title": "Introduction to machine learning",
    "section": "Generalisation to unseen data",
    "text": "Generalisation to unseen data\n\nGoal: \\(f(x_i) \\approx y_i\\) on training data\nMore important: \\(f(x) \\approx y\\) on new data\nCore distinction: not just function approximation, but prediction on unseen data\nAvoiding overfitting to training data is key"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-of-supervised-learning",
    "href": "sessions/week1_lecture.html#examples-of-supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Examples of Supervised Learning",
    "text": "Examples of Supervised Learning\n\nSpam detection\nMedical diagnosis\nAd click prediction"
  },
  {
    "objectID": "sessions/week1_lecture.html#unsupervised-learning",
    "href": "sessions/week1_lecture.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\\[\nx_i \\sim p(x) \\text{ i.i.d.}\n\\]\nLearn about the distribution \\(p\\):\n\nClustering\nDimensionality reduction\nTopic modeling\nOutlier detection"
  },
  {
    "objectID": "sessions/week1_lecture.html#other-types-of-learning",
    "href": "sessions/week1_lecture.html#other-types-of-learning",
    "title": "Introduction to machine learning",
    "section": "Other Types of Learning",
    "text": "Other Types of Learning\n\nSemi-supervised\nActive Learning\nForecasting\nTransfer learning\nIf you understand supervised/unsupervised/reinforcement learning, others are easier to grasp"
  },
  {
    "objectID": "sessions/week1_lecture.html#dont-read-a-book-by-its-name",
    "href": "sessions/week1_lecture.html#dont-read-a-book-by-its-name",
    "title": "Introduction to machine learning",
    "section": "Don’t read a book by its name",
    "text": "Don’t read a book by its name\n\nClassifications of neighbourhoods (e.g. London output area classification) that are actually clustering\nAnomaly detection methods can be unsupervised or supervised learning\nForecasting can be done with lagged features (via supervised learning), or with time series data (via time series analysis), or both"
  },
  {
    "objectID": "sessions/week1_lecture.html#what-does-llm-belong-to",
    "href": "sessions/week1_lecture.html#what-does-llm-belong-to",
    "title": "Introduction to machine learning",
    "section": "What does LLM belong to?",
    "text": "What does LLM belong to?\n\n\n\nImage Credit: medium.com"
  },
  {
    "objectID": "sessions/week1_lecture.html#classification-vs.-regression",
    "href": "sessions/week1_lecture.html#classification-vs.-regression",
    "title": "Introduction to machine learning",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression\n\n\nClassification\n\nTarget \\(y\\) is discrete\nExample: Is this patient sick?\n\n\nRegression\n\nTarget \\(y\\) is continuous\nExample: How long to recover?"
  },
  {
    "objectID": "sessions/week1_lecture.html#relationship-to-statistics",
    "href": "sessions/week1_lecture.html#relationship-to-statistics",
    "title": "Introduction to machine learning",
    "section": "Relationship to Statistics",
    "text": "Relationship to Statistics\n\n\nStatistics\n\nModel first\nEstimation emphasis\nYes/no questions\nWith many assumptions, need to test\nInterpretation is key\ne.g. Does smoking lead to lung cancer?\n\n\nMachine Learning\n\nData first\nPrediction emphasis\nFuture predictions\nFew assumptions (but not assumption-free)\nInterpretation isn’t primary\ne.g. Predict tomorrow’s weather"
  },
  {
    "objectID": "sessions/week1_lecture.html#following-statements-are-not-recommended",
    "href": "sessions/week1_lecture.html#following-statements-are-not-recommended",
    "title": "Introduction to machine learning",
    "section": "Following statements are not recommended",
    "text": "Following statements are not recommended\n\n“Linear regression is not needed anymore because of machine learning.”\n“Machine learning is just a fad; statistics is more important.”\n“Machine learning models are black boxes; we can’t interpret them at all.”\n“Machine learning models are as interpretable as linear models.”\n“Machine learning models have no assumptions.”"
  },
  {
    "objectID": "sessions/week1_lecture.html#guiding-principles-goal-considerations",
    "href": "sessions/week1_lecture.html#guiding-principles-goal-considerations",
    "title": "Introduction to machine learning",
    "section": "Guiding Principles: Goal Considerations",
    "text": "Guiding Principles: Goal Considerations\n\nDefine the goal clearly\nDefine how to measure success\nThink about context and baseline\nAsk: what’s the benefit?"
  },
  {
    "objectID": "sessions/week1_lecture.html#thinking-in-context",
    "href": "sessions/week1_lecture.html#thinking-in-context",
    "title": "Introduction to machine learning",
    "section": "Thinking in Context",
    "text": "Thinking in Context\n\nWhat do you want to achieve?\nWhat’s the easiest way?\nWhat improvement over baseline do you need?"
  },
  {
    "objectID": "sessions/week1_lecture.html#good-and-bad-substitutes",
    "href": "sessions/week1_lecture.html#good-and-bad-substitutes",
    "title": "Introduction to machine learning",
    "section": "Good and Bad Substitutes",
    "text": "Good and Bad Substitutes\n\nChoose metrics carefully\nSubstitute metrics can be misleading\nOptimize for the right goal\nUnderstand side effects"
  },
  {
    "objectID": "sessions/week1_lecture.html#communicating-results",
    "href": "sessions/week1_lecture.html#communicating-results",
    "title": "Introduction to machine learning",
    "section": "Communicating Results",
    "text": "Communicating Results\n\nExplain why your approach works\nCommunicate uncertainty\nShow impact and limitations\nConvince stakeholders"
  },
  {
    "objectID": "sessions/week1_lecture.html#explainable-results",
    "href": "sessions/week1_lecture.html#explainable-results",
    "title": "Introduction to machine learning",
    "section": "Explainable Results",
    "text": "Explainable Results\n\n\n\n\nUsers want to know why recommendations are made\nExplainability improves engagement\nImportant for trust and transparency"
  },
  {
    "objectID": "sessions/week1_lecture.html#ethical-considerations",
    "href": "sessions/week1_lecture.html#ethical-considerations",
    "title": "Introduction to machine learning",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\n\n\n\nBias in risk assessments\nFairness in automated decisions\nTransparency and accountability"
  },
  {
    "objectID": "sessions/week1_lecture.html#ethics-its-in-the-application",
    "href": "sessions/week1_lecture.html#ethics-its-in-the-application",
    "title": "Introduction to machine learning",
    "section": "Ethics: It’s in the Application!",
    "text": "Ethics: It’s in the Application!\n\nUnderstand biases in your system\nConsider the impact of predictions\nUse algorithms responsibly\nSame algorithm, different outcomes"
  },
  {
    "objectID": "sessions/week1_lecture.html#data-and-data-collection",
    "href": "sessions/week1_lecture.html#data-and-data-collection",
    "title": "Introduction to machine learning",
    "section": "Data and Data Collection",
    "text": "Data and Data Collection\n\nCritical component of ML\nMore data usually helps (if from right source)\nConsider marginal cost vs. marginal benefit"
  },
  {
    "objectID": "sessions/week1_lecture.html#free-vs-expensive-data",
    "href": "sessions/week1_lecture.html#free-vs-expensive-data",
    "title": "Introduction to machine learning",
    "section": "Free vs Expensive Data",
    "text": "Free vs Expensive Data\n\n\nFree Data\n\nOpen data from gov and census, often aggregated (e.g. ONS, London Fire Brigade, NASA)\nOpen data from companies (e.g. Google Street View) (Read the license first)\nSynthetic data\nWeb scraping (be careful with legality and ethics)\n\n\nExpensive Data\n\nIndividual data (e.g. health records)\nMobile phone data (high cost)\nUser survey\nHigh-resolution imagery (e.g. satellite, aerial, drone)"
  },
  {
    "objectID": "sessions/week1_lecture.html#big-data-considerations",
    "href": "sessions/week1_lecture.html#big-data-considerations",
    "title": "Introduction to machine learning",
    "section": "Big Data Considerations",
    "text": "Big Data Considerations\n\nMore data can be more expensive to work with\nSubsample to RAM when possible (512GB available in cloud)\nRuntime and analyst time matter\nAlways try with a small sample first"
  },
  {
    "objectID": "sessions/week1_lecture.html#theorem-garbage-in-garbage-out-gigo",
    "href": "sessions/week1_lecture.html#theorem-garbage-in-garbage-out-gigo",
    "title": "Introduction to machine learning",
    "section": "Theorem: Garbage in, garbage out (GIGO)",
    "text": "Theorem: Garbage in, garbage out (GIGO)\n\nGreat algorithms + bad data = bad results\n\n\n\n\nModel performance is constrained by data quality.\nBiased, noisy, or incomplete data leads to misleading predictions.\n\n\n\n\n\nImage Credit: x.com/xschelling/status/954936528555429888"
  },
  {
    "objectID": "sessions/week1_lecture.html#good-data-large-size-high-quality.",
    "href": "sessions/week1_lecture.html#good-data-large-size-high-quality.",
    "title": "Introduction to machine learning",
    "section": "Good data = large size + high quality.",
    "text": "Good data = large size + high quality.\n\n\n\nSufficient sample size to capture variability in the problem\n\nHigh-quality labels and accurate measurements.\n\nRepresentative of the population and application context.\n\n\n\n\n\nImage Credit: Internet"
  },
  {
    "objectID": "sessions/week1_lecture.html#data-size-and-performance",
    "href": "sessions/week1_lecture.html#data-size-and-performance",
    "title": "Introduction to machine learning",
    "section": "Data size and performance",
    "text": "Data size and performance\n\n\nThe performance of ML/DL increases rapidly with the size of the data:\n\nLarge neural nets benefit the most from big data.\n\nMedium and small neural nets also improve with more data.\n\nTraditional ML algorithms (e.g. random forest, SVM) may saturate earlier.\n\n\n\n\n\nPerformance vs data size for ML/DL models"
  },
  {
    "objectID": "sessions/week1_lecture.html#feature-engineering-eda-90-time",
    "href": "sessions/week1_lecture.html#feature-engineering-eda-90-time",
    "title": "Introduction to machine learning",
    "section": "Feature engineering & EDA (~90% time)",
    "text": "Feature engineering & EDA (~90% time)\n\nMostly manual rather than automated (no automated methods for EDA)\nDomain knowledge from expertise is desirable\nUsing visualisation to identify patterns & data relationship\nRemoving noisy or erroneous data\n\nDealing with missing data\n\nGenerating new features by combining existing ones"
  },
  {
    "objectID": "sessions/week1_lecture.html#example-representation-of-geospatial-locations-in-ml-models",
    "href": "sessions/week1_lecture.html#example-representation-of-geospatial-locations-in-ml-models",
    "title": "Introduction to machine learning",
    "section": "Example: Representation of geospatial locations in ML models",
    "text": "Example: Representation of geospatial locations in ML models\n\nRaw coordinates (e.g. long/lat) may not be directly useful\nNeed to engineer features that capture spatial relationships\nLong/lat\n\nDistance to POIs (train stations/schools).\n\nUsing adjacency matrix between spatial units"
  },
  {
    "objectID": "sessions/week1.html",
    "href": "sessions/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "This week will introduce definition and types of machine learning.",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#introduction",
    "href": "sessions/week1.html#introduction",
    "title": "Week 1",
    "section": "",
    "text": "This week will introduce definition and types of machine learning.",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#learning-objectives",
    "href": "sessions/week1.html#learning-objectives",
    "title": "Week 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand three primary types for machine learning.\nAppreciate Garbage in, Garbage out theorem.",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#lecture",
    "href": "sessions/week1.html#lecture",
    "title": "Week 1",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#quiz",
    "href": "sessions/week1.html#quiz",
    "title": "Week 1",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#practical",
    "href": "sessions/week1.html#practical",
    "title": "Week 1",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/week1.html#further-resources",
    "href": "sessions/week1.html#further-resources",
    "title": "Week 1",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Basics",
      "1. Explanatory Data Analysis #1"
    ]
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#framework",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#framework",
    "title": "Supervised learning",
    "section": "Framework",
    "text": "Framework\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in\n\\begin{cases}\n\\mathbb{R}, & \\text{(regression)} \\\\\n\\mathcal{Y} \\text{ (finite set)}, & \\text{(classification)}\n\\end{cases} \\\\\n\\text{learn } f(x_i) &\\approx y_i \\\\\n\\text{such that } f(x) &\\approx y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#regression-vs.-classification",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#regression-vs.-classification",
    "title": "Supervised learning",
    "section": "Regression vs. classification",
    "text": "Regression vs. classification\n\n\n\n\n\n\n\n\n\nRegression\nClassification\n\n\n\n\nTarget variable\nContinuous value (y )\nDiscrete label (y ) (finite set)\n\n\nTask\nPredict “how much” / “how many”\nPredict “which class” / “which category”\n\n\nIntuition\nFind a ‘line’ close to all points\nFind a ‘boundary’ between classis\n\n\nExample\nPredicting house prices from features\nPredicting spam vs. not spam for an email"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#example---linear-regression",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#example---linear-regression",
    "title": "Supervised learning",
    "section": "Example - linear regression",
    "text": "Example - linear regression\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndata = fetch_california_housing()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.2f}\")\nprint(f\"Intercept: {model.intercept_:.2f}\")\n\nMedInc: 0.45\nHouseAge: 0.01\nAveRooms: -0.12\nAveBedrms: 0.78\nPopulation: -0.00\nAveOccup: -0.00\nLatitude: -0.42\nLongitude: -0.43\nIntercept: -37.02"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#terms",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#terms",
    "title": "Supervised learning",
    "section": "Terms",
    "text": "Terms\n\n\n\n\nTerm\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\nAlgorithm\n\n\nprocedure that runs on data to create a model\n\n\nlinear regression\n\n\n\n\nModel\n\n\nan output by algorithm and data\n\n\n\\(\\hat{y}_i = \\sum_k \\beta_k x_{ik} + \\beta_0\\)\n\n\n\n\nMetric\n\n\nto evalute the model performace\n\n\nSquared error\n\n\n\n\nHyperparameter\n\n\nalgorithm settings, predefined by user instead of learned from data\n\n\nNone\n\n\n\n\nParameter\n\n\nmodel components learned from data\n\n\ncoefficients & intercept\n\n\n\n\nModel training\n\n\nprocess of estimating parameters from data\n\n\nusing maximum likelihood estimation"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#classification",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#classification",
    "title": "Supervised learning",
    "section": "Classification",
    "text": "Classification\n\nMost classification algorithms follow two steps:\n\n\npredict probabilities for each class\nassign class with highest probability\n\n\nWe can talk about common challenges for supervised learning (despite their differences)"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#common-challenges",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#common-challenges",
    "title": "Supervised learning",
    "section": "Common Challenges",
    "text": "Common Challenges\n\nTo select evaluation metrics (so that the model solves the right problem)\nTo design workflow (so that the model generalises well and avoids overfitting)"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression",
    "title": "Supervised learning",
    "section": "Metrics for regression",
    "text": "Metrics for regression\n\n\n\n\n\n\n\nformula\n\n\nunit\n\n\nnotes\n\n\n\n\n\n\nRMSE\n\n\n\\(\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\)\n\n\nSame unit as target \\(y\\)\n\n\nPenalises large errors more; sensitive to outliers\n\n\n\n\nR²\n\n\n\\(1 - \\dfrac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\)\n\n\nDimensionless (between 0 and 1 for most cases)\n\n\nMeasures proportion of variance explained by the model; can be negative if model is worse than \\(y_i=\\bar{y}\\)\n\n\n\n\nMAE\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^{n}\\lvert y_i - \\hat{y}_i\\rvert\\)\n\n\nSame unit as target \\(y\\)\n\n\nMore robust to outliers than RMSE; interpretable as average absolute error\n\n\n\n\nMAPE\n\n\n\\(\\frac{100}{n}\\sum_{i=1}^{n}\\left\\lvert \\frac{y_i - \\hat{y}_i}{y_i} \\right\\rvert\\)\n\n\nPercent (%)\n\n\nsensitive to very small \\(y_i\\); relative error"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression-1",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression-1",
    "title": "Supervised learning",
    "section": "Metrics for regression",
    "text": "Metrics for regression\n\nUsing RMSE or squared error in most cases\nBy default, regressors in sklearn and XGBoost use Squared Error as loss function\nDifference between R2 for OLS (within [0,1]) vs. R2 for regression (&lt;=1, can be negative)"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-classification-binary",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-classification-binary",
    "title": "Supervised learning",
    "section": "Metrics for classification (binary)",
    "text": "Metrics for classification (binary)\n\n\n\nImage Credit: COMS4995-s20\n\n\n\\(Accuracy=\\frac{TP+TN}{TP+TN+FP+FN}\\)\n\nSelecting Positive label: often the minority class"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#example-using-breast-cancer-dataset",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#example-using-breast-cancer-dataset",
    "title": "Supervised learning",
    "section": "Example using Breast Cancer dataset",
    "text": "Example using Breast Cancer dataset\n\n\nclass malignant: 212, 37.258%\nclass benign: 357, 62.742%\nPredictive accuracy: 0.930"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#limitation-of-accuracy-accuracy-paradox",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#limitation-of-accuracy-accuracy-paradox",
    "title": "Supervised learning",
    "section": "Limitation of accuracy (Accuracy paradox)",
    "text": "Limitation of accuracy (Accuracy paradox)\n\nScenario: Data with 90% negatives (imbalanced data)\nA majority strategy that predicts all as negative gets 90% accuracy, but this is useless.\nDifferent models can have the same accuracy (0.9) but make very different types of errors.\n\ny_pred_1: Predicts all negative (90 TN, 0 TP)\ny_pred_2: Predicts some positives correctly but misses others\ny_pred_3: A mix of errors"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#precision-recall-f1-score-auc",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#precision-recall-f1-score-auc",
    "title": "Supervised learning",
    "section": "Precision, Recall, F1-score, AUC",
    "text": "Precision, Recall, F1-score, AUC\n\nPrecision (Positive Predicted Value): \\(\\frac{TP}{TP+FP}\\). Among predicted positives, how many are actually positive.\nRecall (Sensitivity, True Positive Rate): \\(\\frac{TP}{TP+FN}\\). Among actual positives, how many are correctly predicted.\nF1-score (Harmonic mean of precision & recall): \\(F=2\\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision}+\\text{recall}}\\)"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#addressing-accuracy-paradox",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#addressing-accuracy-paradox",
    "title": "Supervised learning",
    "section": "Addressing accuracy paradox",
    "text": "Addressing accuracy paradox"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#trade-off-between-precision-and-recall",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#trade-off-between-precision-and-recall",
    "title": "Supervised learning",
    "section": "Trade-off between precision and recall",
    "text": "Trade-off between precision and recall\n\nWhen precision increases, recall decreases. Vice versa.\nBy changing classification threshold (default=0.5, from 0 to 1), we can see the balance between precision and recall.\n\n\n\n\n\n\nPrecision–Recall curve for LR and Random Forest"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#roc-curve",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#roc-curve",
    "title": "Supervised learning",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nReceiver Operating Characteristic (ROC) curve plots True Positive Rate vs. False Positive Rate. Similar to precision-recall curve.\nThe identity line y=x represents random classifier (e.g. tossing a coin)."
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#auc-area-under-roc-curve",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#auc-area-under-roc-curve",
    "title": "Supervised learning",
    "section": "AUC (Area under ROC Curve)",
    "text": "AUC (Area under ROC Curve)\n\nThe integral of the ROC curve (or the area size under the curve)\nAUC is always 0.5 for random predictions\nMaximum AUC is 1.0 for perfect predictions"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#aggregating-metrics-across-classes",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#aggregating-metrics-across-classes",
    "title": "Supervised learning",
    "section": "Aggregating metrics across classes",
    "text": "Aggregating metrics across classes\n\nMacro average: \\(\\frac{1}{|L|}\\sum_{l\\in L}R(y_{l},\\hat{y}_{l})\\)\nUnweighted mean of per-class scores. Each class contributes equally, regardless of sample size.\nMicro average: \\(\\frac{1}{n}\\sum_{i=1}^{n}R(y_{i},\\hat{y}_{i})\\)\nSum individual TP, FP, FN, TN across all classes, then compute metric. Equivalent to accuracy for multiclass.\nWeighted average: \\(\\frac{1}{n}\\sum_{l\\in L}n_{l}R(y_{l},\\hat{y}_{l})\\)\nWeighted by support (number of samples in each class). Balances class sizes in the final score."
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#classification-report",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#classification-report",
    "title": "Supervised learning",
    "section": "Classification report",
    "text": "Classification report\n\nprint(classification_report(y_test, rf.predict(X_test), target_names=data.target_names))\n\n              precision    recall  f1-score   support\n\n   malignant       0.92      0.92      0.92        53\n      benign       0.96      0.96      0.96        90\n\n    accuracy                           0.94       143\n   macro avg       0.94      0.94      0.94       143\nweighted avg       0.94      0.94      0.94       143"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#picking-a-metric",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#picking-a-metric",
    "title": "Supervised learning",
    "section": "Picking a metric",
    "text": "Picking a metric\n\nReal-world problems are rarely balanced.\nAccuracy is rarely what you want.\nFind the right criterion for the specific task.\nDecide between emphasis on recall or precision.\nIdentify which classes are important."
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metric-for-breast-cancer-detection",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metric-for-breast-cancer-detection",
    "title": "Supervised learning",
    "section": "Metric for breast cancer detection",
    "text": "Metric for breast cancer detection\n\n“1” indicates malignant/cancer, “0” indicates benign/no cancer.\nMissing a cancer (FN) is much worse than a false alarm (FP)\nSo, we care more about recall than precision or accuracy.\nA model with high recall is preferred, even if it has lower precision."
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#next-question-how-can-i-optimise-for-a-specific-metric",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#next-question-how-can-i-optimise-for-a-specific-metric",
    "title": "Supervised learning",
    "section": "Next question: how can I optimise for a specific metric?",
    "text": "Next question: how can I optimise for a specific metric?\n\nRandomForestClassifier (and other classifiers) in sklearn by default optimises for accuracy and have no direct way to optimise for recall.\nWe have workarounds (topics for next week)\n\nUse recall metric during hyperparameter tuning and cross-validation\nAdjust classification threshold after training\nUse class weights to penalise misclassifications of the positive class more heavily during training"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#generalisation-to-multi-class",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#generalisation-to-multi-class",
    "title": "Supervised learning",
    "section": "Generalisation to multi-class",
    "text": "Generalisation to multi-class\n\nMost metrics can be generalised to multi-class using macro, micro, or weighted averaging.\nROC curve and AUC can be computed using one-vs-rest approach for each class and then averaged."
  },
  {
    "objectID": "sessions/index.html",
    "href": "sessions/index.html",
    "title": "Overview",
    "section": "",
    "text": "Understand the structure and focus of the module.\nDevelop a method for tackling quantitative problems.\nFormulate a research question and structure quantitative writing.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#objectives",
    "href": "sessions/index.html#objectives",
    "title": "Overview",
    "section": "",
    "text": "Understand the structure and focus of the module.\nDevelop a method for tackling quantitative problems.\nFormulate a research question and structure quantitative writing.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#why-study-a-quantitative-methods-course",
    "href": "sessions/index.html#why-study-a-quantitative-methods-course",
    "title": "Overview",
    "section": "Why study a Quantitative Methods course?",
    "text": "Why study a Quantitative Methods course?\n\nCoding alone is not enough.\nUnderstanding models aids in correct tool selection and bug handling.\nGoogle/ChatGPT can make mistakes; detecting them is crucial.\nMathematics understanding is not always required.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#course-objectives",
    "href": "sessions/index.html#course-objectives",
    "title": "Overview",
    "section": "Course Objectives",
    "text": "Course Objectives\n\nUnderstand a broad range of quantitative techniques.\nApply these skills in research.\nFormulate a coherent quantitative argument.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#prerequisites",
    "href": "sessions/index.html#prerequisites",
    "title": "Overview",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nNo prerequisite of university-level maths/statistics.\nNo prerequisite programming, but this module doesn’t teach programming.\nCASA0013 is strongly recommneded if you don’t know Python before.\nPython is required for practicals and assessments.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#course-structure",
    "href": "sessions/index.html#course-structure",
    "title": "Overview",
    "section": "Course Structure",
    "text": "Course Structure\n\nLectures: 10 weeks (Wednesdays 9:00–10:30).\nTutorials: 10 weeks (Wednesdays 10:30–12:00).",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#platforms",
    "href": "sessions/index.html#platforms",
    "title": "Overview",
    "section": "Platforms",
    "text": "Platforms\n\nEmail for important notices and private questions.\nGithub & website for lecture notes and notebooks.\nMoodle for lectures recording and assessments.\nSlack for public questions.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#weekly-schedule",
    "href": "sessions/index.html#weekly-schedule",
    "title": "Overview",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\n\n\nSession\nTopic\nLecturer\n\n\n\n\n1\nIntroduction to data\nHuanfa\n\n\n2\nProbability and distribution\nBea\n\n\n3\nHypothesis testing\nBea\n\n\n4\nIntroduction to linear algebra\nBea\n\n\n5\nCorrelation and regression\nHuanfa\n\n\n6\nMultiple regression\nAdam\n\n\n7\nGeneralised linear model\nAdam\n\n\n8\nMultilevel regression\nAdam\n\n\n9\nDimensionality reduction\nHuanfa\n\n\n10\nClustering Analysis\nHuanfa",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#attendance-recording",
    "href": "sessions/index.html#attendance-recording",
    "title": "Overview",
    "section": "Attendance Recording",
    "text": "Attendance Recording\n\n70% attendance required for student visa holders.\nPlease attend all lectures and workshops.\nContact module lead & bartlett.pg-casa@ucl.ac.uk, if you can’t attend.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#assessment-1",
    "href": "sessions/index.html#assessment-1",
    "title": "Overview",
    "section": "Assessment",
    "text": "Assessment\n\nWritten Investigation (summative): 100%\nWeekly quiz (formative)",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#ucl-assessment-policy",
    "href": "sessions/index.html#ucl-assessment-policy",
    "title": "Overview",
    "section": "UCL Assessment Policy",
    "text": "UCL Assessment Policy\n\nAll submissions via Moodle, not emails.\nLate penalties: Up to 48h (-10 points); up to 7 days (capped at 50); over 7 days (scores 0).\nDAP or Extenuating circumstances: to submit on Portico.\nRespect word count limits\nAvoid plagiarism and fake references",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#moodle-feedback",
    "href": "sessions/index.html#moodle-feedback",
    "title": "Overview",
    "section": "Moodle Feedback",
    "text": "Moodle Feedback\nPlease provide anonymous feedback on Moodle for every week.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#github-feedback",
    "href": "sessions/index.html#github-feedback",
    "title": "Overview",
    "section": "Github Feedback",
    "text": "Github Feedback\nYou can also give feedback on Github issues.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#history",
    "href": "sessions/W05_neural_networks/neural_networks.html#history",
    "title": "Neural Networks",
    "section": "History",
    "text": "History\n\nNearly everything we talk about today existed ~1990\nWhat changed?\n\nMore data\nFaster computers (GPUs)\nSome improvements: relu, dropout, adam, batch-normalization, residual networks"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#logistic-regression-as-neural-net",
    "href": "sessions/W05_neural_networks/neural_networks.html#logistic-regression-as-neural-net",
    "title": "Neural Networks",
    "section": "Logistic Regression as Neural Net",
    "text": "Logistic Regression as Neural Net"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#basic-architecture",
    "href": "sessions/W05_neural_networks/neural_networks.html#basic-architecture",
    "title": "Neural Networks",
    "section": "Basic Architecture",
    "text": "Basic Architecture\n\n\n\n\\(h(x) = f(W_1x+b_1)\\)\n\\(o(x) = g(W_2h(x) + b_2)\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#more-layers",
    "href": "sessions/W05_neural_networks/neural_networks.html#more-layers",
    "title": "Neural Networks",
    "section": "More Layers",
    "text": "More Layers\n\n\n\n\nHidden layers usually all have the same non-linear function\nMany layers → “deep learning”\nMultilayer perceptron, feed-forward neural network, vanilla feed-forward neural network\nRegression: single output neuron with linear activation\nClassification: one-hot-encoding of classes, n_classes output variables with softmax"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#nonlinear-activation-functions",
    "href": "sessions/W05_neural_networks/neural_networks.html#nonlinear-activation-functions",
    "title": "Neural Networks",
    "section": "Nonlinear Activation Functions",
    "text": "Nonlinear Activation Functions\n\n\n\n\nStandard choices: tanh or rectified linear unit (relu)\nTanh squashes between -1 and 1; saturates towards infinities\nReLU is constant zero for negative numbers, then identity"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#supervised-neural-networks",
    "href": "sessions/W05_neural_networks/neural_networks.html#supervised-neural-networks",
    "title": "Neural Networks",
    "section": "Supervised Neural Networks",
    "text": "Supervised Neural Networks\n\nNon-linear models for classification and regression\nWork well for very large datasets\nNon-convex optimization\nNotoriously slow to train – need for GPUs\nUse dot products; require preprocessing similar to SVM or linear models, unlike trees\nMany variants: Convolutional nets, GRUs, LSTMs, recursive networks, VAEs, GANs, deep RL"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#training-objective",
    "href": "sessions/W05_neural_networks/neural_networks.html#training-objective",
    "title": "Neural Networks",
    "section": "Training Objective",
    "text": "Training Objective\n\\(h(x) = f(W_1x+b_1)\\)\n\\(o(x) = g(W_2h(x)+b_2) = g(W_2f(W_1x + b_1) + b_2)\\)\n\\(\\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,o(x_i))\\)\n\\(= \\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,g(W_2f(W_1x+b_1)+b_2))\\)\n\n\\(l\\) = Squared loss for regression; Cross-entropy loss for classification"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#backpropagation",
    "href": "sessions/W05_neural_networks/neural_networks.html#backpropagation",
    "title": "Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nNeed \\(\\frac{\\partial l(y, o)}{\\partial W_i}\\) and \\(\\frac{\\partial l(y, o)}{\\partial b_i}\\)\n\n\\(\\text{net}(x) := W_1x + b_1\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#gradient-computation",
    "href": "sessions/W05_neural_networks/neural_networks.html#gradient-computation",
    "title": "Neural Networks",
    "section": "Gradient Computation",
    "text": "Gradient Computation\n\nBackpropagation is clever application of chain rule for derivatives\nSingle backward pass from output to input computes derivatives\nNot an optimization algorithm, just a way to compute gradients"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#relu-differentiability",
    "href": "sessions/W05_neural_networks/neural_networks.html#relu-differentiability",
    "title": "Neural Networks",
    "section": "ReLU Differentiability",
    "text": "ReLU Differentiability\n\n\n\n\nReLU not differentiable at zero\nUse subgradient descent; any gradient below function works\nIn practice, never hit zero with floating point numbers"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#optimizing-w-b",
    "href": "sessions/W05_neural_networks/neural_networks.html#optimizing-w-b",
    "title": "Neural Networks",
    "section": "Optimizing W, b",
    "text": "Optimizing W, b\nBatch \\(W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=1}^N \\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)\nOnline/Stochastic \\(W_i \\leftarrow W_i - \\eta\\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)\nMinibatch \\(W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=k}^{k+m} \\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#learning-heuristics",
    "href": "sessions/W05_neural_networks/neural_networks.html#learning-heuristics",
    "title": "Neural Networks",
    "section": "Learning Heuristics",
    "text": "Learning Heuristics\n\nConstant \\(\\eta\\) not good\nCan decrease \\(\\eta\\) over time\nBetter: adaptive \\(\\eta\\) for each entry of \\(W_i\\)\nState-of-the-art: adam (with magic numbers)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#picking-optimization-algorithms",
    "href": "sessions/W05_neural_networks/neural_networks.html#picking-optimization-algorithms",
    "title": "Neural Networks",
    "section": "Picking Optimization Algorithms",
    "text": "Picking Optimization Algorithms\n\nSmall dataset: off the shelf like l-bfgs\nBig dataset: adam / rmsprop\nHave time & nerve: tune the schedule"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#neural-nets-with-sklearn",
    "href": "sessions/W05_neural_networks/neural_networks.html#neural-nets-with-sklearn",
    "title": "Neural Networks",
    "section": "Neural Nets with sklearn",
    "text": "Neural Nets with sklearn\n\n\n\nmlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\nprint(mlp.score(X_train, y_train))\nprint(mlp.score(X_test, y_test))\n\nDon’t use sklearn for anything but toy problems in neural nets"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#random-state",
    "href": "sessions/W05_neural_networks/neural_networks.html#random-state",
    "title": "Neural Networks",
    "section": "Random State",
    "text": "Random State\n\n\n\n\nNetwork is way over capacity and can overfit in many ways\nRegularization might make it less dependent on initialization"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#hidden-layer-size",
    "href": "sessions/W05_neural_networks/neural_networks.html#hidden-layer-size",
    "title": "Neural Networks",
    "section": "Hidden Layer Size",
    "text": "Hidden Layer Size\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,), random_state=10)\nmlp.fit(X_train, y_train)\n\n\n\n\nSingle hidden layer with 5 units\nEach unit corresponds to different part of decision boundary"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#multiple-hidden-layers",
    "href": "sessions/W05_neural_networks/neural_networks.html#multiple-hidden-layers",
    "title": "Neural Networks",
    "section": "Multiple Hidden Layers",
    "text": "Multiple Hidden Layers\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10), random_state=0)\nmlp.fit(X_train, y_train)\n\n\n\n\n3 hidden layers each of size 10\nMain way to control complexity"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#activation-functions",
    "href": "sessions/W05_neural_networks/neural_networks.html#activation-functions",
    "title": "Neural Networks",
    "section": "Activation Functions",
    "text": "Activation Functions\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10),\n                    activation='tanh', random_state=0)\nmlp.fit(X_train, y_train)\n\n\n\n\nUsing tanh gives smoother boundaries\nReLU doesn’t work as well with l-bfgs on small networks\nFor large networks, relu is preferred"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#regression",
    "href": "sessions/W05_neural_networks/neural_networks.html#regression",
    "title": "Neural Networks",
    "section": "Regression",
    "text": "Regression\n\n\n\nfrom sklearn.neural_network import MLPRegressor\nmlp_relu = MLPRegressor(solver=\\\"lbfgs\\\").fit(X, y)\nmlp_tanh = MLPRegressor(solver=\\\"lbfgs\\\", activation='tanh').fit(X, y)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#complexity-control",
    "href": "sessions/W05_neural_networks/neural_networks.html#complexity-control",
    "title": "Neural Networks",
    "section": "Complexity Control",
    "text": "Complexity Control\n\nNumber of parameters\nRegularization\nEarly Stopping\nDropout"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#grid-searching-neural-nets",
    "href": "sessions/W05_neural_networks/neural_networks.html#grid-searching-neural-nets",
    "title": "Neural Networks",
    "section": "Grid-Searching Neural Nets",
    "text": "Grid-Searching Neural Nets\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, stratify=data.target, random_state=0)\n\nfrom sklearn.model_selection import GridSearchCV\npipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\\\"lbfgs\\\", random_state=1))\nparam_grid = {'mlpclassifier__alpha': np.logspace(-3, 3, 7)}\ngrid = GridSearchCV(pipe, param_grid)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#searching-hidden-layer-sizes",
    "href": "sessions/W05_neural_networks/neural_networks.html#searching-hidden-layer-sizes",
    "title": "Neural Networks",
    "section": "Searching Hidden Layer Sizes",
    "text": "Searching Hidden Layer Sizes\nfrom sklearn.model_selection import GridSearchCV\npipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\\\"lbfgs\\\", random_state=1))\nparam_grid = {'mlpclassifier__hidden_layer_sizes':\n              [(10,), (50,), (100,), (500,), (10, 10), (50, 50), (100, 100), (500, 500)]}\ngrid = GridSearchCV(pipe, param_grid)\ngrid.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#write-your-own-neural-networks",
    "href": "sessions/W05_neural_networks/neural_networks.html#write-your-own-neural-networks",
    "title": "Neural Networks",
    "section": "Write Your Own Neural Networks",
    "text": "Write Your Own Neural Networks\nclass NeuralNetwork(object):\n    def __init__(self):\n        # initialize coefficients and biases\n        pass\n    def forward(self, x):\n        activation = x\n        for coef, bias in zip(self.coef_, self.bias_):\n            activation = self.nonlinearity(np.dot(activation, coef) + bias)\n        return activation\n    def backward(self, x):\n        # compute gradient of stuff in forward pass\n        pass"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#autodiff",
    "href": "sessions/W05_neural_networks/neural_networks.html#autodiff",
    "title": "Neural Networks",
    "section": "Autodiff",
    "text": "Autodiff\nclass array(object) :\n    \\\"\\\"\\\"Simple Array object that support autodiff.\\\"\\\"\\\"\n    def __init__(self, value, name=None):\n        self.value = value\n        if name:\n            self.grad = lambda g : {name : g}\n    def __add__(self, other):\n        assert isinstance(other, int)\n        ret = array(self.value + other)\n        ret.grad = lambda g : self.grad(g)\n        return ret\n    def __mul__(self, other):\n        assert isinstance(other, array)\n        ret = array(self.value * other.value)\n        def grad(g):\n            x = self.grad(g * other.value)\n            x.update(other.grad(g * self.value))\n            return x\n        ret.grad = grad\n        return ret"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#autodiff-example",
    "href": "sessions/W05_neural_networks/neural_networks.html#autodiff-example",
    "title": "Neural Networks",
    "section": "Autodiff Example",
    "text": "Autodiff Example\na = array(np.array([1, 2]), 'a')\nb = array(np.array([3, 4]), 'b')\nc = b * a\nd = c + 1\nprint(d.value)\nprint(d.grad(1))\n[4 9]\n{'b': array([1, 2]), 'a': array([3, 4])}\n\nAutomatic differentiation avoids writing gradients manually\nKeep track of computation while executing forward pass\nHard-code derivative for each operation (no symbolic differentiation)\nBuild computation graph automatically"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#gpu-support",
    "href": "sessions/W05_neural_networks/neural_networks.html#gpu-support",
    "title": "Neural Networks",
    "section": "GPU Support",
    "text": "GPU Support\n\n\n\n\nImportant limitation: GPUs have much less memory than RAM\nMemory copies between RAM and GPU are expensive"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#computation-graph",
    "href": "sessions/W05_neural_networks/neural_networks.html#computation-graph",
    "title": "Neural Networks",
    "section": "Computation Graph",
    "text": "Computation Graph\n\n\n\n\nStore different intermediate results depending on derivatives needed\nGiven limited GPU memory, important to know what to cache/discard\nHelps with visual debugging and understanding network structure"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#deep-learning-framework-requirements",
    "href": "sessions/W05_neural_networks/neural_networks.html#deep-learning-framework-requirements",
    "title": "Neural Networks",
    "section": "Deep Learning Framework Requirements",
    "text": "Deep Learning Framework Requirements\n\nAutodiff\nGPU support\nOptimization and inspection of computation graph\nOn-the-fly generation of computation graph (optional)\nDistribution over multiple GPUs and/or cluster (optional)\n\nCurrent choices: TensorFlow, PyTorch / Torch, Chainer"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#deep-learning-libraries",
    "href": "sessions/W05_neural_networks/neural_networks.html#deep-learning-libraries",
    "title": "Neural Networks",
    "section": "Deep Learning Libraries",
    "text": "Deep Learning Libraries\n\nKeras (TensorFlow, CNTK, Theano)\nPyTorch (torch)\nChainer (chainer)\nMXNet (MXNet)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#quick-look-at-tensorflow",
    "href": "sessions/W05_neural_networks/neural_networks.html#quick-look-at-tensorflow",
    "title": "Neural Networks",
    "section": "Quick Look at TensorFlow",
    "text": "Quick Look at TensorFlow\n\n\"Down to the metal\" - don’t use for everyday tasks\nThree steps for learning:\n\nBuild computation graph (using array operations and functions)\nCreate Optimizer (gradient descent, adam, etc.) attached to graph\nRun actual computation\n\nEager mode (default in TensorFlow 2.0): write imperative code directly"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#pytorch-example",
    "href": "sessions/W05_neural_networks/neural_networks.html#pytorch-example",
    "title": "Neural Networks",
    "section": "PyTorch Example",
    "text": "PyTorch Example\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n\nN = 100\nx = torch.randn(N, 1, device=device, dtype=dtype)\ny = torch.randn(N, 1, device=device, dtype=dtype)\nw = torch.randn(D_in, H, device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    y_pred = x.mm(w1)\n    loss = (y_pred - y).pow(2).sum().item()\n    loss.backward()\n    w1 -= learning_rate * w1.grad\n    w1.grad.zero_()"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#best-practices",
    "href": "sessions/W05_neural_networks/neural_networks.html#best-practices",
    "title": "Neural Networks",
    "section": "Best Practices",
    "text": "Best Practices\n\nDon’t go down to the metal (i.e. write low-level code) unless you have to!\nDon’t write TensorFlow, write Keras!\nDon’t write PyTorch, write pytorch.nn or FastAI (or Skorch or ignite)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#convolutional-neural-networks",
    "href": "sessions/W05_neural_networks/neural_networks.html#convolutional-neural-networks",
    "title": "Neural Networks",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#idea-behind-cnns",
    "href": "sessions/W05_neural_networks/neural_networks.html#idea-behind-cnns",
    "title": "Neural Networks",
    "section": "Idea Behind CNNs",
    "text": "Idea Behind CNNs\n\nTranslation invariance\nWeight sharing"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#definition-of-convolution",
    "href": "sessions/W05_neural_networks/neural_networks.html#definition-of-convolution",
    "title": "Neural Networks",
    "section": "Definition of Convolution",
    "text": "Definition of Convolution\n\\[(f*g)[n] = \\sum\\limits_{m=-\\infty}^\\infty f[m]g[n-m]\\]\n\\[= \\sum\\limits_{m=-\\infty}^\\infty f[n-m]g[m]\\]"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#d-example-gaussian-smoothing",
    "href": "sessions/W05_neural_networks/neural_networks.html#d-example-gaussian-smoothing",
    "title": "Neural Networks",
    "section": "1D Example: Gaussian Smoothing",
    "text": "1D Example: Gaussian Smoothing"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#convolutions-in-2d",
    "href": "sessions/W05_neural_networks/neural_networks.html#convolutions-in-2d",
    "title": "Neural Networks",
    "section": "Convolutions in 2D",
    "text": "Convolutions in 2D\n\n\n\nsource: Arden Dertat"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#d-convolution-animation",
    "href": "sessions/W05_neural_networks/neural_networks.html#d-convolution-animation",
    "title": "Neural Networks",
    "section": "2D Convolution Animation",
    "text": "2D Convolution Animation\n\n\n\nsource: Arden Dertat"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#d-smoothing",
    "href": "sessions/W05_neural_networks/neural_networks.html#d-smoothing",
    "title": "Neural Networks",
    "section": "2D Smoothing",
    "text": "2D Smoothing"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#d-gradients",
    "href": "sessions/W05_neural_networks/neural_networks.html#d-gradients",
    "title": "Neural Networks",
    "section": "2D Gradients",
    "text": "2D Gradients"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#max-pooling",
    "href": "sessions/W05_neural_networks/neural_networks.html#max-pooling",
    "title": "Neural Networks",
    "section": "Max Pooling",
    "text": "Max Pooling\n\n\n\n\nNeed to remember position of maximum for back-propagation\nAgain not differentiable → subgradient descent"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#convolutional-neural-networks-1",
    "href": "sessions/W05_neural_networks/neural_networks.html#convolutional-neural-networks-1",
    "title": "Neural Networks",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\n\n\n\n\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner: Gradient-based learning applied to document recognition"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#other-architectures",
    "href": "sessions/W05_neural_networks/neural_networks.html#other-architectures",
    "title": "Neural Networks",
    "section": "Other Architectures",
    "text": "Other Architectures"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#conv-nets-with-keras",
    "href": "sessions/W05_neural_networks/neural_networks.html#conv-nets-with-keras",
    "title": "Neural Networks",
    "section": "Conv-nets with Keras",
    "text": "Conv-nets with Keras"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#preparing-data",
    "href": "sessions/W05_neural_networks/neural_networks.html#preparing-data",
    "title": "Neural Networks",
    "section": "Preparing Data",
    "text": "Preparing Data\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\nimg_rows, img_cols = 28, 28\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nX_train_images = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nX_test_images = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#create-tiny-network",
    "href": "sessions/W05_neural_networks/neural_networks.html#create-tiny-network",
    "title": "Neural Networks",
    "section": "Create Tiny Network",
    "text": "Create Tiny Network\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\n\nnum_classes = 10\ncnn = Sequential()\ncnn.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Conv2D(32, (3, 3), activation='relu'))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Flatten())\ncnn.add(Dense(64, activation='relu'))\ncnn.add(Dense(num_classes, activation='softmax'))"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#number-of-parameters",
    "href": "sessions/W05_neural_networks/neural_networks.html#number-of-parameters",
    "title": "Neural Networks",
    "section": "Number of Parameters",
    "text": "Number of Parameters\n\n\nConvolutional Network for MNIST\n\n\n\n\nDense Network for MNIST"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#train-and-evaluate",
    "href": "sessions/W05_neural_networks/neural_networks.html#train-and-evaluate",
    "title": "Neural Networks",
    "section": "Train and Evaluate",
    "text": "Train and Evaluate\ncnn.compile(\\\"adam\\\", \\\"categorical_crossentropy\\\", metrics=['accuracy'])\nhistory_cnn = cnn.fit(X_train_images, y_train,\n                      batch_size=128, epochs=20, verbose=1, validation_split=.1)\ncnn.evaluate(X_test_images, y_test)\n 9952/10000 [============================&gt;.] - ETA: 0s\n [0.089020583277629253, 0.98429999999999995]"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#visualize-filters",
    "href": "sessions/W05_neural_networks/neural_networks.html#visualize-filters",
    "title": "Neural Networks",
    "section": "Visualize Filters",
    "text": "Visualize Filters\nweights, biases = cnn_small.layers[0].get_weights()\nweights2, biases2 = cnn_small.layers[2].get_weights()\nprint(weights.shape)\nprint(weights2.shape)\n(3,3,1,8)\n(3,3,8,8)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#learned-features",
    "href": "sessions/W05_neural_networks/neural_networks.html#learned-features",
    "title": "Neural Networks",
    "section": "Learned Features",
    "text": "Learned Features"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#convnets-vs-fully-connected-nets-illustrated",
    "href": "sessions/W05_neural_networks/neural_networks.html#convnets-vs-fully-connected-nets-illustrated",
    "title": "Neural Networks",
    "section": "Convnets vs Fully Connected Nets Illustrated",
    "text": "Convnets vs Fully Connected Nets Illustrated"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#mnist-and-permuted-mnist",
    "href": "sessions/W05_neural_networks/neural_networks.html#mnist-and-permuted-mnist",
    "title": "Neural Networks",
    "section": "MNIST and Permuted MNIST",
    "text": "MNIST and Permuted MNIST\n\n\n\nrng = np.random.RandomState(42)\nperm = rng.permutation(784)\nX_train_perm = X_train.reshape(-1, 784)[:, perm].reshape(-1, 28, 28)\nX_test_perm = X_test.reshape(-1, 784)[:, perm].reshape(-1, 28, 28)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#questions",
    "href": "sessions/W05_neural_networks/neural_networks.html#questions",
    "title": "Neural Networks",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#introduction-to-keras",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#introduction-to-keras",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Introduction to Keras",
    "text": "Introduction to Keras"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#keras-sequential",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#keras-sequential",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Keras Sequential",
    "text": "Keras Sequential\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential([\n    Dense(32, input_shape=(784,)),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax')])\n\n# or\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\n\n# or\nmodel = Sequential([\n    Dense(32, input_shape=(784,), activation='relu'),\n    Dense(10, activation='softmax')])"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#model-summary",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#model-summary",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Model Summary",
    "text": "Model Summary\nmodel.summary()"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#setting-optimizer",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#setting-optimizer",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Setting Optimizer",
    "text": "Setting Optimizer\n\n\n\nmodel.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#training-the-model",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#training-the-model",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Training the Model",
    "text": "Training the Model"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#preparing-mnist-data",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#preparing-mnist-data",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Preparing MNIST Data",
    "text": "Preparing MNIST Data\nfrom keras.datasets import mnist\nimport keras\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#fit-model",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#fit-model",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Fit Model",
    "text": "Fit Model\nmodel.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#fit-with-validation",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#fit-with-validation",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Fit with Validation",
    "text": "Fit with Validation\nmodel.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1,\n          validation_split=.1)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#evaluating-on-test-set",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#evaluating-on-test-set",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Evaluating on Test Set",
    "text": "Evaluating on Test Set\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Test loss: {:.3f}\".format(score[0]))\nprint(\"Test Accuracy: {:.3f}\".format(score[1]))\nTest loss: 0.120\nTest Accuracy: 0.966"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#loggers-and-callbacks",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#loggers-and-callbacks",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Loggers and Callbacks",
    "text": "Loggers and Callbacks\nhistory_callback = model.fit(X_train, y_train, batch_size=128,\n                             epochs=100, verbose=1, validation_split=.1)\npd.DataFrame(history_callback.history).plot()"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#wrappers-for-sklearn",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#wrappers-for-sklearn",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Wrappers for sklearn",
    "text": "Wrappers for sklearn\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nfrom sklearn.model_selection import GridSearchCV\n\ndef make_model(optimizer=\"adam\", hidden_size=32):\n    model = Sequential([\n        Dense(hidden_size, input_shape=(784,)),\n        Activation('relu'),\n        Dense(10),\n        Activation('softmax'),\n    ])\n    model.compile(optimizer=optimizer,loss=\"categorical_crossentropy\",\n                  metrics=['accuracy'])\n    return model\n\nclf = KerasClassifier(make_model)\nparam_grid = {'epochs': [1, 5, 10],\n              'hidden_size': [32, 64, 256]}\ngrid = GridSearchCV(clf, param_grid=param_grid)\ngrid.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#grid-search-results",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#grid-search-results",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Grid Search Results",
    "text": "Grid Search Results\nres = pd.DataFrame(grid.cv_results_)\nres.pivot_table(index=[\"param_epochs\", \"param_hidden_size\"],\n                values=['mean_train_score', \"mean_test_score\"])"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutional-neural-networks",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutional-neural-networks",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#idea-behind-cnns",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#idea-behind-cnns",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Idea Behind CNNs",
    "text": "Idea Behind CNNs\n\nTranslation invariance\nWeight sharing"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#definition-of-convolution",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#definition-of-convolution",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Definition of Convolution",
    "text": "Definition of Convolution\n\\[(f*g)[n] = \\sum\\limits_{m=-\\infty}^\\infty f[m]g[n-m]\\]\n\\[= \\sum\\limits_{m=-\\infty}^\\infty f[n-m]g[m]\\]"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-example-gaussian-smoothing",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-example-gaussian-smoothing",
    "title": "Keras & Convolutional Neural Nets",
    "section": "1D Example: Gaussian Smoothing",
    "text": "1D Example: Gaussian Smoothing"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutions-in-2d",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutions-in-2d",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Convolutions in 2D",
    "text": "Convolutions in 2D\n\n\n\nsource: Arden Dertat"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-convolution-animation",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-convolution-animation",
    "title": "Keras & Convolutional Neural Nets",
    "section": "2D Convolution Animation",
    "text": "2D Convolution Animation\n\n\n\nsource: Arden Dertat"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-smoothing",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-smoothing",
    "title": "Keras & Convolutional Neural Nets",
    "section": "2D Smoothing",
    "text": "2D Smoothing"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-gradients",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-gradients",
    "title": "Keras & Convolutional Neural Nets",
    "section": "2D Gradients",
    "text": "2D Gradients"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#max-pooling",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#max-pooling",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Max Pooling",
    "text": "Max Pooling\n\n\n\n\nNeed to remember position of maximum for back-propagation\nAgain not differentiable → subgradient descent"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutional-neural-networks-1",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutional-neural-networks-1",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\n\n\n\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner: Gradient-based learning applied to document recognition"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#other-architectures",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#other-architectures",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Other Architectures",
    "text": "Other Architectures"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#conv-nets-with-keras",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#conv-nets-with-keras",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Conv-nets with Keras",
    "text": "Conv-nets with Keras"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#preparing-data",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#preparing-data",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Preparing Data",
    "text": "Preparing Data\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\nimg_rows, img_cols = 28, 28\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nX_train_images = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nX_test_images = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#create-tiny-network",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#create-tiny-network",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Create Tiny Network",
    "text": "Create Tiny Network\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\n\nnum_classes = 10\ncnn = Sequential()\ncnn.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Conv2D(32, (3, 3), activation='relu'))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Flatten())\ncnn.add(Dense(64, activation='relu'))\ncnn.add(Dense(num_classes, activation='softmax'))"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#number-of-parameters",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#number-of-parameters",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Number of Parameters",
    "text": "Number of Parameters\n\n\nConvolutional Network for MNIST\n\n\n\n\nDense Network for MNIST"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#train-and-evaluate",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#train-and-evaluate",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Train and Evaluate",
    "text": "Train and Evaluate\ncnn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\nhistory_cnn = cnn.fit(X_train_images, y_train,\n                      batch_size=128, epochs=20, verbose=1, validation_split=.1)\ncnn.evaluate(X_test_images, y_test)\n 9952/10000 [============================&gt;.] - ETA: 0s\n [0.089020583277629253, 0.98429999999999995]"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#visualize-filters",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#visualize-filters",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Visualize Filters",
    "text": "Visualize Filters\nweights, biases = cnn_small.layers[0].get_weights()\nweights2, biases2 = cnn_small.layers[2].get_weights()\nprint(weights.shape)\nprint(weights2.shape)\n(3,3,1,8)\n(3,3,8,8)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#learned-features",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#learned-features",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Learned Features",
    "text": "Learned Features"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#convnets-vs-fully-connected-nets-illustrated",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#convnets-vs-fully-connected-nets-illustrated",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Convnets vs Fully Connected Nets Illustrated",
    "text": "Convnets vs Fully Connected Nets Illustrated"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#mnist-and-permuted-mnist",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#mnist-and-permuted-mnist",
    "title": "Keras & Convolutional Neural Nets",
    "section": "MNIST and Permuted MNIST",
    "text": "MNIST and Permuted MNIST\n\n\n\nrng = np.random.RandomState(42)\nperm = rng.permutation(784)\nX_train_perm = X_train.reshape(-1, 784)[:, perm].reshape(-1, 28, 28)\nX_test_perm = X_test.reshape(-1, 784)[:, perm].reshape(-1, 28, 28)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#questions",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#questions",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#last-week",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#last-week",
    "title": "Supervised learning workflow",
    "section": "Last week",
    "text": "Last week\n\nFramework of supervised learning\nEvaluation metrics for regression and classification"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#objectives-of-this-week",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#objectives-of-this-week",
    "title": "Supervised learning workflow",
    "section": "Objectives of this week",
    "text": "Objectives of this week\n\nUnderstand different workflow of supervised learning.\nUnderstand train-test split and train-validation-test split.\nUnderstand cross validation and its extensions\nKnow when to use different model evaluation methods."
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#supervised-learning",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#supervised-learning",
    "title": "Supervised learning workflow",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in\n\\begin{cases}\n\\mathbb{R}, & \\text{(regression)} \\\\\n\\mathcal{Y} \\text{ (finite set)}, & \\text{(classification)}\n\\end{cases} \\\\\n\\text{learn } f(x_i) &\\approx y_i \\\\\n\\text{such that } f(x) &\\approx y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#challenges-of-training-supervised-models",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#challenges-of-training-supervised-models",
    "title": "Supervised learning workflow",
    "section": "Challenges of training supervised models",
    "text": "Challenges of training supervised models\n\nGeneralise to new data\nAvoid overfitting/underfitting\nModel selection (hyperparameter tuning)"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-7525",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-7525",
    "title": "Supervised learning workflow",
    "section": "Train-Test Split (75/25)",
    "text": "Train-Test Split (75/25)"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#why-train-test-split",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#why-train-test-split",
    "title": "Supervised learning workflow",
    "section": "Why Train-Test Split?",
    "text": "Why Train-Test Split?\n\n\nStatistics\n\nNo train-test split\nTrain and evaluate on whole data\nEstimation is key\nLow model complexity\n\n\nMachine Learning\n\nTrain-test split\nTrain on part, evaluate on held-out part\nPrediction (Generalisation) is key\nHigh model complexity; overfitting risk"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#influence-of-n_neighbors-k3",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#influence-of-n_neighbors-k3",
    "title": "Supervised learning workflow",
    "section": "Influence of n_neighbors (k=3)",
    "text": "Influence of n_neighbors (k=3)\n\nLarger k → smoother boundary\nSmaller k → complex boundary"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting",
    "title": "Supervised learning workflow",
    "section": "Overfitting vs Underfitting",
    "text": "Overfitting vs Underfitting"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting-2",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting-2",
    "title": "Supervised learning workflow",
    "section": "Overfitting vs Underfitting",
    "text": "Overfitting vs Underfitting"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#so-far-happy-ending",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#so-far-happy-ending",
    "title": "Supervised learning workflow",
    "section": "So far: Happy ending?",
    "text": "So far: Happy ending?\n\nReport: best k=19, test accuracy=0.77\nGood for choosing k\nBut: overly optimistic for generalisation\nProblem: test set used for both choosing k and final evaluation"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#why-overfitting-the-validation-set",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#why-overfitting-the-validation-set",
    "title": "Supervised learning workflow",
    "section": "Why? overfitting the Validation Set",
    "text": "Why? overfitting the Validation Set\n\n\n\n\nInteresting reading Preventing Overfitting in cross-validation - Ng 1997"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-the-validation-set",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-the-validation-set",
    "title": "Supervised learning workflow",
    "section": "Overfitting the Validation Set",
    "text": "Overfitting the Validation Set"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#threefold-split-code",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#threefold-split-code",
    "title": "Supervised learning workflow",
    "section": "Threefold Split (Code)",
    "text": "Threefold Split (Code)\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=0)\n\nval_scores = []\nneighbors = np.arange(1, 15, 2)\nfor i in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    val_scores.append(knn.score(X_val, y_val))\nprint(f\"best validation score: {np.max(val_scores):.3}\\n\")\nbest_n_neighbors = neighbors[np.argmax(val_scores)]\nprint(f\"best n_neighbors:{best_n_neighbors}\\n\")\n\nknn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\nknn.fit(X_trainval, y_trainval)\nprint(f\"test-set score: {knn.score(X_test, y_test):.3f}\")\nbest validation score: 0.991 best n_neighbors: 11 test-set score: 0.951\n\nHere is an implementation of the three-fold split for selecting the number of neighbors. For each number of neighbors that we want to try, we build a model on the training set, and evaluate it on the validation set. We then pick the best validation set score, here that’s 99.1%, achieved when using 11 neighbors. We then retrain the model with this parameter, and evaluate on the test set. The retraining step is somewhat optional. We could also just use the best model. But retraining allows us to make better use of all the data.\nStill, our results depend on how exactly we split the datasets. So how can we make this more robust?"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#new-problem-with-threefold-split",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#new-problem-with-threefold-split",
    "title": "Supervised learning workflow",
    "section": "New problem with threefold split",
    "text": "New problem with threefold split\n\nFixed train/val/test split → results depend on split\nHigh variance in best k and test score, not robust\n\n\n\n   random_seed  best_validation_score  best_k  test_set_score\n0            0                    0.7       5        0.846154\n1            1                    0.7       1        0.538462\n2            2                    1.0      13        0.692308\n3            3                    0.7       1        0.846154\n4            4                    0.9       5        0.769231\n5            5                    0.8      11        0.769231"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-cv-test-set",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-cv-test-set",
    "title": "Supervised learning workflow",
    "section": "Cross-Validation (CV) + Test Set",
    "text": "Cross-Validation (CV) + Test Set"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#n_neighbors-search-results",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#n_neighbors-search-results",
    "title": "Supervised learning workflow",
    "section": "n_neighbors Search Results",
    "text": "n_neighbors Search Results"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-strategies",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-strategies",
    "title": "Supervised learning workflow",
    "section": "Cross-Validation Strategies",
    "text": "Cross-Validation Strategies"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#how-many-cv-folds",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#how-many-cv-folds",
    "title": "Supervised learning workflow",
    "section": "How many CV folds?",
    "text": "How many CV folds?\n\nRecommend to run 5-fold or 10-fold CV multiple times, while shuffling the dataset\nMore folds → more training data per fold → better generlisation performance estimate, but slower\nExtreme: LeaveOneOut CV (one fold per sample)"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#repeated-kfold-and-leaveoneout",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#repeated-kfold-and-leaveoneout",
    "title": "Supervised learning workflow",
    "section": "Repeated KFold and LeaveOneOut",
    "text": "Repeated KFold and LeaveOneOut\n\nLeaveOneOut: high variance, slow\nShuffleSplit: repeated random splits\nRepeatedKFold: multiple shuffled KFold runs"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#shuffle-split",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#shuffle-split",
    "title": "Supervised learning workflow",
    "section": "Shuffle Split",
    "text": "Shuffle Split"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#standard-cv-not-preserving-class-distribution",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#standard-cv-not-preserving-class-distribution",
    "title": "Supervised learning workflow",
    "section": "Standard CV not preserving class distribution",
    "text": "Standard CV not preserving class distribution\n\nStandard CV leads to folds with different class distributions"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#stratified-cv-for-multiclass-classification-or-imbalanced-data",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#stratified-cv-for-multiclass-classification-or-imbalanced-data",
    "title": "Supervised learning workflow",
    "section": "Stratified CV: for multiclass classification or imbalanced data",
    "text": "Stratified CV: for multiclass classification or imbalanced data\n\nPreserve class distribution in each fold"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cv-in-scikit-learn",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cv-in-scikit-learn",
    "title": "Supervised learning workflow",
    "section": "CV in scikit-learn",
    "text": "CV in scikit-learn\n\n5-fold CV (default)\nClassification CV is stratified by default\ntrain_test_split(..., stratify=y) to stratify\nNo shuffle by default (repeatable results)"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cv-for-grouped-data",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cv-for-grouped-data",
    "title": "Supervised learning workflow",
    "section": "CV for grouped data?",
    "text": "CV for grouped data?\n\nCV is more complicated when data are grouped\nData points within a group are correlated (e.g., city, patient, user)\ne.g. The task is to if a patient has a disease based on medical records from 9 cities\nHow CV should be done depends on the application scenario"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#scenario-1-to-predict-new-data-from-existing-cities-i.i.d.",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#scenario-1-to-predict-new-data-from-existing-cities-i.i.d.",
    "title": "Supervised learning workflow",
    "section": "Scenario 1: To predict new data from existing cities (i.i.d.)",
    "text": "Scenario 1: To predict new data from existing cities (i.i.d.)\n\nStandard CV (e.g. KFold, RepeatedKFold) can be used\nGroup information can be ignored"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#scenario-2-to-predict-new-data-from-unknown-cities-not-i.i.d.",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#scenario-2-to-predict-new-data-from-unknown-cities-not-i.i.d.",
    "title": "Supervised learning workflow",
    "section": "Scenario 2: To predict new data from unknown cities (not i.i.d.)",
    "text": "Scenario 2: To predict new data from unknown cities (not i.i.d.)\n\nGroupKFold should be used; ensure each group is contained in exactly one fold (either train or test)\nData from 9 cities; 5-fold CV; GroupKFold as below."
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#standard-train-test-split-or-cv-not-suitable-for-time-series",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#standard-train-test-split-or-cv-not-suitable-for-time-series",
    "title": "Supervised learning workflow",
    "section": "Standard train-test split or CV not suitable for time series",
    "text": "Standard train-test split or CV not suitable for time series"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-for-time-series",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-for-time-series",
    "title": "Supervised learning workflow",
    "section": "Train-Test Split for Time Series",
    "text": "Train-Test Split for Time Series\n\nUsing past data to train, future data to test"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#timeseriessplit",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#timeseriessplit",
    "title": "Supervised learning workflow",
    "section": "TimeSeriesSplit",
    "text": "TimeSeriesSplit"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#time-series-cv",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#time-series-cv",
    "title": "Supervised learning workflow",
    "section": "Time Series CV",
    "text": "Time Series CV"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overview",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overview",
    "title": "Supervised learning workflow",
    "section": "Overview",
    "text": "Overview\nWe’ve covered:\n\nTrain-test split, threefold split\nCross-validation and its extensions, especially RepeatedKFold\nStratified CV (StratifiedKFold) for multiple classes and imbalanced data\nCV for grouped data and time series data"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#questions",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#questions",
    "title": "Supervised learning workflow",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "We all need help from time to time, and while we will always do our best to support you because we know that this module is hard for students who are new to quantitative modules or statistics, the best way to ‘get help’ will also always be taking steps to ‘help yourself’ first."
  },
  {
    "objectID": "help.html#how-to-help-yourself",
    "href": "help.html#how-to-help-yourself",
    "title": "Getting Help",
    "section": "How to Help Yourself",
    "text": "How to Help Yourself\nHere are at least six things that you can do to ‘help yourself’:\n\nMake use of practical sessions–we can’t help you if we don’t know that you’re struggling. Please talk to the lecturer or TAs during the pratical sessions.\nUse the dedicated #casa0007_qm channel on Slack –this provides a much richer experience than the Moodle Forum and should be your primary means of requesting help outside of scheduled teaching hours.\nDo the readings–regardless of whether we ask you questions in class about them (or not), the readings are designed to support the module’s learning outcomes, so if you are struggling with a concept or an idea then please look to the week’s readings! You should also review the full bibliography while developing your thinking for the final project.\nUse Google or Stack Overflow–as you become a better programmer you’ll start to understand how to frame your question in ways that produce the right answer right away, but whether you’re a beginner or an expert Stack Overflow is your friend.\nSign up for online classes–there are lots of plausible online classes on LinkedIn course or Coursera. Please check the reviews before you take an online module."
  },
  {
    "objectID": "assessments/index.html",
    "href": "assessments/index.html",
    "title": "CASA0006 Assessment",
    "section": "",
    "text": "This Assessment is worth 100% of the grade for this course.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#assessment-aims",
    "href": "assessments/index.html#assessment-aims",
    "title": "CASA0006 Assessment",
    "section": "Assessment Aims",
    "text": "Assessment Aims\nThis assessment is designed to test your understanding of the quantitative methods introduced in this course, but also, crucially, your ability to really understand how, if used correctly and applied to appropriate data, these methods can help you tell a powerful story and be the underpinning evidence-base for a relevant local, national or international issue.\nUsing quantitative methods to contribute to public debates is vital if students, academics and universities are to demonstrate our value to the wider world. We don’t want you to just apply a method and regurgitate unintelligible coefficients. Quantitative methods can only have real value if they are used to support wider debates in ways that everyone should be able to understand.\nWe are not testing your coding abilities – you may use either of the main coding languages taught this term (R or Python). We are not testing your ability to write a standard piece of academic writing. But we are testing your ability to use quantitative methods appropriately and with a clear connection between the data, methods and outputs, to help others understand a particular issue through a data lens.\n\nPart 1 - Article\nYour task is to write a short 800-1000-word piece of ‘public-facing scholarly writing’ in the style of an article that that could appear in The Conversation or the Financial Times in their data section. This piece could relate to a local, national or international topic which could be either serious or frivolous but must employ appropriate quantitative methods learned in this course to derive novel insights from a particular dataset (or range of datasets) associated with a particular topic of your choosing. Data analysis and the use of quantitative methods should be central to your piece, but outputs must be appropriate for a general (non-academic) audience with your piece illuminated by appropriate visual outputs – maps, graphs or other data visualisations. Your piece should contain a range of graphical or tabular elements.\n\n\nPart 2 - Technical Appendix\nYour 800-1000-word article should be accompanied by a max 1000-word technical appendix detailing the analysis you have carried out behind the scenes to allow you to make the observations you have in your article. Here you might want to include additional exploratory visualisations, tabular outputs, interpretations of those outputs, equations etc. and you could also include any observations about the dataset or the validity / statistical significance of any models you employ. The purpose of the appendix is to reassure anyone who wants to delve deeper, that the observations you made in your main article are valid and reliable and your interpretations valid.\n\n\nTopic\nYour topic can be anything you like broadly related to human, urban or social issues, as long as you can find some suitable data to analyse. For inspiration on relevant topics, you might want to review some of the articles that John Burn-Murdoch has written for the Financial Times in recent years - https://www.ft.com/john-burn-murdoch (you can log-in via your UCL credentials) or some of the pieces in the Conversation - https://theconversation.com/.\nIf you are struggling for inspiration, you are welcome to explore an educational topic using DfE schools data used in class, but you are encouraged to be creative in your data choices (as you are being partially marked on your originality), and you should not repeat analyses carried out on variables in any of the practical sessions. The only topic we will not permit in this assessment is anything related to AirBnB as this is the focus of CASA0013.\n\n\nArticle Content\nYou will note that most of John Burn-Murdoch’s articles generally contain the sorts of analyses we would describe as exploratory. While your article should contain basic exploratory analysis in the form of visualisations, you should also use an appropriate method from the second half of the course (lectures 5-10) related to either more sophisticated exploratory analysis like multivariate statistical analysis (e.g. dimensionality reduction or cluster analysis) or some explanatory / predictive methods such as ANOVA, linear regression or some of the generalised linear models also introduced.\n\n\nStyle of Briefing\nYou should write in plain English and avoid the use of jargon or technical language. For tips on how to write in this style, The Conversation has produced a guide: https://socsci.web.ox.ac.uk/files/conversation-writing-public-why-and-how\n\n\nTypes of Data Permissible and Sources\nAnything you like, but you should choose carefully so that you are able to demonstrate the appropriate skills. There are many potential sources of data – these could be linked from FT or Conversation articles, or you could try sites like:\nhttps://data.gov/\nhttps://www.data.gov.uk/\nhttps://opendata.nhsbsa.net/\nhttps://tfl.gov.uk/info-for/open-data-users/  \nhttps://data.europa.eu/data/datasets?locale=en\nhttps://data.worldbank.org/\nhttps://data.london.gov.uk/\nYou can probably find many more!\n\n\nReferencing\nWe will not expect standard academic referencing in this piece, however, this doesn’t mean that you shouldn’t include references – you should. In this style of public facing scholarly writing, it is common to use hyperlinks and footnotes and you should make use of these to support your narrative.\n\n\nFormat of the Piece\nExamples – Here is just one example of the kind of article you might produce (minus the technical appendix), but read widely around the data journalism sites linked from publications like the Financial Times, New York Times, Guardian.\nhttps://theconversation.com/constituency-level-data-reveals-which-parties-are-most-threatened-by-reform-264422\n\n\nDeadline and Handing In\nThe Deadline for the assessment is Monday, 27 April 2026 @ 10:00.\nYour report and technical appendix should be uploaded to Moodle as a single PDF document.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#mark-scheme",
    "href": "assessments/index.html#mark-scheme",
    "title": "CASA0006 Assessment",
    "section": "Mark Scheme",
    "text": "Mark Scheme\nThis is how we will mark your work - take note of them\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../data/assessment.csv\", encoding = \"ISO-8859-1\")\nfrom IPython.display import display, Markdown\ndisplay(Markdown(df.to_markdown()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriterion\n80-100%\n70-79% (A)\n60-69% (B)\n50-59% (C)\n40-49% (D)\n1-39% (E)\n\n\n\n\n0\nExploratory Analysis (25%)\nOutstanding selection of exploratory statistics and/or sophisticated data visualisations which illuminate the underlying data, revealing distributions / trends / relationships and associations in the data with absolute clarity. Graphics are labelled such that readers are in no doubt about what is being shown. Data may have been transformed, normalised or standardised in some way to reveal otherwise hidden patterns and justified impeccably.\nExcellent selection of exploratory statistics and/or sophisticated data visualisations which illuminate the underlying data, revealing distributions / trends / relationships and associations in the data. Graphics are labelled such that readers are in no doubt about what is being shown. Data may have been transformed, normalised or standardised in some way to reveal otherwise hidden patterns and justified.\nGood selection of exploratory statistics and/or data visualisations which illuminate the underlying data, revealing distributions / trends / relationships and associations. Graphics are labelled such that readers are able to interpret the plots with ease.\nAdequate selection of exploratory statistics and/or data visualisations which illuminate some of the underlying data, revealing some distributions / trends / relationships and associations. Graphics are labelled, but may lack clarity. Data may not be transformed, normalised or standardised in a way to reveal patterns.\nInadequate selection of statistics or visualisations. Graphics are poorly labelled or unclear, and do not illuminate the underlying data. Data transformations are incorrect or missing.\nNegligible use of statistics or visualisations. Any visualisations are irrelevant, inaccurate, or inaccessible. The analysis shows no grasp of the underlying data.\n\n\n1\nUse of multivariate statistical analysis or explanatory / predictive methods (25%)\nExceptional selection of data / variables entirely appropriate for the chosen article topic. Masterful understanding of the nuances related to the careful pruning and selection of appropriate variables. Outstanding understanding of the methods employed and their interaction with the data to hand with masterful understanding of outputs produced\nExcellent selection of data / variables entirely appropriate for the chosen article topic. Highly competent understanding of the nuances related to the careful pruning and selection of appropriate variables. Highly competent understanding of the methods employed and their interaction with the data to hand.\nGood selection of data / variables appropriate for the chosen article topic. Competent understanding of the methods employed and their interaction with the data.\nAdequate selection of data / variables for the chosen article topic. Basic understanding of the methods employed and their interaction with the data. May get some of the nuances in the outputs, but may also ignore some key features in the data\nInadequate selection of data / variables for the chosen article topic. Little to no understanding of the methods employed or how they interact with the data.\nNegligible or irrelevant selection of data / variables. No grasp of the methods or their application to the data.\n\n\n2\nOriginality, article narrative and communication (25%)\nHighly original topic selection, or of exceptional relevance to a contemporary debate in the society, the media or politics at a local, national or international level with broad interest. Article narrative shows flare or originality which draws the reader in and reveals something entirely new. Writing style is highly accessible - clear, concise and creative and the reader is left without query or misunderstanding.\nExcellent topic selection, of high relevance to a contemporary debate. The narrative is engaging and written with excellent clarity, flowing well from one section to the next. The work is of a very high standard. Writing style clear and concise with few wasted words.\nGood topic selection, of relevance to a contemporary debate. The narrative is clear and well-structured. The work shows some evidence of originality. The narrative is good and the message emerging from the analysis is conveyed well.\nAdequate topic selection, but may lack relevance or wider interest. The narrative is satisfactory but may lack clarity or logical flow  ideas appearing slightly disorganised. The reader can understand the piece but may have to work hard to derive meaning from it.\nInadequate topic selection  perhaps dated or totally irrelevant to the degree programme (i.e. not even a human topic). The narrative is lacking in clarity and is difficult to follow. Deriving meaning from the work is a challenge.\nThe work has no clear topic or narrative. The communication is confused, unclear, or inaccessible.\n\n\n3\nConceptual understanding and Critical Reflection (25%)\nBoth the Article and Technical Appendix show exemplary understanding of the topic / wider issues associated with it and of the methods employed to interrogate the data. A clear understanding of any data / methodological shortcomings / issues / challenges is presented with a highly sophisticated degree of critical reflection in relation to the substantive topic and / or methods employed is demonstrated.\nArticle and Technical Appendix show excellent and highly competent conceptual understanding of key concepts and theories related to both the topic and methods employed. The work demonstrates a thorough understanding of the chosen example and recognises and reflects lucidly on any shortcomings and / or the wider significance of the findings in a way that is not contrived or formulaic but shows a sophisticated level of insight.\nArticle and Technical Appendix show good understanding of key concepts and theories related to both the topic and methods employed. The work demonstrates a sound understanding of the chosen example and recognises and reflects lucidly on any shortcomings and / or the wider significance of the findings in a way that is not contrived or formulaic but shows a sophisticated level of insight.\nArticle and Technical Appendix show basic understanding of key concepts and theories related to both the topic and methods employed. The work demonstrates a rudimentary understanding of the chosen example and recognises and may offer only some reflection on the shortcomings of the work or not at all / contrived at the bottom end.\nInadequate and insufficient conceptual understanding of key concepts and theories. The work demonstrates an invalid or lack of understanding of the concepts introduced. Any analysis attempted fails to support the observations made.\nNegligible or no conceptual understanding of key concepts and theories. The work demonstrates an irrelevant, inaccurate, confused, unclear, or inaccessible understanding of the concepts.\n\n\n\n\n\nThe purpose of this assessment is to test your understanding of the various methods introduced in this course and your ability to apply them appropriately to a topic of your choice. One of the differentiators at Masters level is the ability to think both creatively and critically while showing an awareness or knowledge of contemporary issues either in relation to your specific discipline of study or more widely. Being able to demonstrate how established techniques of research and enquiry can be used to create and interpret knowledge is at the core of Masters level thinking and this assessment piece. The ability to demonstrate self-direction (in choosing an appropriate topic for this assessment) and to think autonomously in designing your own article.\n\nMark Scheme – Explained\nLevel 7 Descriptors - https://www.qaa.ac.uk/docs/qaa/quality-code/the-frameworks-for-higher-education-qualifications-of-uk-degree-awarding-bodies-2024.pdf - p26\nMaster’s degree\nThe descriptor provided for this level of the Frameworks is for any master’s degree which should meet the descriptor in full. This qualification descriptor should also be used as a reference point for other qualifications at Level 7 on the FHEQ/SCQF Level 11 on the FQHEIS, including postgraduate certificates and postgraduate diplomas.\nMaster’s degrees are awarded to students who have demonstrated:\n•         a systematic understanding of knowledge, and a critical awareness of current problems and/or new insights, much of which is at, or informed by, the forefront of their academic discipline, field of study or area of professional practice\n•         a comprehensive understanding of techniques applicable to their own research or advanced scholarship\n•         originality in the application of knowledge, together with a practical understanding of how established techniques of research and enquiry are used to create and interpret knowledge in the discipline\n•         conceptual understanding that enables the student:\n•         to evaluate critically current research and advanced scholarship in the discipline\n•         to evaluate methodologies and develop critiques of them and, where appropriate, to propose new hypotheses.\nTypically, holders of the qualification will be able to:\n•         deal with complex issues - both systematically and creatively, make sound judgements in the absence of complete data, and communicate their conclusions clearly to specialist and non-specialist audiences\n•         demonstrate self-direction and originality in tackling and solving problems, and act autonomously in planning and implementing tasks at a professional or equivalent level\n•         continue to advance their knowledge and understanding, and to develop new skills to a high level. \nAnd holders will have:\n•         the qualities and transferable skills necessary for employment requiring:  \n•         the exercise of initiative and personal responsibility\n•         decision-making in complex and unpredictable situations\n•         the independent learning ability required for continuing professional development.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "The Data Science for Spatial Systems (DSSS) module is a optional element of CASA’s MSc/MRes USS Course and is intended provide an introduction to advanced computational techniques in spatial analysis, or advanced spatial data sicence.\nAs with most computational analysis, spatial data science builds on two pillars: methods and data. In recent year, as AI (represented by deep learning and large-language models) has developed rapidly, numerous efficient techniques have been invented and open-sourced, which enables integration of heterougenous data. For example, we are now able to combine census data, remote sensing data, street-view imagery, and text data for understanding urban security or urban heat island effect on an ordnary laptop. Notably, (geospatial) foundation models, which are large, pre-trained AI models, are expected to become versatile bases for many specific tasks. It seems like we could model and predict almost everything in spatial analysis, even if we don’t understand what foundation models are doing. However, this is not entirely true. So far, AI is not silver bullet and AI has halluciations. We need to understand the principle and limitations of various computational models; we need to be critical about methods and results; we need to ask critical questions for any task; and we need to link modelling with applications so as to inform policy and practice.\nThe module is structured progressively across three core sections. The first section introduces supervised machine learning and their applications. We will firstly introduce the principle and workflow of supervised machine learning, and then talk about tree-based methods and neural networks….\nFollowing TBC\nThe second section builds upon this foundation to explore the core of statistical modelling: understanding and quantifying relationships between variables. Students will progress from measuring correlations to building sophisticated regression models. The curriculum covers the workhorse of social science, the Generalised Linear Model (GLM), before advancing to Multilevel Models, a critical technique for handling the nested and hierarchical data that is common in geographical and social research.\nThe final section of the course introduces advanced techniques for uncovering hidden structures within complex, high-dimensional datasets. Students will learn methods for dimensionality reduction to simplify complexity without losing vital information, and clustering analysis to identify natural groupings and patterns in data.\nTherefore, this module guides students on a complete analytical journey, from foundational principles to the application of advanced modelling techniques. It serves as a vital prerequisite for more specialised analytical modules and is essential for students wishing to undertake quantitative geospatial research. Ultimately, this module will provide quantitative skills that are in high demand across public, private, and academic sectors."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe really appreciate the support from various people:\n\nJon for setting up this wonderful CASA-themed quarto template.\nOllie and Andy for inspiring us to pursue a Quarto-based module website."
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#last-week",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#last-week",
    "title": "Tree-Based Methods",
    "section": "Last week",
    "text": "Last week\n\nAnalysis workflow of supervised learning\nModel evaluation methods: train-test split, cross validation (and extensions)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#objectives-of-this-week",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#objectives-of-this-week",
    "title": "Tree-Based Methods",
    "section": "Objectives of this week",
    "text": "Objectives of this week\n\nUnderstand decision trees and tree-based ensemble methods (random forests, gradient boosting)\nCan apply tree-based methods to real-world problems"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#decision-trees-dt",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#decision-trees-dt",
    "title": "Tree-Based Methods",
    "section": "Decision Trees (DT)",
    "text": "Decision Trees (DT)\n\nMany types of decision trees: classification and regression trees (CART), C4.5, ID3, CHAID, etc.\nFocus on CART: binary trees for classification and regression\nBinary splits on features; leaves store predictions"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#why-dt",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#why-dt",
    "title": "Tree-Based Methods",
    "section": "Why DT?",
    "text": "Why DT?\n\nNon-linear, strong models for classification and regression\nMinimal preprocessing: scale and distributions matter little\nSmall trees can be explained; large trees power strong ensembles"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#decision-trees-for-classification",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#decision-trees-for-classification",
    "title": "Tree-Based Methods",
    "section": "Decision Trees for Classification",
    "text": "Decision Trees for Classification\n\nAsk a sequence of binary questions on features\nAxis-parallel splits; thresholds on single features\nLeaves store class distributions"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#building-trees",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#building-trees",
    "title": "Tree-Based Methods",
    "section": "Building Trees",
    "text": "Building Trees\n\n\n\nSearch all features and thresholds\nChoose split that most reduces impurity of data on a node\nRecurse on each child until stopping rule"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#criteria-classification",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#criteria-classification",
    "title": "Tree-Based Methods",
    "section": "Criteria (Classification)",
    "text": "Criteria (Classification)\n\nGini: \\(H_{\\text{gini}}(X_m) = \\sum_{k \\in \\mathcal{Y}} p_{mk}(1-p_{mk})\\)\nCross-Entropy: \\(H_{\\text{CE}}(X_m) = - \\sum_{k \\in \\mathcal{Y}} p_{mk}\\log p_{mk}\\)\nHere, \\(p_{mk}\\) is class \\(k\\) proportion in node \\(m\\)\nExample: a dataset with 10 samples in three classes (0, 1, 2) with proportions (0.2, 0.5, 0.3).\nGini = 0.2 × 0.8 + 0.5 × 0.5 + 0.3 × 0.7 = 0.66\nCross-Entropy = −(0.2 log 0.2 + 0.5 log 0.5 + 0.3 log 0.3) ≈ 1.0296"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#prediction",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#prediction",
    "title": "Tree-Based Methods",
    "section": "Prediction",
    "text": "Prediction\n\n\n\n\nGiven a new sample, start from the top\nTraverse splits; follow feature tests\nPredict majority class in the reached leaf"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#regression-trees",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#regression-trees",
    "title": "Tree-Based Methods",
    "section": "Regression Trees",
    "text": "Regression Trees\n\nPredict mean in leaf: \\(\\bar{y}_m = \\frac{1}{N_m}\\sum_{i \\in N_m} y_i\\)\nTwo impurity metrics\n\nMSE: \\(H(X_m) = \\frac{1}{N_m}\\sum_{i \\in N_m}(y_i-\\bar{y}_m)^2\\)\nMAE: \\(\\frac{1}{N_m}\\sum_{i \\in N_m}|y_i-\\bar{y}_m|\\)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#visualizing-trees-sklearn",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#visualizing-trees-sklearn",
    "title": "Tree-Based Methods",
    "section": "Visualizing Trees (sklearn)",
    "text": "Visualizing Trees (sklearn)\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=0)\n\ntree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#visualizing-trees-plot_tree",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#visualizing-trees-plot_tree",
    "title": "Tree-Based Methods",
    "section": "Visualizing Trees (plot_tree)",
    "text": "Visualizing Trees (plot_tree)\nfrom sklearn.tree import plot_tree\ntree_dot = plot_tree(tree, feature_names=cancer.feature_names)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#parameter-tuning",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#parameter-tuning",
    "title": "Tree-Based Methods",
    "section": "Parameter Tuning",
    "text": "Parameter Tuning\n\nPre-pruning (limit growth)\n\nmax_depth\nmax_leaf_nodes\nmin_samples_split\nmin_impurity_decrease\n\nPost-pruning (cost-complexity) after full growth"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#no-pruning",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#no-pruning",
    "title": "Tree-Based Methods",
    "section": "No Pruning",
    "text": "No Pruning"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#max_depth-4",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#max_depth-4",
    "title": "Tree-Based Methods",
    "section": "max_depth = 4",
    "text": "max_depth = 4"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#max_leaf_nodes-8",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#max_leaf_nodes-8",
    "title": "Tree-Based Methods",
    "section": "max_leaf_nodes = 8",
    "text": "max_leaf_nodes = 8"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#min_samples_split-50",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#min_samples_split-50",
    "title": "Tree-Based Methods",
    "section": "min_samples_split = 50",
    "text": "min_samples_split = 50"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#grid-search-max_depth",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#grid-search-max_depth",
    "title": "Tree-Based Methods",
    "section": "Grid Search: max_depth",
    "text": "Grid Search: max_depth\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth': range(1, 7)}\ngrid = GridSearchCV(DecisionTreeClassifier(random_state=0),\n                    param_grid=param_grid, cv=10)\ngrid.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#grid-search-max_leaf_nodes",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#grid-search-max_leaf_nodes",
    "title": "Tree-Based Methods",
    "section": "Grid Search: max_leaf_nodes",
    "text": "Grid Search: max_leaf_nodes\nparam_grid = {'max_leaf_nodes': range(2, 20)}"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#cost-complexity-pruning",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#cost-complexity-pruning",
    "title": "Tree-Based Methods",
    "section": "Cost Complexity Pruning",
    "text": "Cost Complexity Pruning\n\nObjective: \\(R_\\alpha(T) = R(T) + \\alpha |T|\\)\n\\(R(T)\\) = total leaf impurity; \\(|T|\\) = number of leaves; tune \\(\\alpha\\)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#efficient-pruning-path",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#efficient-pruning-path",
    "title": "Tree-Based Methods",
    "section": "Efficient Pruning Path",
    "text": "Efficient Pruning Path\nclf = DecisionTreeClassifier(random_state=0)\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#post--vs-pre-pruning",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#post--vs-pre-pruning",
    "title": "Tree-Based Methods",
    "section": "Post- vs Pre-Pruning",
    "text": "Post- vs Pre-Pruning\n\n\n\nCost-complexity pruning result\n\n\n\n\n\n\nmax_leaf_nodes search result"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#feature-importance",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#feature-importance",
    "title": "Tree-Based Methods",
    "section": "Feature Importance",
    "text": "Feature Importance\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, stratify=iris.target, random_state=0)\ntree = DecisionTreeClassifier(max_leaf_nodes=6).fit(X_train, y_train)\ntree.feature_importances_\n\n\n\n\nSum of impurity decreases per feature; magnitude only (no sign)\nUnstable with correlated features or different splits"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#categorical-data",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#categorical-data",
    "title": "Tree-Based Methods",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nTrees can split categories into subsets; many possible splits\nExact search costly; efficient for binary classification + Gini\nIn sklearn, one-hot encoding is needed today"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#predicting-probabilities",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#predicting-probabilities",
    "title": "Tree-Based Methods",
    "section": "Predicting Probabilities",
    "text": "Predicting Probabilities\n\nLeaf probability = class fraction in leaf\nDeep, unpruned trees give overconfident (100%) probabilities\nPruning helps but calibration may still be poor"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#flexible-split-functions",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#flexible-split-functions",
    "title": "Tree-Based Methods",
    "section": "Flexible Split Functions",
    "text": "Flexible Split Functions\n\n\n\nSource: Shotton et al., Real-Time Human Pose Recognition (Kinect v1)\n\n\n\nCan compare pixels, regions, or other engineered tests\nLinear models inside nodes possible when extrapolation needed"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#limitations-of-trees",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#limitations-of-trees",
    "title": "Tree-Based Methods",
    "section": "Limitations of Trees",
    "text": "Limitations of Trees\n\nCannot extrapolate beyond training data range\nInstability/overfitting: small data changes can alter splits. (Emsembles help)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-limits",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-limits",
    "title": "Tree-Based Methods",
    "section": "Extrapolation Limits",
    "text": "Extrapolation Limits\n\n\n\n\nTrees behave like nearest neighbors; cannot extrapolate beyond observed range"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-train-on-data-before-2000",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-train-on-data-before-2000",
    "title": "Tree-Based Methods",
    "section": "Extrapolation: train on data before 2000",
    "text": "Extrapolation: train on data before 2000"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-test-on-data-after-2000",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-test-on-data-after-2000",
    "title": "Tree-Based Methods",
    "section": "Extrapolation: test on data after 2000",
    "text": "Extrapolation: test on data after 2000\n\nTree predictions flatten outside training support; linear models can extrapolate"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#instability",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#instability",
    "title": "Tree-Based Methods",
    "section": "Instability",
    "text": "Instability\n\n\n\n\n\n\n\n\n\n\n\nSmall data changes can alter splits"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#poor-mans-ensemble",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#poor-mans-ensemble",
    "title": "Tree-Based Methods",
    "section": "Poor man’s ensemble",
    "text": "Poor man’s ensemble\n\nCombine multiple models to reduce variance / improve accuracy\nTrain several models with different seeds; average predictions\nOwen Zhang (long time kaggle 1st): build XGBoosting models with different random seeds.\nWorks across model families (e.g., tree + linear + RF + NN)\nKey to success: diversity among models\nsklearn: VotingClassifier\n\nsoft: average the probabilities of all models and take the arg max (need models provide calibrated probabilities)\nhard: let each model make a prediction and take majority vote"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#votingclassifier-example",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#votingclassifier-example",
    "title": "Tree-Based Methods",
    "section": "VotingClassifier Example",
    "text": "VotingClassifier Example\nfrom sklearn.ensemble import VotingClassifier\nvoting = VotingClassifier(\n    [('logreg', LogisticRegression(C=100)),\n     ('tree', DecisionTreeClassifier(max_depth=3, random_state=0))],\n    voting='soft')\nvoting.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#tree-ensembles-two-types",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#tree-ensembles-two-types",
    "title": "Tree-Based Methods",
    "section": "Tree ensembles: two types",
    "text": "Tree ensembles: two types\n\nBagging (Bootstrap Aggregation): random forests\nBoosting: gradient boosting machines (GBM), XGBoost, LightGBM, CatBoost, etc."
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#bagging-bootstrap-aggregation",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#bagging-bootstrap-aggregation",
    "title": "Tree-Based Methods",
    "section": "Bagging (Bootstrap Aggregation)",
    "text": "Bagging (Bootstrap Aggregation)\n\n\n\nSample with replacement (same size as dataset)\nTrain a model on each bootstrap sample\nAverage predictions to cut variance"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#bias-and-variance",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#bias-and-variance",
    "title": "Tree-Based Methods",
    "section": "Bias and Variance",
    "text": "Bias and Variance\n\n\n\n\nAim for low bias + low variance; averaging high-variance models can lower variance"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#ensembles-bias-vs-variance",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#ensembles-bias-vs-variance",
    "title": "Tree-Based Methods",
    "section": "Ensembles: Bias vs Variance",
    "text": "Ensembles: Bias vs Variance\n\nGeneralization improves with strong base learners and low correlation\nDiversifying models (or data/features) helps more than sheer count"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#random-forests",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#random-forests",
    "title": "Tree-Based Methods",
    "section": "Random Forests",
    "text": "Random Forests\n\n\n\n\nBagging + feature subsampling at each split"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#randomise-in-two-ways",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#randomise-in-two-ways",
    "title": "Tree-Based Methods",
    "section": "Randomise in Two Ways",
    "text": "Randomise in Two Ways\n\n\n\nFor each tree: bootstrap sample of rows\nFor each split: sample features without replacement\nMore trees → lower variance (diminishing returns)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#tuning-random-forests",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#tuning-random-forests",
    "title": "Tree-Based Methods",
    "section": "Tuning Random Forests",
    "text": "Tuning Random Forests\n\nmax_features: ~\\(\\sqrt{p}\\) for classification, ~\\(p\\) for regression\nn_estimators: use 100+; more reduces variance\nPre-pruning (max_depth, max_leaf_nodes, min_samples_split) can cut size/time"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#extremely-randomized-trees",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#extremely-randomized-trees",
    "title": "Tree-Based Methods",
    "section": "Extremely Randomized Trees",
    "text": "Extremely Randomized Trees\n\nAdd randomness: draw split thresholds at random per feature\nOften no bootstrap; faster (no threshold search)\nCan yield smoother boundaries; less common than standard RF"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#warm-starts",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#warm-starts",
    "title": "Tree-Based Methods",
    "section": "Warm-Starts",
    "text": "Warm-Starts\nrf = RandomForestClassifier(warm_start=True)\nfor n in range(1, 100, 5):\n    rf.n_estimators = n\n    rf.fit(X_train, y_train)\n\n\n\n\nIncrease trees incrementally; stop when scores stabilize"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#out-of-bag-estimates",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#out-of-bag-estimates",
    "title": "Tree-Based Methods",
    "section": "Out-of-Bag Estimates",
    "text": "Out-of-Bag Estimates\n\nEach tree trains on ~66% of data; predict remaining ~34%\nAverage OOB predictions as a free validation score\n\nrf = RandomForestClassifier(max_features=m, oob_score=True,\n                            n_estimators=200, random_state=0)\nrf.fit(X_train, y_train)\noob_scores.append(rf.oob_score_)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#variable-importance-rf",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#variable-importance-rf",
    "title": "Tree-Based Methods",
    "section": "Variable Importance (RF)",
    "text": "Variable Importance (RF)\nrf = RandomForestClassifier().fit(X_train, y_train)\nrf.feature_importances_\n\n\n\n\nMore stable than single-tree importances; still magnitude-only"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#trees-takeaways",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#trees-takeaways",
    "title": "Tree-Based Methods",
    "section": "Trees: Takeaways",
    "text": "Trees: Takeaways\n\nNon-linear without heavy preprocessing\nSingle small trees interpretable; forests robust baselines\nBeware extrapolation limits and instability"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-descent",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-descent",
    "title": "Tree-Based Methods",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\n\nOptimise \\(\\arg\\min_w F(w)\\) by stepping along \\(-\\nabla F(w)\\)\nUpdate: \\(w_{i+1} = w_i - \\eta_i \\nabla F(w_i)\\)\nConverges to a local minimum (global for convex losses)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#stochastic-gradient-descent",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#stochastic-gradient-descent",
    "title": "Tree-Based Methods",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nLogistic regression objective: log-loss + regularizer\nSGD uses one (or a mini-batch of) example(s) per step to approximate the gradient\nFaster on large data; noisier updates\n\nfrom sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier().fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-idea",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-idea",
    "title": "Tree-Based Methods",
    "section": "Gradient Boosting Idea",
    "text": "Gradient Boosting Idea\n\nTrain a small tree to predict \\(y\\)\nTrain next tree on residuals of previous model (or on points poorly predicted)\nRepeat; sum scaled predictions: \\(f(x)=\\sum \\gamma g_k(x)\\) with learning rate \\(\\gamma\\)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-algorithm",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-algorithm",
    "title": "Tree-Based Methods",
    "section": "Gradient Boosting Algorithm",
    "text": "Gradient Boosting Algorithm\n$ f_{1}(x) y $ $ f_{2}(x) y - f_{1}(x) $ $ f_{3}(x) y - f_{1}(x) - f_{2}(x) $\n\nEach new tree fixes remaining error\nSmall \\(\\gamma\\) (e.g., 0.1) for smoother learning"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-as-gradient-descent",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-as-gradient-descent",
    "title": "Tree-Based Methods",
    "section": "Gradient Boosting as Gradient Descent",
    "text": "Gradient Boosting as Gradient Descent\n\n\n\nLinear regression minimizes \\(\\sum (y - w^T x - b)^2\\)\nGradient descent updates weights\n\n\n\nGradient boosting minimises same loss over predictions \\(\\hat{y}\\)\nUpdate \\(\\hat{y}\\) by adding trees along negative gradient"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#regression-example",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#regression-example",
    "title": "Tree-Based Methods",
    "section": "Regression Example",
    "text": "Regression Example\n\n\n\n\nShallow trees fit residuals sequentially until residuals shrink"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#classification-example",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#classification-example",
    "title": "Tree-Based Methods",
    "section": "Classification Example",
    "text": "Classification Example\n\n\n\n\nProbability surfaces become sharper as trees accumulate\nMulticlass: one regression tree per class per step"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#early-stopping",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#early-stopping",
    "title": "Tree-Based Methods",
    "section": "Early Stopping",
    "text": "Early Stopping\n\nMore trees can overfit\nStop when validation metric stops improving\nEither fix \\(n_{\\text{estimators}}\\) and tune \\(\\gamma\\), or fix \\(\\gamma\\) and stop early"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#tuning-gradient-boosting",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#tuning-gradient-boosting",
    "title": "Tree-Based Methods",
    "section": "Tuning Gradient Boosting",
    "text": "Tuning Gradient Boosting\n\nUse shallow trees (strong pruning via max_depth)\nTune learning rate, n_estimators, subsampling of rows/columns\nOptional regularisation (min_samples_split, min_impurity_decrease)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#ensuring-uncorrelated-trees-aggressive-subsampling",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#ensuring-uncorrelated-trees-aggressive-subsampling",
    "title": "Tree-Based Methods",
    "section": "Ensuring uncorrelated trees: Aggressive Subsampling",
    "text": "Ensuring uncorrelated trees: Aggressive Subsampling\n\nRow and column subsampling reduce correlation and overfitting\nCommon in XGBoost/LightGBM"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#xgboost",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#xgboost",
    "title": "Tree-Based Methods",
    "section": "XGBoost",
    "text": "XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier().fit(X_train, y_train)\n\nFast, supports missing values, GPU/cluster training\nL1/L2 on leaves; fast approximate splits; sparse data friendly"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#lightgbm",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#lightgbm",
    "title": "Tree-Based Methods",
    "section": "LightGBM",
    "text": "LightGBM\nfrom lightgbm.sklearn import LGBMClassifier\nlgbm = LGBMClassifier().fit(X_train, y_train)\n\nNative categorical handling; missing values; GPU/cluster support"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#catboost",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#catboost",
    "title": "Tree-Based Methods",
    "section": "CatBoost",
    "text": "CatBoost\nfrom catboost.sklearn import CatBoostClassifier\ncatb = CatBoostClassifier().fit(X_train, y_train)\n\nStrong on categorical data; symmetric trees; target-encoding tricks; GPU"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#advantages-of-gradient-boosting",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#advantages-of-gradient-boosting",
    "title": "Tree-Based Methods",
    "section": "Advantages of Gradient Boosting",
    "text": "Advantages of Gradient Boosting\n\nOften more accurate than random forests with tuning\nSmall models; fast prediction\nHist/XGB/LightGBM implementations are very fast"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#when-to-use-tree-ensembles",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#when-to-use-tree-ensembles",
    "title": "Tree-Based Methods",
    "section": "When to Use Tree Ensembles",
    "text": "When to Use Tree Ensembles\n\nFor tabular data\nNeed non-linear relationships and minimal preprocessing\nSingle small tree for interpretability;\nRandom forests very robust, good benchmark\nGradient boosting for best accuracy when tuned"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#questions",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#questions",
    "title": "Tree-Based Methods",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#history",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#history",
    "title": "Neural Networks",
    "section": "History",
    "text": "History\n\nNearly everything we talk about today existed ~1990\nWhat changed?\n\nMore data\nFaster computers (GPUs)\nSome improvements: relu, dropout, adam, batch-normalization, residual networks"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#logistic-regression-as-neural-net",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#logistic-regression-as-neural-net",
    "title": "Neural Networks",
    "section": "Logistic Regression as Neural Net",
    "text": "Logistic Regression as Neural Net"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#basic-architecture",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#basic-architecture",
    "title": "Neural Networks",
    "section": "Basic Architecture",
    "text": "Basic Architecture\n\n\n\n\\(h(x) = f(W_1x+b_1)\\)\n\\(o(x) = g(W_2h(x) + b_2)\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#more-layers",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#more-layers",
    "title": "Neural Networks",
    "section": "More Layers",
    "text": "More Layers\n\n\n\n\nHidden layers usually all have the same non-linear function\nMany layers → “deep learning”\nMultilayer perceptron, feed-forward neural network, vanilla feed-forward neural network\nRegression: single output neuron with linear activation\nClassification: one-hot-encoding of classes, n_classes output variables with softmax"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#nonlinear-activation-functions",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#nonlinear-activation-functions",
    "title": "Neural Networks",
    "section": "Nonlinear Activation Functions",
    "text": "Nonlinear Activation Functions\n\n\n\n\nStandard choices: tanh or rectified linear unit (relu)\nTanh squashes between -1 and 1; saturates towards infinities\nReLU is constant zero for negative numbers, then identity"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#supervised-neural-networks",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#supervised-neural-networks",
    "title": "Neural Networks",
    "section": "Supervised Neural Networks",
    "text": "Supervised Neural Networks\n\nNon-linear models for classification and regression\nWork well for very large datasets\nNon-convex optimization\nNotoriously slow to train – need for GPUs\nUse dot products; require preprocessing similar to SVM or linear models, unlike trees\nMany variants: Convolutional nets, GRUs, LSTMs, recursive networks, VAEs, GANs, deep RL"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#training-objective",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#training-objective",
    "title": "Neural Networks",
    "section": "Training Objective",
    "text": "Training Objective\n\\(h(x) = f(W_1x+b_1)\\)\n\\(o(x) = g(W_2h(x)+b_2) = g(W_2f(W_1x + b_1) + b_2)\\)\n\\(\\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,o(x_i))\\)\n\\(= \\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,g(W_2f(W_1x+b_1)+b_2))\\)\n\n\\(l\\) = Squared loss for regression; Cross-entropy loss for classification"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#backpropagation",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#backpropagation",
    "title": "Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nNeed \\(\\frac{\\partial l(y, o)}{\\partial W_i}\\) and \\(\\frac{\\partial l(y, o)}{\\partial b_i}\\)\n\n\\(\\text{net}(x) := W_1x + b_1\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#gradient-computation",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#gradient-computation",
    "title": "Neural Networks",
    "section": "Gradient Computation",
    "text": "Gradient Computation\n\nBackpropagation is clever application of chain rule for derivatives\nSingle backward pass from output to input computes derivatives\nNot an optimization algorithm, just a way to compute gradients"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#relu-differentiability",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#relu-differentiability",
    "title": "Neural Networks",
    "section": "ReLU Differentiability",
    "text": "ReLU Differentiability\n\n\n\n\nReLU not differentiable at zero\nUse subgradient descent; any gradient below function works\nIn practice, never hit zero with floating point numbers"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#optimizing-w-b",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#optimizing-w-b",
    "title": "Neural Networks",
    "section": "Optimizing W, b",
    "text": "Optimizing W, b\nBatch \\(W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=1}^N \\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)\nOnline/Stochastic \\(W_i \\leftarrow W_i - \\eta\\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)\nMinibatch \\(W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=k}^{k+m} \\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#learning-heuristics",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#learning-heuristics",
    "title": "Neural Networks",
    "section": "Learning Heuristics",
    "text": "Learning Heuristics\n\nConstant \\(\\eta\\) not good\nCan decrease \\(\\eta\\) over time\nBetter: adaptive \\(\\eta\\) for each entry of \\(W_i\\)\nState-of-the-art: adam (with magic numbers)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#picking-optimization-algorithms",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#picking-optimization-algorithms",
    "title": "Neural Networks",
    "section": "Picking Optimization Algorithms",
    "text": "Picking Optimization Algorithms\n\nSmall dataset: off the shelf like l-bfgs\nBig dataset: adam / rmsprop\nHave time & nerve: tune the schedule"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#neural-nets-with-sklearn",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#neural-nets-with-sklearn",
    "title": "Neural Networks",
    "section": "Neural Nets with sklearn",
    "text": "Neural Nets with sklearn\n\n\n\nmlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\nprint(mlp.score(X_train, y_train))\nprint(mlp.score(X_test, y_test))\n\nDon’t use sklearn for anything but toy problems in neural nets"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#random-state",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#random-state",
    "title": "Neural Networks",
    "section": "Random State",
    "text": "Random State\n\n\n\n\nNetwork is way over capacity and can overfit in many ways\nRegularization might make it less dependent on initialization"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#hidden-layer-size",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#hidden-layer-size",
    "title": "Neural Networks",
    "section": "Hidden Layer Size",
    "text": "Hidden Layer Size\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,), random_state=10)\nmlp.fit(X_train, y_train)\n\n\n\n\nSingle hidden layer with 5 units\nEach unit corresponds to different part of decision boundary"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#multiple-hidden-layers",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#multiple-hidden-layers",
    "title": "Neural Networks",
    "section": "Multiple Hidden Layers",
    "text": "Multiple Hidden Layers\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10), random_state=0)\nmlp.fit(X_train, y_train)\n\n\n\n\n3 hidden layers each of size 10\nMain way to control complexity"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#activation-functions",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#activation-functions",
    "title": "Neural Networks",
    "section": "Activation Functions",
    "text": "Activation Functions\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10),\n                    activation='tanh', random_state=0)\nmlp.fit(X_train, y_train)\n\n\n\n\nUsing tanh gives smoother boundaries\nReLU doesn’t work as well with l-bfgs on small networks\nFor large networks, relu is preferred"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#regression",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#regression",
    "title": "Neural Networks",
    "section": "Regression",
    "text": "Regression\n\n\n\nfrom sklearn.neural_network import MLPRegressor\nmlp_relu = MLPRegressor(solver=\"lbfgs\").fit(X, y)\nmlp_tanh = MLPRegressor(solver=\"lbfgs\", activation='tanh').fit(X, y)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#complexity-control",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#complexity-control",
    "title": "Neural Networks",
    "section": "Complexity Control",
    "text": "Complexity Control\n\nNumber of parameters\nRegularization\nEarly Stopping\nDropout"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#grid-searching-neural-nets",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#grid-searching-neural-nets",
    "title": "Neural Networks",
    "section": "Grid-Searching Neural Nets",
    "text": "Grid-Searching Neural Nets\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, stratify=data.target, random_state=0)\n\nfrom sklearn.model_selection import GridSearchCV\npipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\"lbfgs\", random_state=1))\nparam_grid = {'mlpclassifier__alpha': np.logspace(-3, 3, 7)}\ngrid = GridSearchCV(pipe, param_grid)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#searching-hidden-layer-sizes",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#searching-hidden-layer-sizes",
    "title": "Neural Networks",
    "section": "Searching Hidden Layer Sizes",
    "text": "Searching Hidden Layer Sizes\nfrom sklearn.model_selection import GridSearchCV\npipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\"lbfgs\", random_state=1))\nparam_grid = {'mlpclassifier__hidden_layer_sizes':\n              [(10,), (50,), (100,), (500,), (10, 10), (50, 50), (100, 100), (500, 500)]}\ngrid = GridSearchCV(pipe, param_grid)\ngrid.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#write-your-own-neural-networks",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#write-your-own-neural-networks",
    "title": "Neural Networks",
    "section": "Write Your Own Neural Networks",
    "text": "Write Your Own Neural Networks\nclass NeuralNetwork(object):\n    def __init__(self):\n        # initialize coefficients and biases\n        pass\n    def forward(self, x):\n        activation = x\n        for coef, bias in zip(self.coef_, self.bias_):\n            activation = self.nonlinearity(np.dot(activation, coef) + bias)\n        return activation\n    def backward(self, x):\n        # compute gradient of stuff in forward pass\n        pass"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#autodiff",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#autodiff",
    "title": "Neural Networks",
    "section": "Autodiff",
    "text": "Autodiff\nclass array(object) :\n    \"\"\"Simple Array object that support autodiff.\"\"\"\n    def __init__(self, value, name=None):\n        self.value = value\n        if name:\n            self.grad = lambda g : {name : g}\n    def __add__(self, other):\n        assert isinstance(other, int)\n        ret = array(self.value + other)\n        ret.grad = lambda g : self.grad(g)\n        return ret\n    def __mul__(self, other):\n        assert isinstance(other, array)\n        ret = array(self.value * other.value)\n        def grad(g):\n            x = self.grad(g * other.value)\n            x.update(other.grad(g * self.value))\n            return x\n        ret.grad = grad\n        return ret"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#autodiff-example",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#autodiff-example",
    "title": "Neural Networks",
    "section": "Autodiff Example",
    "text": "Autodiff Example\na = array(np.array([1, 2]), 'a')\nb = array(np.array([3, 4]), 'b')\nc = b * a\nd = c + 1\nprint(d.value)\nprint(d.grad(1))\n[4 9]\n{'b': array([1, 2]), 'a': array([3, 4])}\n\nAutomatic differentiation avoids writing gradients manually\nKeep track of computation while executing forward pass\nHard-code derivative for each operation (no symbolic differentiation)\nBuild computation graph automatically"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#gpu-support",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#gpu-support",
    "title": "Neural Networks",
    "section": "GPU Support",
    "text": "GPU Support\n\n\n\n\nImportant limitation: GPUs have much less memory than RAM\nMemory copies between RAM and GPU are expensive"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#computation-graph",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#computation-graph",
    "title": "Neural Networks",
    "section": "Computation Graph",
    "text": "Computation Graph\n\n\n\n\nStore different intermediate results depending on derivatives needed\nGiven limited GPU memory, important to know what to cache/discard\nHelps with visual debugging and understanding network structure"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#deep-learning-framework-requirements",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#deep-learning-framework-requirements",
    "title": "Neural Networks",
    "section": "Deep Learning Framework Requirements",
    "text": "Deep Learning Framework Requirements\n\nAutodiff\nGPU support\nOptimization and inspection of computation graph\nOn-the-fly generation of computation graph (optional)\nDistribution over multiple GPUs and/or cluster (optional)\n\nCurrent choices: TensorFlow, PyTorch / Torch, Chainer"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#deep-learning-libraries",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#deep-learning-libraries",
    "title": "Neural Networks",
    "section": "Deep Learning Libraries",
    "text": "Deep Learning Libraries\n\nKeras (TensorFlow, CNTK, Theano)\nPyTorch (torch)\nChainer (chainer)\nMXNet (MXNet)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#quick-look-at-tensorflow",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#quick-look-at-tensorflow",
    "title": "Neural Networks",
    "section": "Quick Look at TensorFlow",
    "text": "Quick Look at TensorFlow\n\n“Down to the metal” - don’t use for everyday tasks\nThree steps for learning:\n\nBuild computation graph (using array operations and functions)\nCreate Optimizer (gradient descent, adam, etc.) attached to graph\nRun actual computation\n\nEager mode (default in TensorFlow 2.0): write imperative code directly"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#pytorch-example",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#pytorch-example",
    "title": "Neural Networks",
    "section": "PyTorch Example",
    "text": "PyTorch Example\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n\nN = 100\nx = torch.randn(N, 1, device=device, dtype=dtype)\ny = torch.randn(N, 1, device=device, dtype=dtype)\nw = torch.randn(D_in, H, device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    y_pred = x.mm(w1)\n    loss = (y_pred - y).pow(2).sum().item()\n    loss.backward()\n    w1 -= learning_rate * w1.grad\n    w1.grad.zero_()"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#best-practices",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#best-practices",
    "title": "Neural Networks",
    "section": "Best Practices",
    "text": "Best Practices\n\nDon’t go down to the metal unless you have to!\nDon’t write TensorFlow, write Keras!\nDon’t write PyTorch, write pytorch.nn or FastAI (or Skorch or ignite)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#questions",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#questions",
    "title": "Neural Networks",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#principles",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#principles",
    "title": "Graph Neural Networks",
    "section": "Principles",
    "text": "Principles\n\nA deep learning framework for graph-structured data\nIt generalises traditional neural networks to handle graph data by leveraging the relationships between nodes in a graph.\nMessage passing: nodes aggregate neighbor features over the graph\nSpatial vs spectral views of convolution on graphs\nTransductive vs inductive learning; sampling enables scalability\nLearnable transformations per layer; nonlinearity and normalization matter\nVariants differ in how neighbors are aggregated and weighted"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#five-major-gnn-architectures",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#five-major-gnn-architectures",
    "title": "Graph Neural Networks",
    "section": "Five Major GNN Architectures",
    "text": "Five Major GNN Architectures\n\n\n\nDeepWalk: unsupervised node embeddings via random walks (word2vec-like)\nGCN: shared filters; normalised neighborhood aggregation (spectral → spatial)\nGraphSAGE: inductive sampling + learnable aggregators (mean/pool/LSTM)\nGAT: attention over neighbors; learns edge weights; multi-head for capacity\nChebNet: spectral filtering via Chebyshev polynomials; localized convolutions"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graph-convolutional-networks-gcn",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graph-convolutional-networks-gcn",
    "title": "Graph Neural Networks",
    "section": "Graph Convolutional Networks (GCN)",
    "text": "Graph Convolutional Networks (GCN)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#introduction-graphs",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#introduction-graphs",
    "title": "Graph Neural Networks",
    "section": "Introduction: Graphs",
    "text": "Introduction: Graphs\n\n\n\nGraph = organized data representation\nConsists of vertices (nodes) V and edges E\nEdges can be weighted or binary\nDirected or undirected graphs\n\nExample graph:\n\\[V = \\{A, B, C, D, E, F, G\\}\\] \\[E = \\{(A,B), (B,C), (C,E), ...\\}\\]"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graph-terminology",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graph-terminology",
    "title": "Graph Neural Networks",
    "section": "Graph Terminology",
    "text": "Graph Terminology\n\n\n\n\nNode: An entity in the graph (represented by circles)\nEdge: Line joining two nodes (represents relationships)\nDegree: Number of edges incident with a vertex\nAdjacency Matrix: N×N matrix representing graph structure"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#why-gcns",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#why-gcns",
    "title": "Graph Neural Networks",
    "section": "Why GCNs?",
    "text": "Why GCNs?\n\nMost real-world datasets come as graphs or networks:\n\nSocial networks\nProtein-interaction networks\nThe World Wide Web\n\nLearning on graphs enables domain-specific insights\nConventional CNNs assume compositional structure on Euclidean space"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#cnns-vs-gcns",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#cnns-vs-gcns",
    "title": "Graph Neural Networks",
    "section": "CNNs vs GCNs",
    "text": "CNNs vs GCNs\n\n\nCNN Key Properties:\n\nLocality\nStationarity (Translation Invariance)\nMulti-scale hierarchies\n\nProblem: Not all data lies on Euclidean space!"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#applications-of-gcns",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#applications-of-gcns",
    "title": "Graph Neural Networks",
    "section": "Applications of GCNs",
    "text": "Applications of GCNs\n\n\n\nFacebook Link Prediction for Suggesting Friends using Social Networks\n\n\n\nFriend prediction algorithms\nSocial network analysis\nProtein interaction prediction\nKnowledge graph completion"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#what-are-gcns",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#what-are-gcns",
    "title": "Graph Neural Networks",
    "section": "What are GCNs?",
    "text": "What are GCNs?\n\nNeural networks operating on graphs\nCapture neighbourhood information for non-euclidean spaces\nRe-define convolution for graph domains\n\nTwo Styles:\n\nSpectral GCNs: Graph signal processing perspective\nSpatial GCNs: Aggregate feature information from neighbours (more flexible)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#how-gcns-work-friend-prediction",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#how-gcns-work-friend-prediction",
    "title": "Graph Neural Networks",
    "section": "How GCNs Work: Friend Prediction",
    "text": "How GCNs Work: Friend Prediction\n\n\nProblem: Predict future friendships\n\nGraph where edges = friendships\nCommon friends → higher likelihood\n\\((1,3)\\) have 2 common friends\n\\((1,5)\\) have 0 common friends"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-mathematical-formulation",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-mathematical-formulation",
    "title": "Graph Neural Networks",
    "section": "GCN Mathematical Formulation",
    "text": "GCN Mathematical Formulation\nLayer-wise propagation:\n\\[H^{i} = f(H^{i-1}, A)\\]\nSimple example:\n\\[f(H^{i}, A) = σ(AH^{i}W^{i})\\]\nwhere:\n\n\\(A\\) = N × N adjacency matrix\n\\(X\\) = input feature matrix (N × F)\n\\(σ\\) = ReLU activation function\n\\(H^{0} = X\\) (initial features)\nEach layer aggregates neighborhood features"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#problems-with-simple-formulation",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#problems-with-simple-formulation",
    "title": "Graph Neural Networks",
    "section": "Problems with Simple Formulation",
    "text": "Problems with Simple Formulation\n\n\nProblem 1: No self-representation\n\nNew features don’t include node’s own features\nSolution: Add self-loops\n\nProblem 2: Degree scaling\n\nHigh-degree nodes get larger values\nLow-degree nodes get smaller values\nSolution: Normalize by degree\n\n\nFixes:\n\nAdd identity: \\(\\hat{A} = A + I\\)\nSymmetric normalization:\n\n\\[\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}\\]\nwhere \\(\\hat{D}\\) is degree matrix of \\(\\hat{A}\\)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#final-gcn-propagation-rule",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#final-gcn-propagation-rule",
    "title": "Graph Neural Networks",
    "section": "Final GCN Propagation Rule",
    "text": "Final GCN Propagation Rule\n\\[f(H^{(l)}, A) = \\sigma\\left( \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}\\right)\\]\nwhere:\n\n\\(\\hat{A} = A + I\\) (adjacency + self-loops)\n\\(I\\) = identity matrix\n\\(\\hat{D}\\) = diagonal degree matrix of \\(\\hat{A}\\)\n\\(W^{(l)}\\) = learnable weight matrix for layer \\(l\\)\n\\(\\sigma\\) = activation function"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graphsage-sample-and-aggregate",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graphsage-sample-and-aggregate",
    "title": "Graph Neural Networks",
    "section": "GraphSAGE (Sample and Aggregate)",
    "text": "GraphSAGE (Sample and Aggregate)\n\n\n\nInductive learning: generalises to unseen nodes/graphs\nNeighborhood sampling for scalability (mini-batches)\nAggregators: mean, pool (MLP+max), LSTM\nConcatenate self-representation with aggregated neighbors\nNormalise embeddings; learn weights per layer\n\n\n\n\n\nInductive representation learning on large graphs"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graphsage-aggregators",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graphsage-aggregators",
    "title": "Graph Neural Networks",
    "section": "GraphSAGE Aggregators",
    "text": "GraphSAGE Aggregators\n\n\n\nMean aggregator: elementwise mean over neighbors\nPool aggregator: transform neighbor features, then max-pool\nLSTM aggregator: sequence model over randomly ordered neighbors"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gat",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gat",
    "title": "Graph Neural Networks",
    "section": "GAT",
    "text": "GAT\n\n\n\nLearn attention weights over neighbors; masked self-attention\nReplaces fixed normalization in GCN with learned coefficients\nMulti-head attention: concatenate intermediate heads; average at output\nImproves interpretability via attention visualization; handles heterophily better"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-vs-gat-aggregation",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-vs-gat-aggregation",
    "title": "Graph Neural Networks",
    "section": "GCN vs GAT Aggregation",
    "text": "GCN vs GAT Aggregation"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gat-multi-head-attention",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gat-multi-head-attention",
    "title": "Graph Neural Networks",
    "section": "GAT: Multi-head Attention",
    "text": "GAT: Multi-head Attention\n\n\n\nConcatenate intermediate heads; average at final layer"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#implementation-in-pytorch",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#implementation-in-pytorch",
    "title": "Graph Neural Networks",
    "section": "Implementation in PyTorch",
    "text": "Implementation in PyTorch"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-convolutional-layer",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-convolutional-layer",
    "title": "Graph Neural Networks",
    "section": "GCN Convolutional Layer",
    "text": "GCN Convolutional Layer\nclass GCNConv(nn.Module):\n    def __init__(self, A, in_channels, out_channels):\n        super(GCNConv, self).__init__()\n        self.A_hat = A + torch.eye(A.size(0))\n        self.D     = torch.diag(torch.sum(A,1))\n        self.D     = self.D.inverse().sqrt()\n        self.A_hat = torch.mm(torch.mm(self.D, self.A_hat), self.D)\n        self.W     = nn.Parameter(torch.rand(in_channels,out_channels))\n    \n    def forward(self, X):\n        out = torch.relu(torch.mm(torch.mm(self.A_hat, X), self.W))\n        return out"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-network-architecture",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-network-architecture",
    "title": "Graph Neural Networks",
    "section": "GCN Network Architecture",
    "text": "GCN Network Architecture\nclass Net(torch.nn.Module):\n    def __init__(self, A, nfeat, nhid, nout):\n        super(Net, self).__init__()\n        self.conv1 = GCNConv(A, nfeat, nhid)\n        self.conv2 = GCNConv(A, nhid, nout)\n        \n    def forward(self, X):\n        H  = self.conv1(X)\n        H2 = self.conv2(H)\n        return H2\n\nStack multiple GCN layers\nLearn hierarchical representations\nOutput layer for classification/regression"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#case-study-zacharys-karate-club",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#case-study-zacharys-karate-club",
    "title": "Graph Neural Networks",
    "section": "Case Study: Zachary’s Karate Club",
    "text": "Case Study: Zachary’s Karate Club\n\n\nHistorical Context (1970-1972):\n\nObserved local karate club\nConflict between administrator “John A” and instructor “Mr. Hi”\nClub split into two groups\nPredict which members join which group"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#semi-supervised-learning-setup",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#semi-supervised-learning-setup",
    "title": "Graph Neural Networks",
    "section": "Semi-Supervised Learning Setup",
    "text": "Semi-Supervised Learning Setup\n\nLabels known for only 2 nodes: John A (0) and Mr. Hi (1)\nPredict labels for all other members based on graph structure\nUse graph connectivity to propagate information\n\n# Only nodes 0 and 33 are labeled\ntarget = torch.tensor([0,-1,-1,-1,...,-1,-1,1])\n\n# Feature matrix (one-hot encoding)\nX = torch.eye(A.size(0))"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#training-the-model",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#training-the-model",
    "title": "Graph Neural Networks",
    "section": "Training the Model",
    "text": "Training the Model\n# Initialize network\nmodel = Net(A, X.size(0), 10, 2)\n\n# Loss and optimizer\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Training loop\nfor epoch in range(200):\n    optimizer.zero_grad()\n    loss = criterion(model(X), target)\n    loss.backward()\n    optimizer.step()"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#training-visualization",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#training-visualization",
    "title": "Graph Neural Networks",
    "section": "Training Visualization",
    "text": "Training Visualization\n\n\n\nNode embeddings learned during training\n\n\n\nModel successfully separates two groups\nClose to actual predictions (except node 9)\nSemi-supervised learning works!"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#pytorch-geometric",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#pytorch-geometric",
    "title": "Graph Neural Networks",
    "section": "PyTorch Geometric",
    "text": "PyTorch Geometric\n\n\nPyTorch Geometric (PyG):\n\nDedicated library for graph deep learning\nEasy, fast, and simple implementation\nBuilt for PyTorch users\nActive development and community\n\nFeatures:\n\nPre-built GCN layers\nVarious graph datasets\nEfficient sparse operations\n\n\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(num_features, 16)\n        self.conv2 = GCNConv(16, num_classes)\n        \n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#key-takeaways",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#key-takeaways",
    "title": "Graph Neural Networks",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nGCNs extend neural networks to non-Euclidean graph data\nAggregate and transform neighbourhood information\nAddress self-representation and degree scaling issues\nSemi-supervised learning on graphs is powerful\nApplications: social networks, molecules, knowledge graphs\n\nCritical Question: How powerful are GCNs really?\nRead: How powerful are Graph Convolutions?"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#resources",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#resources",
    "title": "Graph Neural Networks",
    "section": "Resources",
    "text": "Resources\n\ngraphnet Github repo\n\nKey Papers:\n\nSemi-Supervised Classification with GCNs - Kipf & Welling (2017)\nThomas Kipf’s Blog on GCNs\n\nLibraries:\n\nPyTorch Geometric\nDeep Graph Library (DGL)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#questions",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#questions",
    "title": "Graph Neural Networks",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#what-we-learnt-in-term-1",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#what-we-learnt-in-term-1",
    "title": "Introduction to machine learning",
    "section": "What we learnt in Term 1",
    "text": "What we learnt in Term 1\n\nPython programming\nData types\nVisualisation\nRegression (Ordinary Least Square; Linear Mixed Effects; multicollinearity)\nDimensionality reduction\nClustering"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#learning-objectives",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#learning-objectives",
    "title": "Introduction to machine learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand the basics and classifications of machine learning\nUnderstand the differences between statistical methods and machine learning (estimation vs. prediction)\nAppreciate GIGO theorems in machine learning and data science"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#ml-as-subset-of-ai",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#ml-as-subset-of-ai",
    "title": "Introduction to machine learning",
    "section": "ML as subset of AI",
    "text": "ML as subset of AI\n\n\n\nMachine learning (decision tree, random forest, k-means, etc.)\n\nDeep learning (deep neural networks)\n\nOther AI tools: graphical models, symbolic AI\n\nNote: we don’t distinguish ML/DL and consider NN as part of ML\n\n\n\n\n\nImage Credit: Lecture slide (ML is a subset of AI)"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#definition-of-machine-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#definition-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Definition of machine learning",
    "text": "Definition of machine learning\n\nArthur Samuel (1959): (Machine learning is the) field of study that gives computers the ability to learn without being explicitly programmed.\n\n\nTom Mitchell (1997): A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#what-is-machine-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#what-is-machine-learning",
    "title": "Introduction to machine learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nExtracting knowledge from data\nRelying on data and algorithms\nClosely related to statistics but distinct from linear models\nFocus on prediction rather than estimating relationships or interpretation"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook",
    "text": "Examples: Facebook\n\n\n\n\nMachine learning in news feed ranking\nContent selection and targeting\nAds and recommendations"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nFace detection and recognition\nPhoto organization"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.-1",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.-1",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nPhoto selection and layout"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-amazon",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-amazon",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon",
    "text": "Examples: Amazon\n\n\n\n\nProduct ranking\nPersonalised recommendations\nAds selection"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-amazon-cont.",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-amazon-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon (cont.)",
    "text": "Examples: Amazon (cont.)\n\n\n\n\nSeller selection\nDefault choices\nRelated products"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#science-applications",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#science-applications",
    "title": "Introduction to machine learning",
    "section": "Science Applications",
    "text": "Science Applications\n\n\n\n\nPersonalised cancer treatment\nMedical diagnosis\nDrug discovery\nHiggs boson discovery\nExoplanet detection"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#types-of-machine-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#types-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nSupervised Learning: Learn from input-output pairs (with labelled data)\nUnsupervised Learning: Discover structure in data (without labelled data)\nReinforcement Learning: Learn through interaction with environment (with an actual/simulated environment)"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#supervised-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in \\mathbb{R} \\\\\nf(x_i) &\\approx y_i\n\\end{aligned}\n\\]\nLearn a function \\(f\\) from input-output pairs to predict on new data."
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#generalisation-to-unseen-data",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#generalisation-to-unseen-data",
    "title": "Introduction to machine learning",
    "section": "Generalisation to unseen data",
    "text": "Generalisation to unseen data\n\nGoal: \\(f(x_i) \\approx y_i\\) on training data\nMore important: \\(f(x) \\approx y\\) on new data\nCore distinction: not just function approximation, but prediction on unseen data\nAvoiding overfitting to training data is key"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-of-supervised-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-of-supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Examples of Supervised Learning",
    "text": "Examples of Supervised Learning\n\nSpam detection\nMedical diagnosis\nAd click prediction"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#unsupervised-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\\[\nx_i \\sim p(x) \\text{ i.i.d.}\n\\]\nLearn about the distribution \\(p\\):\n\nClustering\nDimensionality reduction\nTopic modeling\nOutlier detection"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#other-types-of-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#other-types-of-learning",
    "title": "Introduction to machine learning",
    "section": "Other Types of Learning",
    "text": "Other Types of Learning\n\nSemi-supervised\nActive Learning\nForecasting\nTransfer learning\nIf you understand supervised/unsupervised/reinforcement learning, others are easier to grasp"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name",
    "title": "Introduction to machine learning",
    "section": "Don’t read a book by its name",
    "text": "Don’t read a book by its name\n\nClassifications of neighbourhoods (e.g. London output area classification) that are actually clustering\nAnomaly detection methods can be unsupervised or supervised learning\nForecasting can be done with lagged features (via supervised learning), or with time series data (via time series analysis), or both"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#what-does-llm-belong-to",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#what-does-llm-belong-to",
    "title": "Introduction to machine learning",
    "section": "What does LLM belong to?",
    "text": "What does LLM belong to?\n\n\n\nImage Credit: medium.com"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#classification-vs.-regression",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#classification-vs.-regression",
    "title": "Introduction to machine learning",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression\n\n\nClassification\n\nTarget \\(y\\) is discrete\nExample: Is this patient sick?\n\n\nRegression\n\nTarget \\(y\\) is continuous\nExample: How long to recover?"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#relationship-to-statistics",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#relationship-to-statistics",
    "title": "Introduction to machine learning",
    "section": "Relationship to Statistics",
    "text": "Relationship to Statistics\n\n\nStatistics\n\nModel first\nEstimation emphasis\nYes/no questions\nWith many assumptions, need to test\nInterpretation is key\ne.g. Does smoking lead to lung cancer?\n\n\nMachine Learning\n\nData first\nPrediction emphasis\nFuture predictions\nFew assumptions (but not assumption-free)\nInterpretation isn’t primary\ne.g. Predict tomorrow’s weather"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#following-statements-are-not-recommended",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#following-statements-are-not-recommended",
    "title": "Introduction to machine learning",
    "section": "Following statements are not recommended",
    "text": "Following statements are not recommended\n\n“Linear regression is not needed anymore because of machine learning.”\n“Machine learning is just a fad; statistics is more important.”\n“Machine learning models are black boxes; we can’t interpret them at all.”\n“Machine learning models are as interpretable as linear models.”\n“Machine learning models have no assumptions.”"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#guiding-principles-goal-considerations",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#guiding-principles-goal-considerations",
    "title": "Introduction to machine learning",
    "section": "Guiding Principles: Goal Considerations",
    "text": "Guiding Principles: Goal Considerations\n\nDefine the goal clearly\nDefine how to measure success\nThink about context and baseline\nAsk: what’s the benefit?"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#thinking-in-context",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#thinking-in-context",
    "title": "Introduction to machine learning",
    "section": "Thinking in Context",
    "text": "Thinking in Context\n\nWhat do you want to achieve?\nWhat’s the baseline and its performance?\nWhat improvement over baseline do you need?"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#good-and-bad-substitutes",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#good-and-bad-substitutes",
    "title": "Introduction to machine learning",
    "section": "Good and Bad Substitutes",
    "text": "Good and Bad Substitutes\n\nChoose metrics carefully\nSubstitute metrics can be misleading\nOptimize for the right goal\nUnderstand side effects"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#communicating-results",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#communicating-results",
    "title": "Introduction to machine learning",
    "section": "Communicating Results",
    "text": "Communicating Results\n\nExplain why your approach works\nCommunicate uncertainty\nShow impact and limitations\nConvince stakeholders"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#explainable-results",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#explainable-results",
    "title": "Introduction to machine learning",
    "section": "Explainable Results",
    "text": "Explainable Results\n\n\n\n\nUsers want to know why recommendations are made\nExplainability improves engagement\nImportant for trust and transparency"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#ethical-considerations",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#ethical-considerations",
    "title": "Introduction to machine learning",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\n\n\n\nBias in risk assessments\nFairness in automated decisions\nTransparency and accountability"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#ethics-its-in-the-application",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#ethics-its-in-the-application",
    "title": "Introduction to machine learning",
    "section": "Ethics: It’s in the Application!",
    "text": "Ethics: It’s in the Application!\n\nUnderstand biases in your system\nConsider the impact of predictions\nUse algorithms responsibly\nSame algorithm, different outcomes"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#data-and-data-collection",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#data-and-data-collection",
    "title": "Introduction to machine learning",
    "section": "Data and Data Collection",
    "text": "Data and Data Collection\n\nCritical component of ML\nMore data usually helps (if from right source)\nConsider marginal cost vs. marginal benefit"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#free-vs-expensive-data",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#free-vs-expensive-data",
    "title": "Introduction to machine learning",
    "section": "Free vs Expensive Data",
    "text": "Free vs Expensive Data\n\n\nFree Data\n\nOpen data from gov and census, often aggregated (e.g. ONS, London Fire Brigade, NASA)\nOpen data from companies (e.g. Google Street View) (Read the license first)\nSynthetic data\nWeb scraping (be careful with legality and ethics)\n\n\nExpensive Data\n\nIndividual data (e.g. health records)\nMobile phone data (high cost)\nUser survey\nHigh-resolution imagery (e.g. satellite, aerial, drone)"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#big-data-considerations",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#big-data-considerations",
    "title": "Introduction to machine learning",
    "section": "Big Data Considerations",
    "text": "Big Data Considerations\n\nMore data can be more expensive to work with\nSubsample to RAM when possible (512GB available in cloud)\nRuntime and analyst time matter\nAlways try with a small sample first"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#theorem-garbage-in-garbage-out-gigo",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#theorem-garbage-in-garbage-out-gigo",
    "title": "Introduction to machine learning",
    "section": "Theorem: Garbage in, garbage out (GIGO)",
    "text": "Theorem: Garbage in, garbage out (GIGO)\n\nGreat algorithms + bad data = bad results\n\n\n\n\nModel performance is constrained by data quality.\nBiased, noisy, or incomplete data leads to misleading predictions.\n\n\n\n\n\nImage Credit: x.com/xschelling/status/954936528555429888"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#good-data-large-size-high-quality.",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#good-data-large-size-high-quality.",
    "title": "Introduction to machine learning",
    "section": "Good data = large size + high quality.",
    "text": "Good data = large size + high quality.\n\n\n\nSufficient sample size to capture variability in the problem\n\nHigh-quality labels and accurate measurements.\n\nRepresentative of the population and application context.\n\n\n\n\n\nImage Credit: Internet"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#data-size-and-performance",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#data-size-and-performance",
    "title": "Introduction to machine learning",
    "section": "Data size and performance",
    "text": "Data size and performance\n\n\nThe performance of ML/DL increases rapidly with the size of the data:\n\nLarge neural nets benefit the most from big data.\n\nMedium and small neural nets also improve with more data.\n\nTraditional ML algorithms (e.g. random forest, SVM) may saturate earlier.\n\n\n\n\n\nPerformance vs data size for ML/DL models"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#feature-engineering-eda-90-time",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#feature-engineering-eda-90-time",
    "title": "Introduction to machine learning",
    "section": "Feature engineering & EDA (~90% time)",
    "text": "Feature engineering & EDA (~90% time)\n\nMostly manual rather than automated (no automated methods for EDA)\nDomain knowledge from expertise is desirable\nUsing visualisation to identify patterns & data relationship\nRemoving noisy or erroneous data\n\nDealing with missing data\n\nGenerating new features by combining existing ones"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#example-representation-of-geospatial-locations-in-ml-models",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#example-representation-of-geospatial-locations-in-ml-models",
    "title": "Introduction to machine learning",
    "section": "Example: Representation of geospatial locations in ML models",
    "text": "Example: Representation of geospatial locations in ML models\n\nRaw coordinates (e.g. long/lat) may not be directly useful\nNeed to engineer features that capture spatial relationships\nLong/lat\nDistance to POIs (train stations/schools).\nUsing adjacency matrix between spatial units"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#last-week",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#last-week",
    "title": "Supervised learning workflow",
    "section": "Last week",
    "text": "Last week\n\nFramework of supervised learning\nEvaluation metrics for regression and classification"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#objectives-of-this-week",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#objectives-of-this-week",
    "title": "Supervised learning workflow",
    "section": "Objectives of this week",
    "text": "Objectives of this week\n\nUnderstand different workflow of supervised learning.\nUnderstand train-test split and train-validation-test split.\nUnderstand cross validation and its extensions\nKnow when to use different model evaluation methods."
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#supervised-learning",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#supervised-learning",
    "title": "Supervised learning workflow",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in\n\\begin{cases}\n\\mathbb{R}, & \\text{(regression)} \\\\\n\\mathcal{Y} \\text{ (finite set)}, & \\text{(classification)}\n\\end{cases} \\\\\n\\text{learn } f(x_i) &\\approx y_i \\\\\n\\text{such that } f(x) &\\approx y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#challenges-of-training-supervised-models",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#challenges-of-training-supervised-models",
    "title": "Supervised learning workflow",
    "section": "Challenges of training supervised models",
    "text": "Challenges of training supervised models\n\nGeneralise to new data\nAvoid overfitting/underfitting\nModel selection (hyperparameter tuning)"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-7525",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-7525",
    "title": "Supervised learning workflow",
    "section": "Train-Test Split (75/25)",
    "text": "Train-Test Split (75/25)"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#why-train-test-split",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#why-train-test-split",
    "title": "Supervised learning workflow",
    "section": "Why Train-Test Split?",
    "text": "Why Train-Test Split?\n\n\nStatistics\n\nNo train-test split\nTrain and evaluate on whole data\nEstimation is key\nLow model complexity\n\n\nMachine Learning\n\nTrain-test split\nTrain on part, evaluate on held-out part\nPrediction (Generalisation) is key\nHigh model complexity; overfitting risk"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#influence-of-n_neighbors-k3",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#influence-of-n_neighbors-k3",
    "title": "Supervised learning workflow",
    "section": "Influence of n_neighbors (k=3)",
    "text": "Influence of n_neighbors (k=3)\n\nLarger k → smoother boundary\nSmaller k → complex boundary"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting",
    "title": "Supervised learning workflow",
    "section": "Overfitting vs Underfitting",
    "text": "Overfitting vs Underfitting"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting-2",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting-2",
    "title": "Supervised learning workflow",
    "section": "Overfitting vs Underfitting",
    "text": "Overfitting vs Underfitting"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#so-far-happy-ending",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#so-far-happy-ending",
    "title": "Supervised learning workflow",
    "section": "So far: Happy ending?",
    "text": "So far: Happy ending?\n\nReport: best k=19, test accuracy=0.77\nGood for choosing k\nBut: overly optimistic for generalisation\nProblem: test set used for both choosing k and final evaluation"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#why-overfitting-the-validation-set",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#why-overfitting-the-validation-set",
    "title": "Supervised learning workflow",
    "section": "Why? overfitting the Validation Set",
    "text": "Why? overfitting the Validation Set\n\n\n\n\nInteresting reading Preventing Overfitting in cross-validation - Ng 1997"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-the-validation-set",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-the-validation-set",
    "title": "Supervised learning workflow",
    "section": "Overfitting the Validation Set",
    "text": "Overfitting the Validation Set"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#threefold-split-code",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#threefold-split-code",
    "title": "Supervised learning workflow",
    "section": "Threefold Split (Code)",
    "text": "Threefold Split (Code)\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=0)\n\nval_scores = []\nneighbors = np.arange(1, 15, 2)\nfor i in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    val_scores.append(knn.score(X_val, y_val))\nprint(f\"best validation score: {np.max(val_scores):.3}\\n\")\nbest_n_neighbors = neighbors[np.argmax(val_scores)]\nprint(f\"best n_neighbors:{best_n_neighbors}\\n\")\n\nknn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\nknn.fit(X_trainval, y_trainval)\nprint(f\"test-set score: {knn.score(X_test, y_test):.3f}\")\nbest validation score: 0.991 best n_neighbors: 11 test-set score: 0.951\n\nHere is an implementation of the three-fold split for selecting the number of neighbors. For each number of neighbors that we want to try, we build a model on the training set, and evaluate it on the validation set. We then pick the best validation set score, here that’s 99.1%, achieved when using 11 neighbors. We then retrain the model with this parameter, and evaluate on the test set. The retraining step is somewhat optional. We could also just use the best model. But retraining allows us to make better use of all the data.\nStill, our results depend on how exactly we split the datasets. So how can we make this more robust?"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#new-problem-with-threefold-split",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#new-problem-with-threefold-split",
    "title": "Supervised learning workflow",
    "section": "New problem with threefold split",
    "text": "New problem with threefold split\n\nFixed train/val/test split → results depend on split\nHigh variance in best k and test score, not robust\n\n\n\n   random_seed  best_validation_score  best_k  test_set_score\n0            0                    0.7       5        0.846154\n1            1                    0.7       1        0.538462\n2            2                    1.0      13        0.692308\n3            3                    0.7       1        0.846154\n4            4                    0.9       5        0.769231\n5            5                    0.8      11        0.769231"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-cv-test-set",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-cv-test-set",
    "title": "Supervised learning workflow",
    "section": "Cross-Validation (CV) + Test Set",
    "text": "Cross-Validation (CV) + Test Set"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#n_neighbors-search-results",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#n_neighbors-search-results",
    "title": "Supervised learning workflow",
    "section": "n_neighbors Search Results",
    "text": "n_neighbors Search Results"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-strategies",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-strategies",
    "title": "Supervised learning workflow",
    "section": "Cross-Validation Strategies",
    "text": "Cross-Validation Strategies"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#how-many-cv-folds",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#how-many-cv-folds",
    "title": "Supervised learning workflow",
    "section": "How many CV folds?",
    "text": "How many CV folds?\n\nRecommend to run 5-fold or 10-fold CV multiple times, while shuffling the dataset\nMore folds → more training data per fold → better generlisation performance estimate, but slower\nExtreme: LeaveOneOut CV (one fold per sample)"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#repeated-kfold-and-leaveoneout",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#repeated-kfold-and-leaveoneout",
    "title": "Supervised learning workflow",
    "section": "Repeated KFold and LeaveOneOut",
    "text": "Repeated KFold and LeaveOneOut\n\nLeaveOneOut: high variance, slow\nShuffleSplit: repeated random splits\nRepeatedKFold: multiple shuffled KFold runs"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#shuffle-split",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#shuffle-split",
    "title": "Supervised learning workflow",
    "section": "Shuffle Split",
    "text": "Shuffle Split"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#standard-cv-not-preserving-class-distribution",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#standard-cv-not-preserving-class-distribution",
    "title": "Supervised learning workflow",
    "section": "Standard CV not preserving class distribution",
    "text": "Standard CV not preserving class distribution\n\nStandard CV leads to folds with different class distributions"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#stratified-cv-for-multiclass-classification-or-imbalanced-data",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#stratified-cv-for-multiclass-classification-or-imbalanced-data",
    "title": "Supervised learning workflow",
    "section": "Stratified CV: for multiclass classification or imbalanced data",
    "text": "Stratified CV: for multiclass classification or imbalanced data\n\nPreserve class distribution in each fold"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cv-in-scikit-learn",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cv-in-scikit-learn",
    "title": "Supervised learning workflow",
    "section": "CV in scikit-learn",
    "text": "CV in scikit-learn\n\n5-fold CV (default)\nClassification CV is stratified by default\ntrain_test_split(..., stratify=y) to stratify\nNo shuffle by default (repeatable results)"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cv-for-grouped-data",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cv-for-grouped-data",
    "title": "Supervised learning workflow",
    "section": "CV for grouped data?",
    "text": "CV for grouped data?\n\nCV is more complicated when data are grouped\nData points within a group are correlated (e.g., city, patient, user)\ne.g. The task is to if a patient has a disease based on medical records from 9 cities\nHow CV should be done depends on the application scenario"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#scenario-1-to-predict-new-data-from-existing-cities-i.i.d.",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#scenario-1-to-predict-new-data-from-existing-cities-i.i.d.",
    "title": "Supervised learning workflow",
    "section": "Scenario 1: To predict new data from existing cities (i.i.d.)",
    "text": "Scenario 1: To predict new data from existing cities (i.i.d.)\n\nStandard CV (e.g. KFold, RepeatedKFold) can be used\nGroup information can be ignored"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#scenario-2-to-predict-new-data-from-unknown-cities-not-i.i.d.",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#scenario-2-to-predict-new-data-from-unknown-cities-not-i.i.d.",
    "title": "Supervised learning workflow",
    "section": "Scenario 2: To predict new data from unknown cities (not i.i.d.)",
    "text": "Scenario 2: To predict new data from unknown cities (not i.i.d.)\n\nGroupKFold should be used; ensure each group is contained in exactly one fold (either train or test)\nData from 9 cities; 5-fold CV; GroupKFold as below."
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#standard-train-test-split-or-cv-not-suitable-for-time-series",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#standard-train-test-split-or-cv-not-suitable-for-time-series",
    "title": "Supervised learning workflow",
    "section": "Standard train-test split or CV not suitable for time series",
    "text": "Standard train-test split or CV not suitable for time series"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-for-time-series",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-for-time-series",
    "title": "Supervised learning workflow",
    "section": "Train-Test Split for Time Series",
    "text": "Train-Test Split for Time Series\n\nUsing past data to train, future data to test"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#timeseriessplit",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#timeseriessplit",
    "title": "Supervised learning workflow",
    "section": "TimeSeriesSplit",
    "text": "TimeSeriesSplit"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#time-series-cv",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#time-series-cv",
    "title": "Supervised learning workflow",
    "section": "Time Series CV",
    "text": "Time Series CV"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overview",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overview",
    "title": "Supervised learning workflow",
    "section": "Overview",
    "text": "Overview\nWe’ve covered:\n\nTrain-test split, threefold split\nCross-validation and its extensions, especially RepeatedKFold\nStratified CV (StratifiedKFold) for multiple classes and imbalanced data\nCV for grouped data and time series data"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#questions",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#questions",
    "title": "Supervised learning workflow",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "sessions/week10.html",
    "href": "sessions/week10.html",
    "title": "Week 10",
    "section": "",
    "text": "This week will introduce how to group similar data points together to discover patterns and structures within a dataset.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#introduction",
    "href": "sessions/week10.html#introduction",
    "title": "Week 10",
    "section": "",
    "text": "This week will introduce how to group similar data points together to discover patterns and structures within a dataset.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#learning-objectives",
    "href": "sessions/week10.html#learning-objectives",
    "title": "Week 10",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the principle and purpose of clustering analysis.\nUnderstand K-Means and apply K-Means to various datasets.\nInterpret the results of clustering analysis.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#lecture",
    "href": "sessions/week10.html#lecture",
    "title": "Week 10",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#quiz",
    "href": "sessions/week10.html#quiz",
    "title": "Week 10",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#practical",
    "href": "sessions/week10.html#practical",
    "title": "Week 10",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week10.html#further-resources",
    "href": "sessions/week10.html#further-resources",
    "title": "Week 10",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "10. Clustering"
    ]
  },
  {
    "objectID": "sessions/week1_practical.html",
    "href": "sessions/week1_practical.html",
    "title": "Practical 1: describing and representing data",
    "section": "",
    "text": "This week is focussed on ensuring that you’re able to access the teaching materials and to run Jupyter notebooks locally, as well as describing a dataset in Python."
  },
  {
    "objectID": "sessions/week1_practical.html#learning-outcomes",
    "href": "sessions/week1_practical.html#learning-outcomes",
    "title": "Practical 1: describing and representing data",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou have familiarised yourself with how to access the lecture notes and Python notebook of this module.\nYou have familiarised yourself with running the Python notebooks locally.\nYou have familiarised yourself with describing a dataset in Python."
  },
  {
    "objectID": "sessions/week1_practical.html#set-up-the-tools",
    "href": "sessions/week1_practical.html#set-up-the-tools",
    "title": "Practical 1: describing and representing data",
    "section": "Set up the tools",
    "text": "Set up the tools\nPlease follow the Setup page of CASA0013 to install and configure the computing platform, and this page to get started on using the container & JupyterLab."
  },
  {
    "objectID": "sessions/week1_practical.html#download-the-notebook",
    "href": "sessions/week1_practical.html#download-the-notebook",
    "title": "Practical 1: describing and representing data",
    "section": "Download the Notebook",
    "text": "Download the Notebook\nSo for this week, visit the Week 1 of QM page, you’ll see that there is a ‘preview’ link and a a ‘download’ link. If you click the preview link you will be taken to the GitHub page for the notebook where it has been ‘rendered’ as a web page, which is not editable. To make the notebook useable on your computer, you need to download the IPYNB file.\nSo now:\n\nClick on the Download link.\nThe file should download automatically, but if you see a page of raw code, select File then Save Page As....\nMake sure you know where to find the file (e.g. Downloads or Desktop).\nMove the file to your Git repository folder (e.g. ~/Documents/CASA/QM/)\nCheck to see if your browser has added .txt to the file name:\n\nIf no, then you can move to adding the file.\nIf yes, then you can either fix the name in the Finder/Windows Explore, or you can do this in the Terminal using mv &lt;name_of_practical&gt;.ipynb.txt &lt;name_of_practical&gt;.ipynb (you can even do this in JupyterLab’s terminal if it’s already running)."
  },
  {
    "objectID": "sessions/week1_practical.html#running-notebooks-on-jupyterlab",
    "href": "sessions/week1_practical.html#running-notebooks-on-jupyterlab",
    "title": "Practical 1: describing and representing data",
    "section": "Running notebooks on JupyterLab",
    "text": "Running notebooks on JupyterLab\nI am assuming that most of you are already running JupyterLab via Podman using the command.\nIf you are a bit confused with container, JupyterLab, terminal, or Git, please feel free to ask any questions."
  },
  {
    "objectID": "sessions/week1_practical.html#loading-data",
    "href": "sessions/week1_practical.html#loading-data",
    "title": "Practical 1: describing and representing data",
    "section": "Loading data",
    "text": "Loading data\nWe are going to describe the population of local authorities in the UK.\nThe data is sourced from Office for National Statistics and is donwloadable here.\nWe have saved a copy of this dataset to the Github repo, in case that the dataset is removed from the website.\n\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())\n\n   Area code          Area name Area type  Population 2011  Population 2021  \\\n0  K04000001  England and Wales  National       56075912.0       59597542.0   \n1  E92000001            England   Country       53012456.0       56490048.0   \n2  W92000004              Wales   Country        3063456.0        3107494.0   \n3  E12000001         North East    Region        2596886.0        2647013.0   \n4  E12000002         North West    Region        7052177.0        7417397.0   \n\n   Percentage change  \n0                6.3  \n1                6.6  \n2                1.4  \n3                1.9  \n4                5.2  \n\n\nYou might wonder why skipping the first 5 rows and setting thousands=‘,’. I learnt this after opening this csv file in a text editor and lots of trial-and-errors.\n\n\nThen, we printed the first few rows of this dataset using df_pop.head()."
  },
  {
    "objectID": "sessions/week1_practical.html#describing-the-dataframe",
    "href": "sessions/week1_practical.html#describing-the-dataframe",
    "title": "Practical 1: describing and representing data",
    "section": "Describing the dataframe",
    "text": "Describing the dataframe\n\nWhich columns are included?\n\nlist(df_pop.columns)\n\n['Area code',\n 'Area name',\n 'Area type',\n 'Population 2011',\n 'Population 2021',\n 'Percentage change']\n\n\nIt is a pain to deal with whitespaces in a column, so good practice is to replace the whitespaces (eg tabs, multiple spaces) within column names with underscore.\n\ndf_pop.columns = df_pop.columns.str.replace(r'\\s+', '_', regex=True)\nprint(list(df_pop.columns)) # check again\n\n['Area_code', 'Area_name', 'Area_type', 'Population_2011', 'Population_2021', 'Percentage_change']\n\n\n\n\nHow many rows & cols are included?\n\nrows, cols = df_pop.shape\nprint(f\"Rows: {rows}, Columns: {cols}\")\n\nRows: 369, Columns: 6\n\n\n\n\nGeography matters\nThis dataset contains multiple geographies of UK and different geographies are incomparable. We can check the Area_type column:\n\nprint(df_pop.Area_type.value_counts())\n\nArea_type\nLocal Authority    355\nRegion               9\nCountry              2\nNational             1\nName: count, dtype: int64\n\n\nSo there are 355 records of Local Authority， 9 records of Region, 2 of Country, and 1 of ‘National’. For an introduction to these terms, see this article on ONS.\nWe will focus on the local authorities, so we apply a filter:\n\ndf_pop_la = df_pop[df_pop['Area_type'] == 'Local Authority']\n\n\n\nOverview of the columns\nThere are two pandas functions that give overview of a dataframe. - info(): shows column data types, non‑null counts, and memory usage. - describe(): shows summary statistics for numeric data (count, mean, std, min, quartiles, max) - describe(include='all'): for both numeric data and non‑numeric data (count, unique, top value, frequency).\n\nprint(df_pop_la.info())\nprint(df_pop_la.describe())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 355 entries, 12 to 366\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Area_code          355 non-null    object \n 1   Area_name          355 non-null    object \n 2   Area_type          355 non-null    object \n 3   Population_2011    355 non-null    float64\n 4   Population_2021    355 non-null    float64\n 5   Percentage_change  355 non-null    float64\ndtypes: float64(3), object(3)\nmemory usage: 19.4+ KB\nNone\n       Population_2011  Population_2021  Percentage_change\ncount     3.550000e+02     3.550000e+02         355.000000\nmean      2.132867e+05     2.268876e+05           6.070423\nstd       2.099628e+05     2.245442e+05           4.608338\nmin       2.203000e+03     2.054000e+03          -9.600000\n25%       1.000530e+05     1.055705e+05           2.950000\n50%       1.382650e+05     1.477760e+05           5.800000\n75%       2.487865e+05     2.628895e+05           9.000000\nmax       1.463740e+06     1.576069e+06          22.100000"
  },
  {
    "objectID": "sessions/week1_practical.html#describing-census-2021-population",
    "href": "sessions/week1_practical.html#describing-census-2021-population",
    "title": "Practical 1: describing and representing data",
    "section": "Describing census 2021 population",
    "text": "Describing census 2021 population\nNow, we focus on describing the local authority population from census 2021. The first question is, what data type is this variable - nominal, ordinal, interval, or ratio？\n\n\n\n\n\n\nNote\n\n\n\nThe data type of a variable is different from how it’s stored in memory. For example, the Area_type variable can be encoded for convenience as 0 (“national”), 1 (“country”), and 2 (“local authority”). Although these are stored as numbers, Area_type is not truly numeric data — it’s an nominal variable.\n\n\nDoes it make sense to say ‘The population of LA AAA is twice of LA BBB’? Yes. So, this variable is of ratio type.\n\nmax and min\nWhat is the maximum population size in census 2021?\nprint(\"Max population: \", df_pop_la['Population_2021'].max(skipna=True))\nWhich LAs have the maximum population size? The code above is a bit complicated.\n\nprint(\"{} have the maximum population of {}\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Population_2021'] == df_pop_la['Population_2021'].max(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].max(skipna=True))\n    )\n\nKent have the maximum population of 1576069.0\n\n\nWhat it does: - Finds the max population while ignoring NaNs. - Selects all rows with that population. - Joins their Area_name values into a comma-separated string.\nTwo new Python functions here: - format(): Inserts variables into a string by replacing {} placeholders in order with provided arguments. - join(): Combines the elements of an iterable into one string using the given separator before .join().\nWhich LAs have the minimum population?\n\nprint(\"{} have the minimum population of {}\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Population_2021'] == df_pop_la['Population_2021'].min(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].min(skipna=True))\n    )\n\nIsles of Scilly have the minimum population of 2054.0\n\n\n\n\nNA value and outliers?\nAre there NA values or outliers in this variable? From results of info(), there are no NA values.\nTo detect outliers, we will implement the Tukey Fences method using pandas function, as pandas does not provide a built-in function for this method.\n\n# Calculate Q1, Q3, and IQR\nQ1 = df_pop_la['Population_2021'].quantile(0.25)\nQ3 = df_pop_la['Population_2021'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Tukey's fences\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Detect outliers\noutliers = df_pop_la[\n    (df_pop_la['Population_2021'] &lt; lower_bound) |\n    (df_pop_la['Population_2021'] &gt; upper_bound)\n]\n\nprint(\"Lower bound:\", lower_bound)\nprint(\"Upper bound:\", upper_bound)\nprint(\"How many outliers?\", outliers.shape[0])\nprint(\"Outliers:\\n\", outliers)\n\nLower bound: -130408.0\nUpper bound: 498868.0\nHow many outliers? 33\nOutliers:\n      Area_code        Area_name        Area_type  Population_2011  \\\n56   E06000047    County Durham  Local Authority         513242.0   \n60   E06000052         Cornwall  Local Authority         532273.0   \n62   E06000054        Wiltshire  Local Authority         470981.0   \n68   E06000060  Buckinghamshire  Local Authority         505283.0   \n254  E08000003       Manchester  Local Authority         503127.0   \n270  E08000019        Sheffield  Local Authority         552698.0   \n275  E08000025       Birmingham  Local Authority        1073045.0   \n282  E08000032         Bradford  Local Authority         522452.0   \n285  E08000035            Leeds  Local Authority         751485.0   \n321  E10000003   Cambridgeshire  Local Authority         621210.0   \n322  E10000006          Cumbria  Local Authority         499858.0   \n323  E10000007       Derbyshire  Local Authority         769686.0   \n324  E10000008            Devon  Local Authority         746399.0   \n325  E10000011      East Sussex  Local Authority         526671.0   \n326  E10000012            Essex  Local Authority        1393587.0   \n327  E10000013  Gloucestershire  Local Authority         596984.0   \n328  E10000014        Hampshire  Local Authority        1317788.0   \n329  E10000015    Hertfordshire  Local Authority        1116062.0   \n330  E10000016             Kent  Local Authority        1463740.0   \n331  E10000017       Lancashire  Local Authority        1171339.0   \n332  E10000018   Leicestershire  Local Authority         650489.0   \n333  E10000019     Lincolnshire  Local Authority         713653.0   \n334  E10000020          Norfolk  Local Authority         857888.0   \n335  E10000023  North Yorkshire  Local Authority         598376.0   \n336  E10000024  Nottinghamshire  Local Authority         785802.0   \n337  E10000025      Oxfordshire  Local Authority         653798.0   \n338  E10000027         Somerset  Local Authority         529972.0   \n339  E10000028    Staffordshire  Local Authority         848489.0   \n340  E10000029          Suffolk  Local Authority         728163.0   \n341  E10000030           Surrey  Local Authority        1132390.0   \n342  E10000031     Warwickshire  Local Authority         545474.0   \n343  E10000032      West Sussex  Local Authority         806892.0   \n344  E10000034   Worcestershire  Local Authority         566169.0   \n\n     Population_2021  Percentage_change  \n56          522068.0                1.7  \n60          570305.0                7.1  \n62          510330.0                8.4  \n68          553078.0                9.5  \n254         551938.0                9.7  \n270         556521.0                0.7  \n275        1144919.0                6.7  \n282         546412.0                4.6  \n285         811953.0                8.0  \n321         678849.0                9.3  \n322         499846.0                0.0  \n323         794636.0                3.2  \n324         811640.0                8.7  \n325         545847.0                3.6  \n326        1503521.0                7.9  \n327         645076.0                8.1  \n328        1400899.0                6.3  \n329        1198798.0                7.4  \n330        1576069.0                7.7  \n331        1235354.0                5.5  \n332         712366.0                9.5  \n333         768364.0                7.7  \n334         916120.0                6.8  \n335         615491.0                2.9  \n336         824822.0                5.0  \n337         725291.0               10.9  \n338         571547.0                7.8  \n339         876104.0                3.3  \n340         760688.0                4.5  \n341        1203108.0                6.2  \n342         596773.0                9.4  \n343         882676.0                9.4  \n344         603676.0                6.6  \n\n\nThere are 33 outliers in this dataset. Think about the three types of outliers that we discussed. Which type do these 33 outliers beloong to?\n\nError Outlier\nIrregular Pattern Outlier\nInfluential Outlier\n\n\n\nBoxplot\nTo create a boxplot of this variable:\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la['Population_2021'].plot(kind='box', title='LA Population 2021 Boxplot')\n\nplt.ylabel('Population')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\nWhat do you observe from this boxplot? There are lots of values above the"
  },
  {
    "objectID": "sessions/week1_practical.html#exploring-percentage_change",
    "href": "sessions/week1_practical.html#exploring-percentage_change",
    "title": "Practical 1: describing and representing data",
    "section": "Exploring Percentage_change",
    "text": "Exploring Percentage_change\nNow, we turn to explore the variable Percentage_change, which represents the relative change from the 2011 census to 2021 census.\nTry completing the code below on your own. Practice makes perfect!\n\nWhich LAs experienced the largest population percentage change? To what extent?\n\nQuestionAnswer\n\n\nprint(\"{} have the largest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[??['??'] == ??['??'].max(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].??(skipna=True))\n    )\n\n\nprint(\"{} have the largest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Percentage_change'] == df_pop_la['Percentage_change'].max(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].max(skipna=True))\n    )\nTower Hamlets have the largest population percentage change of 22.1%\n\n\n\n\n\nWhich LAs experienced the smallest population percentage change? To what extent?\n\nQuestionAnswer\n\n\nprint(\"{} have the smallest population percentage change of {}%\".format(\n    \", \".??(df_pop_la.loc[df_pop_la[??] == ??['Percentage_change'].??(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].??(skipna=True))\n    )\n\n\nprint(\"{} have the smallest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Percentage_change'] == df_pop_la['Percentage_change'].min(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].min(skipna=True))\n    )\nKensington and Chelsea have the smallest population percentage change of -9.6%\n\n\n\n\n\nMake a boxplot of Percentage_change\n\nQuestionAnswer\n\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la[??].plot(kind=??, title='LA Population Percentage Change Boxplot')\n\nplt.??('Percentage change')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la['Percentage_change'].plot(kind='box', title='LA Population Percentage Change Boxplot')\n\nplt.ylabel('Percentage change')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()"
  },
  {
    "objectID": "sessions/week1_practical.html#youre-done",
    "href": "sessions/week1_practical.html#youre-done",
    "title": "Practical 1: describing and representing data",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the first QM practical session! If you are still working on it, take you time.\nDon’t worry about understanding every detail of the Python code — what matters most is knowing which functions to use for a specific task, like checking minimum and maximum values or generating boxplots, and knowing how to debug when it goes wrong. Remember, practice makes perfect."
  },
  {
    "objectID": "sessions/week3.html",
    "href": "sessions/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "This week will introduce analysis workflow of supervised learning.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#introduction",
    "href": "sessions/week3.html#introduction",
    "title": "Week 3",
    "section": "",
    "text": "This week will introduce analysis workflow of supervised learning.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#learning-objectives",
    "href": "sessions/week3.html#learning-objectives",
    "title": "Week 3",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand different workflow of supervised learning.\nUnderstand train-test split and train-validation-test split.\nUnderstand cross validation and its extensions\nKnow when to use different model evaluation methods.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#lecture",
    "href": "sessions/week3.html#lecture",
    "title": "Week 3",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#quiz",
    "href": "sessions/week3.html#quiz",
    "title": "Week 3",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#practical",
    "href": "sessions/week3.html#practical",
    "title": "Week 3",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week3.html#further-resources",
    "href": "sessions/week3.html#further-resources",
    "title": "Week 3",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Basics",
      "3. Hypothesis Testing"
    ]
  },
  {
    "objectID": "sessions/week5.html",
    "href": "sessions/week5.html",
    "title": "Week 5",
    "section": "",
    "text": "This week, we’ll explore how to tell if several groups are truly different and how to measure the strength and direction of the relationship between two variables.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#introduction",
    "href": "sessions/week5.html#introduction",
    "title": "Week 5",
    "section": "",
    "text": "This week, we’ll explore how to tell if several groups are truly different and how to measure the strength and direction of the relationship between two variables.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#learning-objectives",
    "href": "sessions/week5.html#learning-objectives",
    "title": "Week 5",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand ANOVA.\nUnderstand correlation between two variables.\nUnderstand the difference of Pearson and Spearman correlation.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#lecture",
    "href": "sessions/week5.html#lecture",
    "title": "Week 5",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#quiz",
    "href": "sessions/week5.html#quiz",
    "title": "Week 5",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#practical",
    "href": "sessions/week5.html#practical",
    "title": "Week 5",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week5.html#further-resources",
    "href": "sessions/week5.html#further-resources",
    "title": "Week 5",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 2: Correlation and Regression",
      "5. Measuring Relationship"
    ]
  },
  {
    "objectID": "sessions/week9.html",
    "href": "sessions/week9.html",
    "title": "Week 9",
    "section": "",
    "text": "This week will introduce how to simplify complex datasets by reducing the number of variables while preserving important information.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#introduction",
    "href": "sessions/week9.html#introduction",
    "title": "Week 9",
    "section": "",
    "text": "This week will introduce how to simplify complex datasets by reducing the number of variables while preserving important information.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#learning-objectives",
    "href": "sessions/week9.html#learning-objectives",
    "title": "Week 9",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the principle of dimensionality reduction.\nUnderstand the method of principle component analysis.\nVisualise and describe the results of PCA.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#lecture",
    "href": "sessions/week9.html#lecture",
    "title": "Week 9",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#quiz",
    "href": "sessions/week9.html#quiz",
    "title": "Week 9",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#practical",
    "href": "sessions/week9.html#practical",
    "title": "Week 9",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/week9.html#further-resources",
    "href": "sessions/week9.html#further-resources",
    "title": "Week 9",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 3: Dimension Reduction & Clustering",
      "9. Dimension Reduction"
    ]
  },
  {
    "objectID": "sessions/weekX_lecture.html#understanding-and-describing-data",
    "href": "sessions/weekX_lecture.html#understanding-and-describing-data",
    "title": "XXX",
    "section": "Understanding and describing data",
    "text": "Understanding and describing data\n\nQuantitative research is the process of collecting and analysing numerical data to describe, model, and predict variables of interest.\nGarbage in, garbage out.\n\n\nThis lecture focuses on understanding and describing data."
  },
  {
    "objectID": "sessions/weekX_lecture.html#learning-objectives",
    "href": "sessions/weekX_lecture.html#learning-objectives",
    "title": "XXX",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand basic data types;\nConsider how to summarise and represent data."
  }
]