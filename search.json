[
  {
    "objectID": "setup/index.html",
    "href": "setup/index.html",
    "title": "Setting up",
    "section": "",
    "text": "Please follow CASA0013 Set-up page to set up the CASA Computing Environment."
  },
  {
    "objectID": "sessions/weekX_lecture.html#understanding-and-describing-data",
    "href": "sessions/weekX_lecture.html#understanding-and-describing-data",
    "title": "XXX",
    "section": "Understanding and describing data",
    "text": "Understanding and describing data\n\nQuantitative research is the process of collecting and analysing numerical data to describe, model, and predict variables of interest.\nGarbage in, garbage out.\n\n\nThis lecture focuses on understanding and describing data."
  },
  {
    "objectID": "sessions/weekX_lecture.html#learning-objectives",
    "href": "sessions/weekX_lecture.html#learning-objectives",
    "title": "XXX",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand basic data types;\nConsider how to summarise and represent data."
  },
  {
    "objectID": "sessions/week9.html",
    "href": "sessions/week9.html",
    "title": "Week 9",
    "section": "",
    "text": "This week will introduce Machine Learning Operations (MLOps) and how to move from notebooks to scripts. It will also introduce best practices for coding in Python notebooks such as using sklearn pipelines.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "9. Machine Learning Ops"
    ]
  },
  {
    "objectID": "sessions/week9.html#introduction",
    "href": "sessions/week9.html#introduction",
    "title": "Week 9",
    "section": "",
    "text": "This week will introduce Machine Learning Operations (MLOps) and how to move from notebooks to scripts. It will also introduce best practices for coding in Python notebooks such as using sklearn pipelines.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "9. Machine Learning Ops"
    ]
  },
  {
    "objectID": "sessions/week9.html#learning-objectives",
    "href": "sessions/week9.html#learning-objectives",
    "title": "Week 9",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the procedure of Machine Learning Operations (MLOps).\nUnderstand the motivation of moving from notebooks to scripts.\nAppreciate best practices for coding in Python notebooks and using sklearn pipelines.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "9. Machine Learning Ops"
    ]
  },
  {
    "objectID": "sessions/week9.html#lecture",
    "href": "sessions/week9.html#lecture",
    "title": "Week 9",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 3: Advanced Topics",
      "9. Machine Learning Ops"
    ]
  },
  {
    "objectID": "sessions/week9.html#quiz",
    "href": "sessions/week9.html#quiz",
    "title": "Week 9",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "9. Machine Learning Ops"
    ]
  },
  {
    "objectID": "sessions/week9.html#practical",
    "href": "sessions/week9.html#practical",
    "title": "Week 9",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Advanced Topics",
      "9. Machine Learning Ops"
    ]
  },
  {
    "objectID": "sessions/week9.html#further-resources",
    "href": "sessions/week9.html#further-resources",
    "title": "Week 9",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 3: Advanced Topics",
      "9. Machine Learning Ops"
    ]
  },
  {
    "objectID": "sessions/week7.html",
    "href": "sessions/week7.html",
    "title": "Week 7",
    "section": "",
    "text": "This week, we’ll explore how to interpret a trained machine learning models and how to select features for training a machine learning model.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "7. Model Interpretation & Feature Selection"
    ]
  },
  {
    "objectID": "sessions/week7.html#introduction",
    "href": "sessions/week7.html#introduction",
    "title": "Week 7",
    "section": "",
    "text": "This week, we’ll explore how to interpret a trained machine learning models and how to select features for training a machine learning model.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "7. Model Interpretation & Feature Selection"
    ]
  },
  {
    "objectID": "sessions/week7.html#learning-objectives",
    "href": "sessions/week7.html#learning-objectives",
    "title": "Week 7",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand different model interpretation techniques.\nUnderstand methods for selecting features in machine learning.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "7. Model Interpretation & Feature Selection"
    ]
  },
  {
    "objectID": "sessions/week7.html#lecture",
    "href": "sessions/week7.html#lecture",
    "title": "Week 7",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 3: Advanced Topics",
      "7. Model Interpretation & Feature Selection"
    ]
  },
  {
    "objectID": "sessions/week7.html#quiz",
    "href": "sessions/week7.html#quiz",
    "title": "Week 7",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "7. Model Interpretation & Feature Selection"
    ]
  },
  {
    "objectID": "sessions/week7.html#practical",
    "href": "sessions/week7.html#practical",
    "title": "Week 7",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Advanced Topics",
      "7. Model Interpretation & Feature Selection"
    ]
  },
  {
    "objectID": "sessions/week7.html#further-resources",
    "href": "sessions/week7.html#further-resources",
    "title": "Week 7",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 3: Advanced Topics",
      "7. Model Interpretation & Feature Selection"
    ]
  },
  {
    "objectID": "sessions/week5.html",
    "href": "sessions/week5.html",
    "title": "Week 5",
    "section": "",
    "text": "This week, we’ll introduce the design and training of classic neural networks and convolutional neural networks.",
    "crumbs": [
      "Part 2: Methods",
      "5. Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week5.html#introduction",
    "href": "sessions/week5.html#introduction",
    "title": "Week 5",
    "section": "",
    "text": "This week, we’ll introduce the design and training of classic neural networks and convolutional neural networks.",
    "crumbs": [
      "Part 2: Methods",
      "5. Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week5.html#learning-objectives",
    "href": "sessions/week5.html#learning-objectives",
    "title": "Week 5",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the design and principle of neural networks.\nUnderstand the structure of convolutional neural networks.\nCan apply neural networks to supervised learning on tabular data and image data.",
    "crumbs": [
      "Part 2: Methods",
      "5. Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week5.html#lecture",
    "href": "sessions/week5.html#lecture",
    "title": "Week 5",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Methods",
      "5. Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week5.html#quiz",
    "href": "sessions/week5.html#quiz",
    "title": "Week 5",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Methods",
      "5. Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week5.html#practical",
    "href": "sessions/week5.html#practical",
    "title": "Week 5",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Methods",
      "5. Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week5.html#further-resources",
    "href": "sessions/week5.html#further-resources",
    "title": "Week 5",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 2: Methods",
      "5. Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week3.html",
    "href": "sessions/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "This week will introduce analysis workflow of supervised learning.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "3. Supervised Learning Workflow"
    ]
  },
  {
    "objectID": "sessions/week3.html#introduction",
    "href": "sessions/week3.html#introduction",
    "title": "Week 3",
    "section": "",
    "text": "This week will introduce analysis workflow of supervised learning.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "3. Supervised Learning Workflow"
    ]
  },
  {
    "objectID": "sessions/week3.html#learning-objectives",
    "href": "sessions/week3.html#learning-objectives",
    "title": "Week 3",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand different workflow of supervised learning.\nUnderstand train-test split and train-validation-test split.\nUnderstand cross validation and its extensions\nKnow when to use different model evaluation methods.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "3. Supervised Learning Workflow"
    ]
  },
  {
    "objectID": "sessions/week3.html#lecture",
    "href": "sessions/week3.html#lecture",
    "title": "Week 3",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Supervised Learning",
      "3. Supervised Learning Workflow"
    ]
  },
  {
    "objectID": "sessions/week3.html#quiz",
    "href": "sessions/week3.html#quiz",
    "title": "Week 3",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "3. Supervised Learning Workflow"
    ]
  },
  {
    "objectID": "sessions/week3.html#practical",
    "href": "sessions/week3.html#practical",
    "title": "Week 3",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Supervised Learning",
      "3. Supervised Learning Workflow"
    ]
  },
  {
    "objectID": "sessions/week3.html#further-resources",
    "href": "sessions/week3.html#further-resources",
    "title": "Week 3",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Supervised Learning",
      "3. Supervised Learning Workflow"
    ]
  },
  {
    "objectID": "sessions/week1_practical.html",
    "href": "sessions/week1_practical.html",
    "title": "Practical 1: describing and representing data",
    "section": "",
    "text": "This week is focussed on ensuring that you’re able to access the teaching materials and to run Jupyter notebooks locally, as well as describing a dataset in Python."
  },
  {
    "objectID": "sessions/week1_practical.html#learning-outcomes",
    "href": "sessions/week1_practical.html#learning-outcomes",
    "title": "Practical 1: describing and representing data",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou have familiarised yourself with how to access the lecture notes and Python notebook of this module.\nYou have familiarised yourself with running the Python notebooks locally.\nYou have familiarised yourself with describing a dataset in Python."
  },
  {
    "objectID": "sessions/week1_practical.html#set-up-the-tools",
    "href": "sessions/week1_practical.html#set-up-the-tools",
    "title": "Practical 1: describing and representing data",
    "section": "Set up the tools",
    "text": "Set up the tools\nPlease follow the Setup page of CASA0013 to install and configure the computing platform, and this page to get started on using the container & JupyterLab."
  },
  {
    "objectID": "sessions/week1_practical.html#download-the-notebook",
    "href": "sessions/week1_practical.html#download-the-notebook",
    "title": "Practical 1: describing and representing data",
    "section": "Download the Notebook",
    "text": "Download the Notebook\nSo for this week, visit the Week 1 of QM page, you’ll see that there is a ‘preview’ link and a a ‘download’ link. If you click the preview link you will be taken to the GitHub page for the notebook where it has been ‘rendered’ as a web page, which is not editable. To make the notebook useable on your computer, you need to download the IPYNB file.\nSo now:\n\nClick on the Download link.\nThe file should download automatically, but if you see a page of raw code, select File then Save Page As....\nMake sure you know where to find the file (e.g. Downloads or Desktop).\nMove the file to your Git repository folder (e.g. ~/Documents/CASA/QM/)\nCheck to see if your browser has added .txt to the file name:\n\nIf no, then you can move to adding the file.\nIf yes, then you can either fix the name in the Finder/Windows Explore, or you can do this in the Terminal using mv &lt;name_of_practical&gt;.ipynb.txt &lt;name_of_practical&gt;.ipynb (you can even do this in JupyterLab’s terminal if it’s already running)."
  },
  {
    "objectID": "sessions/week1_practical.html#running-notebooks-on-jupyterlab",
    "href": "sessions/week1_practical.html#running-notebooks-on-jupyterlab",
    "title": "Practical 1: describing and representing data",
    "section": "Running notebooks on JupyterLab",
    "text": "Running notebooks on JupyterLab\nI am assuming that most of you are already running JupyterLab via Podman using the command.\nIf you are a bit confused with container, JupyterLab, terminal, or Git, please feel free to ask any questions."
  },
  {
    "objectID": "sessions/week1_practical.html#loading-data",
    "href": "sessions/week1_practical.html#loading-data",
    "title": "Practical 1: describing and representing data",
    "section": "Loading data",
    "text": "Loading data\nWe are going to describe the population of local authorities in the UK.\nThe data is sourced from Office for National Statistics and is donwloadable here.\nWe have saved a copy of this dataset to the Github repo, in case that the dataset is removed from the website.\n\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())\n\n   Area code          Area name Area type  Population 2011  Population 2021  \\\n0  K04000001  England and Wales  National       56075912.0       59597542.0   \n1  E92000001            England   Country       53012456.0       56490048.0   \n2  W92000004              Wales   Country        3063456.0        3107494.0   \n3  E12000001         North East    Region        2596886.0        2647013.0   \n4  E12000002         North West    Region        7052177.0        7417397.0   \n\n   Percentage change  \n0                6.3  \n1                6.6  \n2                1.4  \n3                1.9  \n4                5.2  \n\n\nYou might wonder why skipping the first 5 rows and setting thousands=‘,’. I learnt this after opening this csv file in a text editor and lots of trial-and-errors.\n\n\nThen, we printed the first few rows of this dataset using df_pop.head()."
  },
  {
    "objectID": "sessions/week1_practical.html#describing-the-dataframe",
    "href": "sessions/week1_practical.html#describing-the-dataframe",
    "title": "Practical 1: describing and representing data",
    "section": "Describing the dataframe",
    "text": "Describing the dataframe\n\nWhich columns are included?\n\nlist(df_pop.columns)\n\n['Area code',\n 'Area name',\n 'Area type',\n 'Population 2011',\n 'Population 2021',\n 'Percentage change']\n\n\nIt is a pain to deal with whitespaces in a column, so good practice is to replace the whitespaces (eg tabs, multiple spaces) within column names with underscore.\n\ndf_pop.columns = df_pop.columns.str.replace(r'\\s+', '_', regex=True)\nprint(list(df_pop.columns)) # check again\n\n['Area_code', 'Area_name', 'Area_type', 'Population_2011', 'Population_2021', 'Percentage_change']\n\n\n\n\nHow many rows & cols are included?\n\nrows, cols = df_pop.shape\nprint(f\"Rows: {rows}, Columns: {cols}\")\n\nRows: 369, Columns: 6\n\n\n\n\nGeography matters\nThis dataset contains multiple geographies of UK and different geographies are incomparable. We can check the Area_type column:\n\nprint(df_pop.Area_type.value_counts())\n\nArea_type\nLocal Authority    355\nRegion               9\nCountry              2\nNational             1\nName: count, dtype: int64\n\n\nSo there are 355 records of Local Authority， 9 records of Region, 2 of Country, and 1 of ‘National’. For an introduction to these terms, see this article on ONS.\nWe will focus on the local authorities, so we apply a filter:\n\ndf_pop_la = df_pop[df_pop['Area_type'] == 'Local Authority']\n\n\n\nOverview of the columns\nThere are two pandas functions that give overview of a dataframe. - info(): shows column data types, non‑null counts, and memory usage. - describe(): shows summary statistics for numeric data (count, mean, std, min, quartiles, max) - describe(include='all'): for both numeric data and non‑numeric data (count, unique, top value, frequency).\n\nprint(df_pop_la.info())\nprint(df_pop_la.describe())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 355 entries, 12 to 366\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Area_code          355 non-null    object \n 1   Area_name          355 non-null    object \n 2   Area_type          355 non-null    object \n 3   Population_2011    355 non-null    float64\n 4   Population_2021    355 non-null    float64\n 5   Percentage_change  355 non-null    float64\ndtypes: float64(3), object(3)\nmemory usage: 19.4+ KB\nNone\n       Population_2011  Population_2021  Percentage_change\ncount     3.550000e+02     3.550000e+02         355.000000\nmean      2.132867e+05     2.268876e+05           6.070423\nstd       2.099628e+05     2.245442e+05           4.608338\nmin       2.203000e+03     2.054000e+03          -9.600000\n25%       1.000530e+05     1.055705e+05           2.950000\n50%       1.382650e+05     1.477760e+05           5.800000\n75%       2.487865e+05     2.628895e+05           9.000000\nmax       1.463740e+06     1.576069e+06          22.100000"
  },
  {
    "objectID": "sessions/week1_practical.html#describing-census-2021-population",
    "href": "sessions/week1_practical.html#describing-census-2021-population",
    "title": "Practical 1: describing and representing data",
    "section": "Describing census 2021 population",
    "text": "Describing census 2021 population\nNow, we focus on describing the local authority population from census 2021. The first question is, what data type is this variable - nominal, ordinal, interval, or ratio？\n\n\n\n\n\n\nNote\n\n\n\nThe data type of a variable is different from how it’s stored in memory. For example, the Area_type variable can be encoded for convenience as 0 (“national”), 1 (“country”), and 2 (“local authority”). Although these are stored as numbers, Area_type is not truly numeric data — it’s an nominal variable.\n\n\nDoes it make sense to say ‘The population of LA AAA is twice of LA BBB’? Yes. So, this variable is of ratio type.\n\nmax and min\nWhat is the maximum population size in census 2021?\nprint(\"Max population: \", df_pop_la['Population_2021'].max(skipna=True))\nWhich LAs have the maximum population size? The code above is a bit complicated.\n\nprint(\"{} have the maximum population of {}\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Population_2021'] == df_pop_la['Population_2021'].max(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].max(skipna=True))\n    )\n\nKent have the maximum population of 1576069.0\n\n\nWhat it does: - Finds the max population while ignoring NaNs. - Selects all rows with that population. - Joins their Area_name values into a comma-separated string.\nTwo new Python functions here: - format(): Inserts variables into a string by replacing {} placeholders in order with provided arguments. - join(): Combines the elements of an iterable into one string using the given separator before .join().\nWhich LAs have the minimum population?\n\nprint(\"{} have the minimum population of {}\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Population_2021'] == df_pop_la['Population_2021'].min(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].min(skipna=True))\n    )\n\nIsles of Scilly have the minimum population of 2054.0\n\n\n\n\nNA value and outliers?\nAre there NA values or outliers in this variable? From results of info(), there are no NA values.\nTo detect outliers, we will implement the Tukey Fences method using pandas function, as pandas does not provide a built-in function for this method.\n\n# Calculate Q1, Q3, and IQR\nQ1 = df_pop_la['Population_2021'].quantile(0.25)\nQ3 = df_pop_la['Population_2021'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Tukey's fences\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Detect outliers\noutliers = df_pop_la[\n    (df_pop_la['Population_2021'] &lt; lower_bound) |\n    (df_pop_la['Population_2021'] &gt; upper_bound)\n]\n\nprint(\"Lower bound:\", lower_bound)\nprint(\"Upper bound:\", upper_bound)\nprint(\"How many outliers?\", outliers.shape[0])\nprint(\"Outliers:\\n\", outliers)\n\nLower bound: -130408.0\nUpper bound: 498868.0\nHow many outliers? 33\nOutliers:\n      Area_code        Area_name        Area_type  Population_2011  \\\n56   E06000047    County Durham  Local Authority         513242.0   \n60   E06000052         Cornwall  Local Authority         532273.0   \n62   E06000054        Wiltshire  Local Authority         470981.0   \n68   E06000060  Buckinghamshire  Local Authority         505283.0   \n254  E08000003       Manchester  Local Authority         503127.0   \n270  E08000019        Sheffield  Local Authority         552698.0   \n275  E08000025       Birmingham  Local Authority        1073045.0   \n282  E08000032         Bradford  Local Authority         522452.0   \n285  E08000035            Leeds  Local Authority         751485.0   \n321  E10000003   Cambridgeshire  Local Authority         621210.0   \n322  E10000006          Cumbria  Local Authority         499858.0   \n323  E10000007       Derbyshire  Local Authority         769686.0   \n324  E10000008            Devon  Local Authority         746399.0   \n325  E10000011      East Sussex  Local Authority         526671.0   \n326  E10000012            Essex  Local Authority        1393587.0   \n327  E10000013  Gloucestershire  Local Authority         596984.0   \n328  E10000014        Hampshire  Local Authority        1317788.0   \n329  E10000015    Hertfordshire  Local Authority        1116062.0   \n330  E10000016             Kent  Local Authority        1463740.0   \n331  E10000017       Lancashire  Local Authority        1171339.0   \n332  E10000018   Leicestershire  Local Authority         650489.0   \n333  E10000019     Lincolnshire  Local Authority         713653.0   \n334  E10000020          Norfolk  Local Authority         857888.0   \n335  E10000023  North Yorkshire  Local Authority         598376.0   \n336  E10000024  Nottinghamshire  Local Authority         785802.0   \n337  E10000025      Oxfordshire  Local Authority         653798.0   \n338  E10000027         Somerset  Local Authority         529972.0   \n339  E10000028    Staffordshire  Local Authority         848489.0   \n340  E10000029          Suffolk  Local Authority         728163.0   \n341  E10000030           Surrey  Local Authority        1132390.0   \n342  E10000031     Warwickshire  Local Authority         545474.0   \n343  E10000032      West Sussex  Local Authority         806892.0   \n344  E10000034   Worcestershire  Local Authority         566169.0   \n\n     Population_2021  Percentage_change  \n56          522068.0                1.7  \n60          570305.0                7.1  \n62          510330.0                8.4  \n68          553078.0                9.5  \n254         551938.0                9.7  \n270         556521.0                0.7  \n275        1144919.0                6.7  \n282         546412.0                4.6  \n285         811953.0                8.0  \n321         678849.0                9.3  \n322         499846.0                0.0  \n323         794636.0                3.2  \n324         811640.0                8.7  \n325         545847.0                3.6  \n326        1503521.0                7.9  \n327         645076.0                8.1  \n328        1400899.0                6.3  \n329        1198798.0                7.4  \n330        1576069.0                7.7  \n331        1235354.0                5.5  \n332         712366.0                9.5  \n333         768364.0                7.7  \n334         916120.0                6.8  \n335         615491.0                2.9  \n336         824822.0                5.0  \n337         725291.0               10.9  \n338         571547.0                7.8  \n339         876104.0                3.3  \n340         760688.0                4.5  \n341        1203108.0                6.2  \n342         596773.0                9.4  \n343         882676.0                9.4  \n344         603676.0                6.6  \n\n\nThere are 33 outliers in this dataset. Think about the three types of outliers that we discussed. Which type do these 33 outliers beloong to?\n\nError Outlier\nIrregular Pattern Outlier\nInfluential Outlier\n\n\n\nBoxplot\nTo create a boxplot of this variable:\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la['Population_2021'].plot(kind='box', title='LA Population 2021 Boxplot')\n\nplt.ylabel('Population')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\nWhat do you observe from this boxplot? There are lots of values above the"
  },
  {
    "objectID": "sessions/week1_practical.html#exploring-percentage_change",
    "href": "sessions/week1_practical.html#exploring-percentage_change",
    "title": "Practical 1: describing and representing data",
    "section": "Exploring Percentage_change",
    "text": "Exploring Percentage_change\nNow, we turn to explore the variable Percentage_change, which represents the relative change from the 2011 census to 2021 census.\nTry completing the code below on your own. Practice makes perfect!\n\nWhich LAs experienced the largest population percentage change? To what extent?\n\nQuestionAnswer\n\n\nprint(\"{} have the largest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[??['??'] == ??['??'].max(skipna=True), 'Area_name']), \n    df_pop_la['Population_2021'].??(skipna=True))\n    )\n\n\nprint(\"{} have the largest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Percentage_change'] == df_pop_la['Percentage_change'].max(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].max(skipna=True))\n    )\nTower Hamlets have the largest population percentage change of 22.1%\n\n\n\n\n\nWhich LAs experienced the smallest population percentage change? To what extent?\n\nQuestionAnswer\n\n\nprint(\"{} have the smallest population percentage change of {}%\".format(\n    \", \".??(df_pop_la.loc[df_pop_la[??] == ??['Percentage_change'].??(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].??(skipna=True))\n    )\n\n\nprint(\"{} have the smallest population percentage change of {}%\".format(\n    \", \".join(df_pop_la.loc[df_pop_la['Percentage_change'] == df_pop_la['Percentage_change'].min(skipna=True), 'Area_name']), \n    df_pop_la['Percentage_change'].min(skipna=True))\n    )\nKensington and Chelsea have the smallest population percentage change of -9.6%\n\n\n\n\n\nMake a boxplot of Percentage_change\n\nQuestionAnswer\n\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la[??].plot(kind=??, title='LA Population Percentage Change Boxplot')\n\nplt.??('Percentage change')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\nimport matplotlib.pyplot as plt\n\n# Create boxplot\ndf_pop_la['Percentage_change'].plot(kind='box', title='LA Population Percentage Change Boxplot')\n\nplt.ylabel('Percentage change')\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()"
  },
  {
    "objectID": "sessions/week1_practical.html#youre-done",
    "href": "sessions/week1_practical.html#youre-done",
    "title": "Practical 1: describing and representing data",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the first QM practical session! If you are still working on it, take you time.\nDon’t worry about understanding every detail of the Python code — what matters most is knowing which functions to use for a specific task, like checking minimum and maximum values or generating boxplots, and knowing how to debug when it goes wrong. Remember, practice makes perfect."
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#understanding-and-describing-data",
    "href": "sessions/week1_lecture.backup.html#understanding-and-describing-data",
    "title": "Exploratory Data Analysis #1",
    "section": "Understanding and describing data",
    "text": "Understanding and describing data\n\nQuantitative research is the process of collecting and analysing numerical data to describe, model, and predict variables of interest.\nGarbage in, garbage out.\n\n\nThis lecture focuses on understanding and describing data."
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#learning-objectives",
    "href": "sessions/week1_lecture.backup.html#learning-objectives",
    "title": "Exploratory Data Analysis #1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand the basics and classification of machine learning\nUnderstand the differences between statistical methods and machine learning (estimation vs. prediction)\nAppreciate several important theorems in machine learning and data science\nUnderstand the principle of supervised machine learning"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#four-levels-of-measurements",
    "href": "sessions/week1_lecture.backup.html#four-levels-of-measurements",
    "title": "Exploratory Data Analysis #1",
    "section": "Four levels of measurements",
    "text": "Four levels of measurements\n\n\n\n\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCategorizes and labels variables\n✔\n✔\n✔\n✔\n\n\nRanks categories in order\n\n✔\n✔\n✔\n\n\nHas known, equal intervals\n\n\n✔\n✔\n\n\nHas a true or meaningful zero\n\n\n\n✔\n\n\n\n\nDeveloped by psychologist Stanley Smith Stevens (1906 - 1973)"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#nominal",
    "href": "sessions/week1_lecture.backup.html#nominal",
    "title": "Exploratory Data Analysis #1",
    "section": "Nominal",
    "text": "Nominal\n\nDifferentiates items based only on names; no order between them.\nAlso called categorical data\nExample: colour, gender, country names\nWhat can be said about them?\n\nEquality: ‘apple’ is not ‘pear’, ‘apple’ is ‘apple’\nMode: the most common item"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#ordinal",
    "href": "sessions/week1_lecture.backup.html#ordinal",
    "title": "Exploratory Data Analysis #1",
    "section": "Ordinal",
    "text": "Ordinal\n\nAllow for rank order, but not the relative degree of difference\nExample: measurement of opinions\n\ncompletely agree\nmostly agree\nneither degree nor disagree\nmostly disagree\ncompletely disagree\n\nWhat can be said about them?\n\n✅ Equality; mode\n✅ Median: middle-ranked item\n❌ Differences between two levels; arithmetic mean"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#interval",
    "href": "sessions/week1_lecture.backup.html#interval",
    "title": "Exploratory Data Analysis #1",
    "section": "Interval",
    "text": "Interval\n\nAllow for degree of difference between items, but not the ratio\nThe zero value is arbitrary\nExample: Celsius temperature\n\nDefinition: define 0°C & 100°C, and then separate them into 100 intervals.\nDepends on altitude/elevation\n\nWhat can be said about them?\n\n✅ equality, mode, median\n✅ addition, arithmetic mean\n❌ ratio (100°C is NOT twice 50°C)"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#interval-1",
    "href": "sessions/week1_lecture.backup.html#interval-1",
    "title": "Exploratory Data Analysis #1",
    "section": "Interval",
    "text": "Interval\n\nAnother example: longtitude & latitude coordinates\nThe coordinate of 8 is twice as far as that of 4? ❌"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#ratio",
    "href": "sessions/week1_lecture.backup.html#ratio",
    "title": "Exploratory Data Analysis #1",
    "section": "Ratio",
    "text": "Ratio\n\nAllow for ratio between items\nThe zero value is unique and non-arbitrary.\nExample: mass, length, energy\nWhat can be said about them?\n\n✅ Equality, mode, median, arithmetic mean\n✅ Ratio (2kg is twice as heavy as 1kg)"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#notes",
    "href": "sessions/week1_lecture.backup.html#notes",
    "title": "Exploratory Data Analysis #1",
    "section": "Notes",
    "text": "Notes\n\n‘Encoding’ does not change the nature of a measurement.\nGender variable (male, female, others) is NOMINAL.\nIf this variable is encodeded {male:0, female:1, others:2}, is it NOMINAL, or INTERVAL?"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#another-categorisation-numerical-vs.-categorical",
    "href": "sessions/week1_lecture.backup.html#another-categorisation-numerical-vs.-categorical",
    "title": "Exploratory Data Analysis #1",
    "section": "Another categorisation: numerical vs. categorical",
    "text": "Another categorisation: numerical vs. categorical\n\n\n\n\n\n\n\n\nType\nCategory\nNotes\n\n\n\n\nQuantitative (numerical) data\nDiscrete data\nOnly in whole numbers, e.g. number of staffs\n\n\n\nContinuous data\ne.g. temperature, 23°C, 23.4°C\n\n\nQualitative (categorical) data\nNominal\nSame as nominal in ‘Levels of measurement’\n\n\n\nOrdinal\nSee above"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#quiz-time",
    "href": "sessions/week1_lecture.backup.html#quiz-time",
    "title": "Exploratory Data Analysis #1",
    "section": "Quiz time",
    "text": "Quiz time\n\nMentimeter quiz"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#key-metrics",
    "href": "sessions/week1_lecture.backup.html#key-metrics",
    "title": "Exploratory Data Analysis #1",
    "section": "Key metrics",
    "text": "Key metrics\n\n\n\nQuantity\nThe middle\nThe spread"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#us-city-population",
    "href": "sessions/week1_lecture.backup.html#us-city-population",
    "title": "Exploratory Data Analysis #1",
    "section": "US city population",
    "text": "US city population\n\n\n\nQuantity: 282 values\nThe middle: Mean 302869.3\nThe middle: Median 167744.5 (Very different from mean)\nThe middle: Mode 106433 (Useful? ❌)\nShould use mean or median? median is better than mean, as data is unlikely normal distributed"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#variance-quantifying-spread",
    "href": "sessions/week1_lecture.backup.html#variance-quantifying-spread",
    "title": "Exploratory Data Analysis #1",
    "section": "Variance: quantifying spread",
    "text": "Variance: quantifying spread\nDenote city population by \\([y_1, y_2, ..., y_n]\\) and variance by \\(\\sigma^2\\)\n\\[\n\\begin{aligned}\n\\sigma^2 &= \\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{n} \\\\\n&= \\frac{(y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\dots + (y_n - \\bar{y})^2}{n}\n\\end{aligned}\n\\]\nA large variance means considerable spreadedness in data."
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#standard-deviation-sigma",
    "href": "sessions/week1_lecture.backup.html#standard-deviation-sigma",
    "title": "Exploratory Data Analysis #1",
    "section": "Standard deviation \\(\\sigma\\)",
    "text": "Standard deviation \\(\\sigma\\)\n\\[\n\\begin{aligned}\n\\text{Standard Deviation} = \\sqrt{\\text{Variance}}\n\\end{aligned}\n\\]\n\nUnit: if \\(y\\) is in unit of meter (m), then St. Dev is in unit of m, variance in unit of \\(\\text{m}^2\\)"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#summary-of-key-metrics",
    "href": "sessions/week1_lecture.backup.html#summary-of-key-metrics",
    "title": "Exploratory Data Analysis #1",
    "section": "Summary of key metrics",
    "text": "Summary of key metrics\n\n\nQuantity\nThe middle: mean, median, mode\nThe spread: variance, standard deviation\nVisualising data is also important: BOXPLOT!\nPlease check two types of SPECIAL VALUES before computing these metrics."
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#ft-visual-vocabulary",
    "href": "sessions/week1_lecture.backup.html#ft-visual-vocabulary",
    "title": "Exploratory Data Analysis #1",
    "section": "FT visual vocabulary",
    "text": "FT visual vocabulary\n\nhttps://ft-interactive.github.io/visual-vocabulary/"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#boxplot-for-a-dataset",
    "href": "sessions/week1_lecture.backup.html#boxplot-for-a-dataset",
    "title": "Exploratory Data Analysis #1",
    "section": "Boxplot for a dataset",
    "text": "Boxplot for a dataset\n\nShowing distribution of a dataset"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#boxplot-for-comparing-multiple-datasets",
    "href": "sessions/week1_lecture.backup.html#boxplot-for-comparing-multiple-datasets",
    "title": "Exploratory Data Analysis #1",
    "section": "Boxplot for comparing multiple datasets",
    "text": "Boxplot for comparing multiple datasets"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#when-boxplot-fails",
    "href": "sessions/week1_lecture.backup.html#when-boxplot-fails",
    "title": "Exploratory Data Analysis #1",
    "section": "When boxplot fails?",
    "text": "When boxplot fails?\n\nWhen there are lots of ‘outliers’ in the data\nWhat are ‘Outliers’?"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#null-values",
    "href": "sessions/week1_lecture.backup.html#null-values",
    "title": "Exploratory Data Analysis #1",
    "section": "Null values",
    "text": "Null values\n\nRepresenting the absence of data or an unknow value\nDifferent from zero or a blank space\nNull values should be excluded before computing mean or std"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#null-island-in-geography",
    "href": "sessions/week1_lecture.backup.html#null-island-in-geography",
    "title": "Exploratory Data Analysis #1",
    "section": "Null island in geography",
    "text": "Null island in geography\n\nlong=0, lat=0. Not even an island!\nIt became famous because some software assigned “0,0” for long-lat when location is missing\nMany events were mapped to this one, including lots on Strava\nSource: wikipedia.org"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#outliers",
    "href": "sessions/week1_lecture.backup.html#outliers",
    "title": "Exploratory Data Analysis #1",
    "section": "Outliers",
    "text": "Outliers\n\nA data point that differs significantly from other observations\nThere are three types of outliers\nPlease explain the rationale for removing any identified outliers, including the criteria and methods used"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#outliers-1",
    "href": "sessions/week1_lecture.backup.html#outliers-1",
    "title": "Exploratory Data Analysis #1",
    "section": "Outliers",
    "text": "Outliers\n\n\n\n\n\n\n\n\nType\nSource\nHandling\n\n\n\n\nError Outliers\nFrom mistakes in data collection/entry/measurement, e.g. a temperature sensor reading 500 °C\nShould be corrected or removed\n\n\nIrregular Pattern Outliers\n\n\n\n\nInfluential Outliers"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#outliers-2",
    "href": "sessions/week1_lecture.backup.html#outliers-2",
    "title": "Exploratory Data Analysis #1",
    "section": "Outliers",
    "text": "Outliers\n\n\n\n\n\n\n\n\nType\nSource\nHandling\n\n\n\n\nError Outliers\n\n\n\n\nIrregular Pattern Outliers\nGenuinely occur, but do not follow general pattern or relationship in the dataset, e.g. sudden spikes in sales in Black Friday\nThey might indicate unusual events or anomalies worth investigating. If the purpose is to study overall pattern, they should be removed\n\n\nInfluential Outliers"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#outliers-3",
    "href": "sessions/week1_lecture.backup.html#outliers-3",
    "title": "Exploratory Data Analysis #1",
    "section": "Outliers",
    "text": "Outliers\n\n\n\n\n\n\n\n\nType\nSource\nHandling\n\n\n\n\nError Outliers\n\n\n\n\nIrregular Pattern Outliers\n\n\n\n\nInfluential Outliers\nAppear extreme but are integral to the underlying pattern or model, e.g. NYC in US city population data\nShould keep them, as removing them could distort the analysis or overlook important features of the data"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#detecting-outlier-interquartile-range-iqr",
    "href": "sessions/week1_lecture.backup.html#detecting-outlier-interquartile-range-iqr",
    "title": "Exploratory Data Analysis #1",
    "section": "Detecting outlier: interquartile range (IQR)",
    "text": "Detecting outlier: interquartile range (IQR)"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#quiz-which-type-of-outliers",
    "href": "sessions/week1_lecture.backup.html#quiz-which-type-of-outliers",
    "title": "Exploratory Data Analysis #1",
    "section": "Quiz: which type of outliers?",
    "text": "Quiz: which type of outliers?\n\nerror vs. irregular pattern vs. influential\n\n\nIn building height data, 10-storey building with 1m height\nIn gender ratios of countries data, Vatican City with gender ratio of 7:1\nIn UK city GDP data, GDP of Greater London"
  },
  {
    "objectID": "sessions/week1_lecture.backup.html#summary-describing-a-dataset",
    "href": "sessions/week1_lecture.backup.html#summary-describing-a-dataset",
    "title": "Exploratory Data Analysis #1",
    "section": "Summary: describing a dataset",
    "text": "Summary: describing a dataset\n\nKey metrics\nNull values and outliers (and handle them!)\nVisualising data"
  },
  {
    "objectID": "sessions/week1.html",
    "href": "sessions/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "This week will introduce definition and types of machine learning.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "1. Introduction to machine learning"
    ]
  },
  {
    "objectID": "sessions/week1.html#introduction",
    "href": "sessions/week1.html#introduction",
    "title": "Week 1",
    "section": "",
    "text": "This week will introduce definition and types of machine learning.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "1. Introduction to machine learning"
    ]
  },
  {
    "objectID": "sessions/week1.html#learning-objectives",
    "href": "sessions/week1.html#learning-objectives",
    "title": "Week 1",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand three primary types for machine learning.\nAppreciate Garbage in, Garbage out theorem.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "1. Introduction to machine learning"
    ]
  },
  {
    "objectID": "sessions/week1.html#lecture",
    "href": "sessions/week1.html#lecture",
    "title": "Week 1",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Supervised Learning",
      "1. Introduction to machine learning"
    ]
  },
  {
    "objectID": "sessions/week1.html#quiz",
    "href": "sessions/week1.html#quiz",
    "title": "Week 1",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "1. Introduction to machine learning"
    ]
  },
  {
    "objectID": "sessions/week1.html#practical",
    "href": "sessions/week1.html#practical",
    "title": "Week 1",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Supervised Learning",
      "1. Introduction to machine learning"
    ]
  },
  {
    "objectID": "sessions/week1.html#further-resources",
    "href": "sessions/week1.html#further-resources",
    "title": "Week 1",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Supervised Learning",
      "1. Introduction to machine learning"
    ]
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#framework",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#framework",
    "title": "Supervised learning",
    "section": "Framework",
    "text": "Framework\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in\n\\begin{cases}\n\\mathbb{R}, & \\text{(regression)} \\\\\n\\mathcal{Y} \\text{ (finite set)}, & \\text{(classification)}\n\\end{cases} \\\\\n\\text{learn } f(x_i) &\\approx y_i \\\\\n\\text{such that } f(x) &\\approx y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#regression-vs.-classification",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#regression-vs.-classification",
    "title": "Supervised learning",
    "section": "Regression vs. classification",
    "text": "Regression vs. classification\n\n\n\n\n\n\n\n\n\nRegression\nClassification\n\n\n\n\nTarget variable\nContinuous value (y )\nDiscrete label (y ) (finite set)\n\n\nTask\nPredict “how much” / “how many”\nPredict “which class” / “which category”\n\n\nIntuition\nFind a ‘line’ close to all points\nFind a ‘boundary’ between classis\n\n\nExample\nPredicting house prices from features\nPredicting spam vs. not spam for an email"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#example---linear-regression",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#example---linear-regression",
    "title": "Supervised learning",
    "section": "Example - linear regression",
    "text": "Example - linear regression\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndata = fetch_california_housing()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.2f}\")\nprint(f\"Intercept: {model.intercept_:.2f}\")\n\nMedInc: 0.45\nHouseAge: 0.01\nAveRooms: -0.12\nAveBedrms: 0.78\nPopulation: -0.00\nAveOccup: -0.00\nLatitude: -0.42\nLongitude: -0.43\nIntercept: -37.02"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#terms",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#terms",
    "title": "Supervised learning",
    "section": "Terms",
    "text": "Terms\n\n\n\n\nTerm\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\nAlgorithm\n\n\nprocedure that runs on data to create a model\n\n\nlinear regression\n\n\n\n\nModel\n\n\nan output by algorithm and data\n\n\n\\(\\hat{y}_i = \\sum_k \\beta_k x_{ik} + \\beta_0\\)\n\n\n\n\nMetric\n\n\nto evalute the model performace\n\n\nSquared error\n\n\n\n\nHyperparameter\n\n\nalgorithm settings, predefined by user instead of learned from data\n\n\nNone\n\n\n\n\nParameter\n\n\nmodel components learned from data\n\n\ncoefficients & intercept\n\n\n\n\nModel training\n\n\nprocess of estimating parameters from data\n\n\nusing maximum likelihood estimation"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#classification",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#classification",
    "title": "Supervised learning",
    "section": "Classification",
    "text": "Classification\n\nMost classification algorithms follow two steps:\n\n\npredict probabilities for each class\nassign class with highest probability\n\n\nWe can talk about common challenges for supervised learning (despite their differences)"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#common-challenges",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#common-challenges",
    "title": "Supervised learning",
    "section": "Common Challenges",
    "text": "Common Challenges\n\nTo select evaluation metrics (so that the model solves the right problem)\nTo design workflow (so that the model generalises well and avoids overfitting)"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression",
    "title": "Supervised learning",
    "section": "Metrics for regression",
    "text": "Metrics for regression\n\n\n\n\n\n\n\nformula\n\n\nunit\n\n\nnotes\n\n\n\n\n\n\nRMSE\n\n\n\\(\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\)\n\n\nSame unit as target \\(y\\)\n\n\nPenalises large errors more; sensitive to outliers\n\n\n\n\nR²\n\n\n\\(1 - \\dfrac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\)\n\n\nDimensionless (between 0 and 1 for most cases)\n\n\nMeasures proportion of variance explained by the model; can be negative if model is worse than \\(y_i=\\bar{y}\\)\n\n\n\n\nMAE\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^{n}\\lvert y_i - \\hat{y}_i\\rvert\\)\n\n\nSame unit as target \\(y\\)\n\n\nMore robust to outliers than RMSE; interpretable as average absolute error\n\n\n\n\nMAPE\n\n\n\\(\\frac{100}{n}\\sum_{i=1}^{n}\\left\\lvert \\frac{y_i - \\hat{y}_i}{y_i} \\right\\rvert\\)\n\n\nPercent (%)\n\n\nsensitive to very small \\(y_i\\); relative error"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression-1",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression-1",
    "title": "Supervised learning",
    "section": "Metrics for regression",
    "text": "Metrics for regression\n\nUsing RMSE or squared error in most cases\nBy default, regressors in sklearn and XGBoost use Squared Error as loss function\nDifference between R2 for OLS (within [0,1]) vs. R2 for regression (&lt;=1, can be negative)"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-classification-binary",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-classification-binary",
    "title": "Supervised learning",
    "section": "Metrics for classification (binary)",
    "text": "Metrics for classification (binary)\n\n\n\nImage Credit: COMS4995-s20\n\n\n\\(Accuracy=\\frac{TP+TN}{TP+TN+FP+FN}\\)\n\nSelecting Positive label: often the minority class"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#example-using-breast-cancer-dataset",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#example-using-breast-cancer-dataset",
    "title": "Supervised learning",
    "section": "Example using Breast Cancer dataset",
    "text": "Example using Breast Cancer dataset\n\n\nclass malignant: 212, 37.258%\nclass benign: 357, 62.742%\nPredictive accuracy: 0.930"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#limitation-of-accuracy-accuracy-paradox",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#limitation-of-accuracy-accuracy-paradox",
    "title": "Supervised learning",
    "section": "Limitation of accuracy (Accuracy paradox)",
    "text": "Limitation of accuracy (Accuracy paradox)\n\nScenario: Data with 90% negatives (imbalanced data)\nA majority strategy that predicts all as negative gets 90% accuracy, but this is useless.\nDifferent models can have the same accuracy (0.9) but make very different types of errors.\n\ny_pred_1: Predicts all negative (90 TN, 0 TP)\ny_pred_2: Predicts some positives correctly but misses others\ny_pred_3: A mix of errors"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#precision-recall-f1-score-auc",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#precision-recall-f1-score-auc",
    "title": "Supervised learning",
    "section": "Precision, Recall, F1-score, AUC",
    "text": "Precision, Recall, F1-score, AUC\n\nPrecision (Positive Predicted Value): \\(\\frac{TP}{TP+FP}\\). Among predicted positives, how many are actually positive.\nRecall (Sensitivity, True Positive Rate): \\(\\frac{TP}{TP+FN}\\). Among actual positives, how many are correctly predicted.\nF1-score (Harmonic mean of precision & recall): \\(F=2\\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision}+\\text{recall}}\\)"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#addressing-accuracy-paradox",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#addressing-accuracy-paradox",
    "title": "Supervised learning",
    "section": "Addressing accuracy paradox",
    "text": "Addressing accuracy paradox"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#trade-off-between-precision-and-recall",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#trade-off-between-precision-and-recall",
    "title": "Supervised learning",
    "section": "Trade-off between precision and recall",
    "text": "Trade-off between precision and recall\n\nWhen precision increases, recall decreases. Vice versa.\nBy changing classification threshold (default=0.5, from 0 to 1), we can see the balance between precision and recall.\n\n\n\n\n\n\nPrecision–Recall curve for LR and Random Forest"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#roc-curve",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#roc-curve",
    "title": "Supervised learning",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nReceiver Operating Characteristic (ROC) curve plots True Positive Rate vs. False Positive Rate. Similar to precision-recall curve.\nThe identity line y=x represents random classifier (e.g. tossing a coin)."
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#auc-area-under-roc-curve",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#auc-area-under-roc-curve",
    "title": "Supervised learning",
    "section": "AUC (Area under ROC Curve)",
    "text": "AUC (Area under ROC Curve)\n\nThe integral of the ROC curve (or the area size under the curve)\nAUC is always 0.5 for random predictions\nMaximum AUC is 1.0 for perfect predictions"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#aggregating-metrics-across-classes",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#aggregating-metrics-across-classes",
    "title": "Supervised learning",
    "section": "Aggregating metrics across classes",
    "text": "Aggregating metrics across classes\n\nMacro average: \\(\\frac{1}{|L|}\\sum_{l\\in L}R(y_{l},\\hat{y}_{l})\\)\nUnweighted mean of per-class scores. Each class contributes equally, regardless of sample size.\nMicro average: \\(\\frac{1}{n}\\sum_{i=1}^{n}R(y_{i},\\hat{y}_{i})\\)\nSum individual TP, FP, FN, TN across all classes, then compute metric. Equivalent to accuracy for multiclass.\nWeighted average: \\(\\frac{1}{n}\\sum_{l\\in L}n_{l}R(y_{l},\\hat{y}_{l})\\)\nWeighted by support (number of samples in each class). Balances class sizes in the final score."
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#classification-report",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#classification-report",
    "title": "Supervised learning",
    "section": "Classification report",
    "text": "Classification report\n\nprint(classification_report(y_test, rf.predict(X_test), target_names=data.target_names))\n\n              precision    recall  f1-score   support\n\n   malignant       0.92      0.92      0.92        53\n      benign       0.96      0.96      0.96        90\n\n    accuracy                           0.94       143\n   macro avg       0.94      0.94      0.94       143\nweighted avg       0.94      0.94      0.94       143"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#picking-a-metric",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#picking-a-metric",
    "title": "Supervised learning",
    "section": "Picking a metric",
    "text": "Picking a metric\n\nReal-world problems are rarely balanced.\nAccuracy is rarely what you want.\nFind the right criterion for the specific task.\nDecide between emphasis on recall or precision.\nIdentify which classes are important."
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metric-for-breast-cancer-detection",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#metric-for-breast-cancer-detection",
    "title": "Supervised learning",
    "section": "Metric for breast cancer detection",
    "text": "Metric for breast cancer detection\n\n“1” indicates malignant/cancer, “0” indicates benign/no cancer.\nMissing a cancer (FN) is much worse than a false alarm (FP)\nSo, we care more about recall than precision or accuracy.\nA model with high recall is preferred, even if it has lower precision."
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#next-question-how-can-i-optimise-for-a-specific-metric",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#next-question-how-can-i-optimise-for-a-specific-metric",
    "title": "Supervised learning",
    "section": "Next question: how can I optimise for a specific metric?",
    "text": "Next question: how can I optimise for a specific metric?\n\nRandomForestClassifier (and other classifiers) in sklearn by default optimises for accuracy and have no direct way to optimise for recall.\nWe have workarounds (topics for next week)\n\nUse recall metric during hyperparameter tuning and cross-validation\nAdjust classification threshold after training\nUse class weights to penalise misclassifications of the positive class more heavily during training"
  },
  {
    "objectID": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#generalisation-to-multi-class",
    "href": "sessions/supervised_learning_metrics/supervised_learning_metrics.html#generalisation-to-multi-class",
    "title": "Supervised learning",
    "section": "Generalisation to multi-class",
    "text": "Generalisation to multi-class\n\nMost metrics can be generalised to multi-class using macro, micro, or weighted averaging.\nROC curve and AUC can be computed using one-vs-rest approach for each class and then averaged."
  },
  {
    "objectID": "sessions/index.html",
    "href": "sessions/index.html",
    "title": "Overview",
    "section": "",
    "text": "Understand the structure and focus of the module.\nDevelop a method for tackling quantitative problems.\nFormulate a research question and structure quantitative writing.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#objectives",
    "href": "sessions/index.html#objectives",
    "title": "Overview",
    "section": "",
    "text": "Understand the structure and focus of the module.\nDevelop a method for tackling quantitative problems.\nFormulate a research question and structure quantitative writing.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#why-study-a-quantitative-methods-course",
    "href": "sessions/index.html#why-study-a-quantitative-methods-course",
    "title": "Overview",
    "section": "Why study a Quantitative Methods course?",
    "text": "Why study a Quantitative Methods course?\n\nCoding alone is not enough.\nUnderstanding models aids in correct tool selection and bug handling.\nGoogle/ChatGPT can make mistakes; detecting them is crucial.\nMathematics understanding is not always required.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#course-objectives",
    "href": "sessions/index.html#course-objectives",
    "title": "Overview",
    "section": "Course Objectives",
    "text": "Course Objectives\n\nUnderstand a broad range of quantitative techniques.\nApply these skills in research.\nFormulate a coherent quantitative argument.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#prerequisites",
    "href": "sessions/index.html#prerequisites",
    "title": "Overview",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nNo prerequisite of university-level maths/statistics.\nNo prerequisite programming, but this module doesn’t teach programming.\nCASA0013 is strongly recommneded if you don’t know Python before.\nPython is required for practicals and assessments.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#course-structure",
    "href": "sessions/index.html#course-structure",
    "title": "Overview",
    "section": "Course Structure",
    "text": "Course Structure\n\nLectures: 10 weeks (Wednesdays 9:00–10:30).\nTutorials: 10 weeks (Wednesdays 10:30–12:00).",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#platforms",
    "href": "sessions/index.html#platforms",
    "title": "Overview",
    "section": "Platforms",
    "text": "Platforms\n\nEmail for important notices and private questions.\nGithub & website for lecture notes and notebooks.\nMoodle for lectures recording and assessments.\nSlack for public questions.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#weekly-schedule",
    "href": "sessions/index.html#weekly-schedule",
    "title": "Overview",
    "section": "Weekly Schedule",
    "text": "Weekly Schedule\n\n\n\nSession\nTopic\nLecturer\n\n\n\n\n1\nIntroduction to data\nHuanfa\n\n\n2\nProbability and distribution\nBea\n\n\n3\nHypothesis testing\nBea\n\n\n4\nIntroduction to linear algebra\nBea\n\n\n5\nCorrelation and regression\nHuanfa\n\n\n6\nMultiple regression\nAdam\n\n\n7\nGeneralised linear model\nAdam\n\n\n8\nMultilevel regression\nAdam\n\n\n9\nDimensionality reduction\nHuanfa\n\n\n10\nClustering Analysis\nHuanfa",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#attendance-recording",
    "href": "sessions/index.html#attendance-recording",
    "title": "Overview",
    "section": "Attendance Recording",
    "text": "Attendance Recording\n\n70% attendance required for student visa holders.\nPlease attend all lectures and workshops.\nContact module lead & bartlett.pg-casa@ucl.ac.uk, if you can’t attend.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#assessment-1",
    "href": "sessions/index.html#assessment-1",
    "title": "Overview",
    "section": "Assessment",
    "text": "Assessment\n\nWritten Investigation (summative): 100%\nWeekly quiz (formative)",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#ucl-assessment-policy",
    "href": "sessions/index.html#ucl-assessment-policy",
    "title": "Overview",
    "section": "UCL Assessment Policy",
    "text": "UCL Assessment Policy\n\nAll submissions via Moodle, not emails.\nLate penalties: Up to 48h (-10 points); up to 7 days (capped at 50); over 7 days (scores 0).\nDAP or Extenuating circumstances: to submit on Portico.\nRespect word count limits\nAvoid plagiarism and fake references",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#moodle-feedback",
    "href": "sessions/index.html#moodle-feedback",
    "title": "Overview",
    "section": "Moodle Feedback",
    "text": "Moodle Feedback\nPlease provide anonymous feedback on Moodle for every week.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/index.html#github-feedback",
    "href": "sessions/index.html#github-feedback",
    "title": "Overview",
    "section": "Github Feedback",
    "text": "Github Feedback\nYou can also give feedback on Github issues.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#intro-to-mlops",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#intro-to-mlops",
    "title": "MLOps & Moving to Scripts",
    "section": "Intro to MLOps",
    "text": "Intro to MLOps\n\nwith MLOps, we strive to make machine learning models reliable and maintainable in production\nthree key stages\n\n\n\n\nImage Credit: https://ml-ops.org/content/mlops-principles"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#automation-in-mlops",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#automation-in-mlops",
    "title": "MLOps & Moving to Scripts",
    "section": "Automation in MLOps",
    "text": "Automation in MLOps\n\nManual process: everything is done by hand; using Rapid Application Development (RAD) tools like Jupyter Notebooks.\nML pipline automation: with continuous training (when new data comes in, retrain the model automatically).\nCI/CD pipeline automation: to perform fast & reliable ML model deployment, usually on Cloud (AWS/GCP/Azure)"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#recommendations-for-mlops-online-courses",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#recommendations-for-mlops-online-courses",
    "title": "MLOps & Moving to Scripts",
    "section": "Recommendations for MLOps online courses",
    "text": "Recommendations for MLOps online courses\n\nGetting a MLOps certificate from GCP/AWS is a plus for job hunting\nGo for big providers: Google Cloud (GCP), AWS, Azure\nDon’t forget to turn off MLOps on Cloud to avoid unexpected charges!\nMLOps on Google Skills\nMLOps on AWS"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#focus-today-moving-to-scripts",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#focus-today-moving-to-scripts",
    "title": "MLOps & Moving to Scripts",
    "section": "Focus Today: Moving to Scripts",
    "text": "Focus Today: Moving to Scripts\n\nPreerquisites: using VSCode (or other IDE) instead of Jupyter Notebooks\nWhy? VScode better supports managing many files, with functions like ‘Go to Definition’, ‘Refactor’, etc.\nVSCode works well with conda, docker, Podman, git, etc.\nVSCode is industry standard IDE; good to get used to it early\nvSCode works with LLM plugins: Github copilot, Gemini, etc."
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#python-notebooks-vs-scripts",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#python-notebooks-vs-scripts",
    "title": "MLOps & Moving to Scripts",
    "section": "Python Notebooks vs Scripts",
    "text": "Python Notebooks vs Scripts\nNotebooks have been great so far for development and testing new ideas - Interactive: mixing code, text, and images; story-telling - Stateful: saved code state to global state, so don’t have to rerun code - Great for exploration and prototyping"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#problems-with-notebooks",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#problems-with-notebooks",
    "title": "MLOps & Moving to Scripts",
    "section": "Problems with Notebooks",
    "text": "Problems with Notebooks\n\nNon-linear execution order: hard to track dependencies and what have run\nHard to version control: diffs are messy, hard to review\nDifficult to test: can’t easily run unit tests or integration tests\nNot ideal for production: hard to deploy, monitor, and maintain"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#moving-to-scripts-.py",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#moving-to-scripts-.py",
    "title": "MLOps & Moving to Scripts",
    "section": "Moving to Scripts (.py)",
    "text": "Moving to Scripts (.py)\n\nstateless: each run starts fresh; have to explicitly pass variables to functions and classes\nlinear: we have to run from top to bottom\nmodular: we can split code into functions and classes\ntestable: we can write unit tests and integration tests"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#steps-to-move-from-notebooks-to-scripts",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#steps-to-move-from-notebooks-to-scripts",
    "title": "MLOps & Moving to Scripts",
    "section": "Steps to move from notebooks to scripts",
    "text": "Steps to move from notebooks to scripts\n\nRemove non-essential cells (print X shape, df.describe())\nRefacto code into functions (and classes)\nCombine related functions into Python files (modules)"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#more-considerations",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#more-considerations",
    "title": "MLOps & Moving to Scripts",
    "section": "More considerations",
    "text": "More considerations\n\nUse configuration files (YAML, JSON, config.py) to manage parameters; avoid hardcoding values in other .py files\nAvoid duplicated code; don’t write the same code in multiple places\nuse functions and utils.py to promote code reuse\nWrite unit tests to ensure code correctness and reliability"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#improving-notebooks-as-we-still-use-them",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#improving-notebooks-as-we-still-use-them",
    "title": "MLOps & Moving to Scripts",
    "section": "Improving notebooks (as we still use them)",
    "text": "Improving notebooks (as we still use them)\n\nAvoid very long outputs (e.g. print 100 lines of dataframe)\nAvoid unnecessary EDA (e.g. df.head(), df.describe()). If a notebook contains data processing, only call describe() ONCE after processing\nMake sure plots are properly labelled with titles, axis labels, legends\nIf some outputs are important, save them to files (images -&gt; jpeg/png, models -&gt; pickle/joblib) instead of relying on notebook state\nRestart kernel and rerun all cells before committing changes or sharing; avoid non-linear execution issues"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#using-sklearn-pipelines-to-improve-readability",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#using-sklearn-pipelines-to-improve-readability",
    "title": "MLOps & Moving to Scripts",
    "section": "Using sklearn pipelines to improve readability",
    "text": "Using sklearn pipelines to improve readability\n\nsklearn pipelines help organise code, avoid data leakage, and improve reproducibility\n\n\n\n\nImage Credit: sci-learn.org"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#key-references",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#key-references",
    "title": "MLOps & Moving to Scripts",
    "section": "Key references",
    "text": "Key references\n\nmadewithml: Moving from Notebooks to Scripts\nConvert ML experiments to production Python code, from Azure"
  },
  {
    "objectID": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#summary",
    "href": "sessions/W09_MLOps_moving_to_scripts/MLOps_moving_to_scripts.html#summary",
    "title": "MLOps & Moving to Scripts",
    "section": "Summary",
    "text": "Summary\n\nMLOps ensures reliable and maintainable ML models in production\nMoving from notebooks to scripts improves code quality, testability, and deployability\nUsing IDEs like VSCode enhances productivity and code management\nEnsure good coding practice in notebooks"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#calibration",
    "href": "sessions/W08_imbalanced_data/calibration.html#calibration",
    "title": "Calibration",
    "section": "Calibration",
    "text": "Calibration\n\nProbabilities can be more informative than labels\nExample: “no cancer” vs “40% likely to have cancer”\nGoal: accurate probability estimates from any classifier"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#calibration-curve-reliability-diagram",
    "href": "sessions/W08_imbalanced_data/calibration.html#calibration-curve-reliability-diagram",
    "title": "Calibration",
    "section": "Calibration Curve (Reliability Diagram)",
    "text": "Calibration Curve (Reliability Diagram)\n\n\n\n\n\n\n\n\n\n\n\nFor binary classification\nA model can be calibrated yet inaccurate\nBin predictions, plot fraction positive per bin\nDoes not require ground-truth probabilities"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#calibration_curve-with-sklearn",
    "href": "sessions/W08_imbalanced_data/calibration.html#calibration_curve-with-sklearn",
    "title": "Calibration",
    "section": "calibration_curve with sklearn",
    "text": "calibration_curve with sklearn\n\n\n\nUsing subsample of covertype dataset\nLogisticRegressionCV baseline\n\nlr = LogisticRegressionCV().fit(X_train, y_train)\nprobs = lr.predict_proba(X_test)[:, 1]\nprob_true, prob_pred = calibration_curve(y_test, probs, n_bins=5)"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#influence-of-number-of-bins",
    "href": "sessions/W08_imbalanced_data/calibration.html#influence-of-number-of-bins",
    "title": "Calibration",
    "section": "Influence of Number of Bins",
    "text": "Influence of Number of Bins\n\n\n\n\nMore bins give resolution but add noise\nLarge datasets tolerate more bins; small datasets get noisy"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#comparing-models",
    "href": "sessions/W08_imbalanced_data/calibration.html#comparing-models",
    "title": "Calibration",
    "section": "Comparing Models",
    "text": "Comparing Models\n\n\n\n\nLogistic regression well calibrated\nTrees often overconfident; forests underconfident"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#brier-score-binary",
    "href": "sessions/W08_imbalanced_data/calibration.html#brier-score-binary",
    "title": "Calibration",
    "section": "Brier Score (Binary)",
    "text": "Brier Score (Binary)\n\nMean squared error of probability estimate \\[BS = \\frac{\\sum_{i=1}^{n} (\\hat{p}(y_i)-y_i)^2}{n}\\]\n\n\n\nSmaller is better; mixes accuracy and calibration"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#fixing-it-calibrating-a-classifier",
    "href": "sessions/W08_imbalanced_data/calibration.html#fixing-it-calibrating-a-classifier",
    "title": "Calibration",
    "section": "Fixing It: Calibrating a Classifier",
    "text": "Fixing It: Calibrating a Classifier\n\nLearn mapping from model scores to better probabilities\nUsually 1d model; works even without native probabilities\nTrain calibration on hold-out or via cross-validation"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#platt-scaling",
    "href": "sessions/W08_imbalanced_data/calibration.html#platt-scaling",
    "title": "Calibration",
    "section": "Platt Scaling",
    "text": "Platt Scaling\n\nUse logistic sigmoid as calibration function\nWorks well for SVMs \\[f_{platt} = \\frac{1}{1 + \\exp(-w s(x) - b)}\\]"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#isotonic-regression",
    "href": "sessions/W08_imbalanced_data/calibration.html#isotonic-regression",
    "title": "Calibration",
    "section": "Isotonic Regression",
    "text": "Isotonic Regression\n\nFlexible monotone step-function calibrator\nMinimizes MSE under monotonicity"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#building-the-model",
    "href": "sessions/W08_imbalanced_data/calibration.html#building-the-model",
    "title": "Calibration",
    "section": "Building the Model",
    "text": "Building the Model\n\nUsing the training set for calibration overfits\nUse hold-out set or cross-validation for unbiased probabilities"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#fitting-the-calibration-model",
    "href": "sessions/W08_imbalanced_data/calibration.html#fitting-the-calibration-model",
    "title": "Calibration",
    "section": "Fitting the Calibration Model",
    "text": "Fitting the Calibration Model"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#fitting-the-calibration-model-fitted-curves",
    "href": "sessions/W08_imbalanced_data/calibration.html#fitting-the-calibration-model-fitted-curves",
    "title": "Calibration",
    "section": "Fitting the Calibration Model (Fitted Curves)",
    "text": "Fitting the Calibration Model (Fitted Curves)"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#calibratedclassifiercv",
    "href": "sessions/W08_imbalanced_data/calibration.html#calibratedclassifiercv",
    "title": "Calibration",
    "section": "CalibratedClassifierCV",
    "text": "CalibratedClassifierCV\n\n\n\nCalibrate a pretrained model on validation data\n\nrf = RandomForestClassifier().fit(X_train_sub, y_train_sub)\nscores = rf.predict_proba(X_test)[:, 1]\nplot_calibration_curve(y_test, scores, n_bins=20)"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#calibration-on-random-forest",
    "href": "sessions/W08_imbalanced_data/calibration.html#calibration-on-random-forest",
    "title": "Calibration",
    "section": "Calibration on Random Forest",
    "text": "Calibration on Random Forest\n\n\ncal_rf = CalibratedClassifierCV(rf, cv=\"prefit\", method=\"sigmoid\")\ncal_rf.fit(X_val, y_val)\nscores_sigm = cal_rf.predict_proba(X_test)[:, 1]\n\ncal_rf_iso = CalibratedClassifierCV(rf, cv=\"prefit\", method=\"isotonic\")\ncal_rf_iso.fit(X_val, y_val)\nscores_iso = cal_rf_iso.predict_proba(X_test)[:, 1]\n\n\n\n\n\n\nHold-out calibration with sigmoid or isotonic"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#cross-validated-calibration",
    "href": "sessions/W08_imbalanced_data/calibration.html#cross-validated-calibration",
    "title": "Calibration",
    "section": "Cross-Validated Calibration",
    "text": "Cross-Validated Calibration\n\n\ncal_rf_iso_cv = CalibratedClassifierCV(rf, method=\"isotonic\")\ncal_rf_iso_cv.fit(X_train, y_train)\nscores_iso_cv = cal_rf_iso_cv.predict_proba(X_test)[:, 1]\n\n\n\n\n\n\nUses CV folds; averages calibrated models; more trees, better calibration"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/calibration.html#multi-class-calibration",
    "href": "sessions/W08_imbalanced_data/calibration.html#multi-class-calibration",
    "title": "Calibration",
    "section": "Multi-Class Calibration",
    "text": "Multi-Class Calibration\n\n\n\n\nCalibrate per class then renormalize"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#principles",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#principles",
    "title": "Graph Neural Networks",
    "section": "Principles",
    "text": "Principles\n\nA deep learning framework for graph-structured data\nIt generalises traditional neural networks to handle graph data by leveraging the relationships between nodes in a graph.\nMessage passing: nodes aggregate neighbor features over the graph\nSpatial vs spectral views of convolution on graphs\nTransductive vs inductive learning; sampling enables scalability\nLearnable transformations per layer; nonlinearity and normalization matter\nVariants differ in how neighbors are aggregated and weighted"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#five-major-gnn-architectures",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#five-major-gnn-architectures",
    "title": "Graph Neural Networks",
    "section": "Five Major GNN Architectures",
    "text": "Five Major GNN Architectures\n\n\n\nDeepWalk: unsupervised node embeddings via random walks (word2vec-like)\nGCN: shared filters; normalised neighborhood aggregation (spectral → spatial)\nGraphSAGE: inductive sampling + learnable aggregators (mean/pool/LSTM)\nGAT: attention over neighbors; learns edge weights; multi-head for capacity\nChebNet: spectral filtering via Chebyshev polynomials; localized convolutions"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graph-convolutional-networks-gcn",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graph-convolutional-networks-gcn",
    "title": "Graph Neural Networks",
    "section": "Graph Convolutional Networks (GCN)",
    "text": "Graph Convolutional Networks (GCN)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#introduction-graphs",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#introduction-graphs",
    "title": "Graph Neural Networks",
    "section": "Introduction: Graphs",
    "text": "Introduction: Graphs\n\n\n\nGraph = organized data representation\nConsists of vertices (nodes) V and edges E\nEdges can be weighted or binary\nDirected or undirected graphs\n\nExample graph:\n\\[V = \\{A, B, C, D, E, F, G\\}\\] \\[E = \\{(A,B), (B,C), (C,E), ...\\}\\]"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graph-terminology",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graph-terminology",
    "title": "Graph Neural Networks",
    "section": "Graph Terminology",
    "text": "Graph Terminology\n\n\n\n\nNode: An entity in the graph (represented by circles)\nEdge: Line joining two nodes (represents relationships)\nDegree: Number of edges incident with a vertex\nAdjacency Matrix: N×N matrix representing graph structure"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#why-gcns",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#why-gcns",
    "title": "Graph Neural Networks",
    "section": "Why GCNs?",
    "text": "Why GCNs?\n\nMost real-world datasets come as graphs or networks:\n\nSocial networks\nProtein-interaction networks\nThe World Wide Web\n\nLearning on graphs enables domain-specific insights\nConventional CNNs assume compositional structure on Euclidean space"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#cnns-vs-gcns",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#cnns-vs-gcns",
    "title": "Graph Neural Networks",
    "section": "CNNs vs GCNs",
    "text": "CNNs vs GCNs\n\n\nCNN Key Properties:\n\nLocality\nStationarity (Translation Invariance)\nMulti-scale hierarchies\n\nProblem: Not all data lies on Euclidean space!"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#applications-of-gcns",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#applications-of-gcns",
    "title": "Graph Neural Networks",
    "section": "Applications of GCNs",
    "text": "Applications of GCNs\n\n\n\nFacebook Link Prediction for Suggesting Friends using Social Networks\n\n\n\nFriend prediction algorithms\nSocial network analysis\nProtein interaction prediction\nKnowledge graph completion"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#what-are-gcns",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#what-are-gcns",
    "title": "Graph Neural Networks",
    "section": "What are GCNs?",
    "text": "What are GCNs?\n\nNeural networks operating on graphs\nCapture neighbourhood information for non-euclidean spaces\nRe-define convolution for graph domains\n\nTwo Styles:\n\nSpectral GCNs: Graph signal processing perspective\nSpatial GCNs: Aggregate feature information from neighbours (more flexible)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#how-gcns-work-friend-prediction",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#how-gcns-work-friend-prediction",
    "title": "Graph Neural Networks",
    "section": "How GCNs Work: Friend Prediction",
    "text": "How GCNs Work: Friend Prediction\n\n\nProblem: Predict future friendships\n\nGraph where edges = friendships\nCommon friends → higher likelihood\n\\((1,3)\\) have 2 common friends\n\\((1,5)\\) have 0 common friends"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-mathematical-formulation",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-mathematical-formulation",
    "title": "Graph Neural Networks",
    "section": "GCN Mathematical Formulation",
    "text": "GCN Mathematical Formulation\nLayer-wise propagation:\n\\[H^{i} = f(H^{i-1}, A)\\]\nSimple example:\n\\[f(H^{i}, A) = σ(AH^{i}W^{i})\\]\nwhere:\n\n\\(A\\) = N × N adjacency matrix\n\\(X\\) = input feature matrix (N × F)\n\\(σ\\) = ReLU activation function\n\\(H^{0} = X\\) (initial features)\nEach layer aggregates neighborhood features"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#problems-with-simple-formulation",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#problems-with-simple-formulation",
    "title": "Graph Neural Networks",
    "section": "Problems with Simple Formulation",
    "text": "Problems with Simple Formulation\n\n\nProblem 1: No self-representation\n\nNew features don’t include node’s own features\nSolution: Add self-loops\n\nProblem 2: Degree scaling\n\nHigh-degree nodes get larger values\nLow-degree nodes get smaller values\nSolution: Normalize by degree\n\n\nFixes:\n\nAdd identity: \\(\\hat{A} = A + I\\)\nSymmetric normalization:\n\n\\[\\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}\\]\nwhere \\(\\hat{D}\\) is degree matrix of \\(\\hat{A}\\)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#final-gcn-propagation-rule",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#final-gcn-propagation-rule",
    "title": "Graph Neural Networks",
    "section": "Final GCN Propagation Rule",
    "text": "Final GCN Propagation Rule\n\\[f(H^{(l)}, A) = \\sigma\\left( \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}\\right)\\]\nwhere:\n\n\\(\\hat{A} = A + I\\) (adjacency + self-loops)\n\\(I\\) = identity matrix\n\\(\\hat{D}\\) = diagonal degree matrix of \\(\\hat{A}\\)\n\\(W^{(l)}\\) = learnable weight matrix for layer \\(l\\)\n\\(\\sigma\\) = activation function"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graphsage-sample-and-aggregate",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graphsage-sample-and-aggregate",
    "title": "Graph Neural Networks",
    "section": "GraphSAGE (Sample and Aggregate)",
    "text": "GraphSAGE (Sample and Aggregate)\n\n\n\nInductive learning: generalises to unseen nodes/graphs\nNeighborhood sampling for scalability (mini-batches)\nAggregators: mean, pool (MLP+max), LSTM\nConcatenate self-representation with aggregated neighbors\nNormalise embeddings; learn weights per layer\n\n\n\n\n\nInductive representation learning on large graphs"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graphsage-aggregators",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#graphsage-aggregators",
    "title": "Graph Neural Networks",
    "section": "GraphSAGE Aggregators",
    "text": "GraphSAGE Aggregators\n\n\n\nMean aggregator: elementwise mean over neighbors\nPool aggregator: transform neighbor features, then max-pool\nLSTM aggregator: sequence model over randomly ordered neighbors"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gat",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gat",
    "title": "Graph Neural Networks",
    "section": "GAT",
    "text": "GAT\n\n\n\nLearn attention weights over neighbors; masked self-attention\nReplaces fixed normalization in GCN with learned coefficients\nMulti-head attention: concatenate intermediate heads; average at output\nImproves interpretability via attention visualization; handles heterophily better"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-vs-gat-aggregation",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-vs-gat-aggregation",
    "title": "Graph Neural Networks",
    "section": "GCN vs GAT Aggregation",
    "text": "GCN vs GAT Aggregation"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gat-multi-head-attention",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gat-multi-head-attention",
    "title": "Graph Neural Networks",
    "section": "GAT: Multi-head Attention",
    "text": "GAT: Multi-head Attention\n\n\n\nConcatenate intermediate heads; average at final layer"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#implementation-in-pytorch",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#implementation-in-pytorch",
    "title": "Graph Neural Networks",
    "section": "Implementation in PyTorch",
    "text": "Implementation in PyTorch"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-convolutional-layer",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-convolutional-layer",
    "title": "Graph Neural Networks",
    "section": "GCN Convolutional Layer",
    "text": "GCN Convolutional Layer\nclass GCNConv(nn.Module):\n    def __init__(self, A, in_channels, out_channels):\n        super(GCNConv, self).__init__()\n        self.A_hat = A + torch.eye(A.size(0))\n        self.D     = torch.diag(torch.sum(A,1))\n        self.D     = self.D.inverse().sqrt()\n        self.A_hat = torch.mm(torch.mm(self.D, self.A_hat), self.D)\n        self.W     = nn.Parameter(torch.rand(in_channels,out_channels))\n    \n    def forward(self, X):\n        out = torch.relu(torch.mm(torch.mm(self.A_hat, X), self.W))\n        return out"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-network-architecture",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#gcn-network-architecture",
    "title": "Graph Neural Networks",
    "section": "GCN Network Architecture",
    "text": "GCN Network Architecture\nclass Net(torch.nn.Module):\n    def __init__(self, A, nfeat, nhid, nout):\n        super(Net, self).__init__()\n        self.conv1 = GCNConv(A, nfeat, nhid)\n        self.conv2 = GCNConv(A, nhid, nout)\n        \n    def forward(self, X):\n        H  = self.conv1(X)\n        H2 = self.conv2(H)\n        return H2\n\nStack multiple GCN layers\nLearn hierarchical representations\nOutput layer for classification/regression"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#case-study-zacharys-karate-club",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#case-study-zacharys-karate-club",
    "title": "Graph Neural Networks",
    "section": "Case Study: Zachary’s Karate Club",
    "text": "Case Study: Zachary’s Karate Club\n\n\nHistorical Context (1970-1972):\n\nObserved local karate club\nConflict between administrator “John A” and instructor “Mr. Hi”\nClub split into two groups\nPredict which members join which group"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#semi-supervised-learning-setup",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#semi-supervised-learning-setup",
    "title": "Graph Neural Networks",
    "section": "Semi-Supervised Learning Setup",
    "text": "Semi-Supervised Learning Setup\n\nLabels known for only 2 nodes: John A (0) and Mr. Hi (1)\nPredict labels for all other members based on graph structure\nUse graph connectivity to propagate information\n\n# Only nodes 0 and 33 are labeled\ntarget = torch.tensor([0,-1,-1,-1,...,-1,-1,1])\n\n# Feature matrix (one-hot encoding)\nX = torch.eye(A.size(0))"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#training-the-model",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#training-the-model",
    "title": "Graph Neural Networks",
    "section": "Training the Model",
    "text": "Training the Model\n# Initialize network\nmodel = Net(A, X.size(0), 10, 2)\n\n# Loss and optimizer\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Training loop\nfor epoch in range(200):\n    optimizer.zero_grad()\n    loss = criterion(model(X), target)\n    loss.backward()\n    optimizer.step()"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#training-visualization",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#training-visualization",
    "title": "Graph Neural Networks",
    "section": "Training Visualization",
    "text": "Training Visualization\n\n\n\nNode embeddings learned during training\n\n\n\nModel successfully separates two groups\nClose to actual predictions (except node 9)\nSemi-supervised learning works!"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#pytorch-geometric",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#pytorch-geometric",
    "title": "Graph Neural Networks",
    "section": "PyTorch Geometric",
    "text": "PyTorch Geometric\n\n\nPyTorch Geometric (PyG):\n\nDedicated library for graph deep learning\nEasy, fast, and simple implementation\nBuilt for PyTorch users\nActive development and community\n\nFeatures:\n\nPre-built GCN layers\nVarious graph datasets\nEfficient sparse operations\n\n\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(num_features, 16)\n        self.conv2 = GCNConv(16, num_classes)\n        \n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#key-takeaways",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#key-takeaways",
    "title": "Graph Neural Networks",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nGCNs extend neural networks to non-Euclidean graph data\nAggregate and transform neighbourhood information\nAddress self-representation and degree scaling issues\nSemi-supervised learning on graphs is powerful\nApplications: social networks, molecules, knowledge graphs\n\nCritical Question: How powerful are GCNs really?\nRead: How powerful are Graph Convolutions?"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#resources",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#resources",
    "title": "Graph Neural Networks",
    "section": "Resources",
    "text": "Resources\n\ngraphnet Github repo\n\nKey Papers:\n\nSemi-Supervised Classification with GCNs - Kipf & Welling (2017)\nThomas Kipf’s Blog on GCNs\n\nLibraries:\n\nPyTorch Geometric\nDeep Graph Library (DGL)"
  },
  {
    "objectID": "sessions/W06_graph_neural_networks/graph_neural_networks.html#questions",
    "href": "sessions/W06_graph_neural_networks/graph_neural_networks.html#questions",
    "title": "Graph Neural Networks",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#history",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#history",
    "title": "Neural Networks",
    "section": "History",
    "text": "History\n\nNearly everything we talk about today existed ~1990\nWhat changed?\n\nMore data\nFaster computers (GPUs)\nSome improvements: relu, dropout, adam, batch-normalization, residual networks"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#logistic-regression-as-neural-net",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#logistic-regression-as-neural-net",
    "title": "Neural Networks",
    "section": "Logistic Regression as Neural Net",
    "text": "Logistic Regression as Neural Net"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#basic-architecture",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#basic-architecture",
    "title": "Neural Networks",
    "section": "Basic Architecture",
    "text": "Basic Architecture\n\n\n\n\\(h(x) = f(W_1x+b_1)\\)\n\\(o(x) = g(W_2h(x) + b_2)\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#more-layers",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#more-layers",
    "title": "Neural Networks",
    "section": "More Layers",
    "text": "More Layers\n\n\n\n\nHidden layers usually all have the same non-linear function\nMany layers → “deep learning”\nMultilayer perceptron, feed-forward neural network, vanilla feed-forward neural network\nRegression: single output neuron with linear activation\nClassification: one-hot-encoding of classes, n_classes output variables with softmax"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#nonlinear-activation-functions",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#nonlinear-activation-functions",
    "title": "Neural Networks",
    "section": "Nonlinear Activation Functions",
    "text": "Nonlinear Activation Functions\n\n\n\n\nStandard choices: tanh or rectified linear unit (relu)\nTanh squashes between -1 and 1; saturates towards infinities\nReLU is constant zero for negative numbers, then identity"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#supervised-neural-networks",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#supervised-neural-networks",
    "title": "Neural Networks",
    "section": "Supervised Neural Networks",
    "text": "Supervised Neural Networks\n\nNon-linear models for classification and regression\nWork well for very large datasets\nNon-convex optimization\nNotoriously slow to train – need for GPUs\nUse dot products; require preprocessing similar to SVM or linear models, unlike trees\nMany variants: Convolutional nets, GRUs, LSTMs, recursive networks, VAEs, GANs, deep RL"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#training-objective",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#training-objective",
    "title": "Neural Networks",
    "section": "Training Objective",
    "text": "Training Objective\n\\(h(x) = f(W_1x+b_1)\\)\n\\(o(x) = g(W_2h(x)+b_2) = g(W_2f(W_1x + b_1) + b_2)\\)\n\\(\\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,o(x_i))\\)\n\\(= \\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,g(W_2f(W_1x+b_1)+b_2))\\)\n\n\\(l\\) = Squared loss for regression; Cross-entropy loss for classification"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#backpropagation",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#backpropagation",
    "title": "Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nNeed \\(\\frac{\\partial l(y, o)}{\\partial W_i}\\) and \\(\\frac{\\partial l(y, o)}{\\partial b_i}\\)\n\n\\(\\text{net}(x) := W_1x + b_1\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#gradient-computation",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#gradient-computation",
    "title": "Neural Networks",
    "section": "Gradient Computation",
    "text": "Gradient Computation\n\nBackpropagation is clever application of chain rule for derivatives\nSingle backward pass from output to input computes derivatives\nNot an optimization algorithm, just a way to compute gradients"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#relu-differentiability",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#relu-differentiability",
    "title": "Neural Networks",
    "section": "ReLU Differentiability",
    "text": "ReLU Differentiability\n\n\n\n\nReLU not differentiable at zero\nUse subgradient descent; any gradient below function works\nIn practice, never hit zero with floating point numbers"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#optimizing-w-b",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#optimizing-w-b",
    "title": "Neural Networks",
    "section": "Optimizing W, b",
    "text": "Optimizing W, b\nBatch \\(W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=1}^N \\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)\nOnline/Stochastic \\(W_i \\leftarrow W_i - \\eta\\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)\nMinibatch \\(W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=k}^{k+m} \\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#learning-heuristics",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#learning-heuristics",
    "title": "Neural Networks",
    "section": "Learning Heuristics",
    "text": "Learning Heuristics\n\nConstant \\(\\eta\\) not good\nCan decrease \\(\\eta\\) over time\nBetter: adaptive \\(\\eta\\) for each entry of \\(W_i\\)\nState-of-the-art: adam (with magic numbers)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#picking-optimization-algorithms",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#picking-optimization-algorithms",
    "title": "Neural Networks",
    "section": "Picking Optimization Algorithms",
    "text": "Picking Optimization Algorithms\n\nSmall dataset: off the shelf like l-bfgs\nBig dataset: adam / rmsprop\nHave time & nerve: tune the schedule"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#neural-nets-with-sklearn",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#neural-nets-with-sklearn",
    "title": "Neural Networks",
    "section": "Neural Nets with sklearn",
    "text": "Neural Nets with sklearn\n\n\n\nmlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\nprint(mlp.score(X_train, y_train))\nprint(mlp.score(X_test, y_test))\n\nDon’t use sklearn for anything but toy problems in neural nets"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#random-state",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#random-state",
    "title": "Neural Networks",
    "section": "Random State",
    "text": "Random State\n\n\n\n\nNetwork is way over capacity and can overfit in many ways\nRegularization might make it less dependent on initialization"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#hidden-layer-size",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#hidden-layer-size",
    "title": "Neural Networks",
    "section": "Hidden Layer Size",
    "text": "Hidden Layer Size\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,), random_state=10)\nmlp.fit(X_train, y_train)\n\n\n\n\nSingle hidden layer with 5 units\nEach unit corresponds to different part of decision boundary"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#multiple-hidden-layers",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#multiple-hidden-layers",
    "title": "Neural Networks",
    "section": "Multiple Hidden Layers",
    "text": "Multiple Hidden Layers\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10), random_state=0)\nmlp.fit(X_train, y_train)\n\n\n\n\n3 hidden layers each of size 10\nMain way to control complexity"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#activation-functions",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#activation-functions",
    "title": "Neural Networks",
    "section": "Activation Functions",
    "text": "Activation Functions\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10),\n                    activation='tanh', random_state=0)\nmlp.fit(X_train, y_train)\n\n\n\n\nUsing tanh gives smoother boundaries\nReLU doesn’t work as well with l-bfgs on small networks\nFor large networks, relu is preferred"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#regression",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#regression",
    "title": "Neural Networks",
    "section": "Regression",
    "text": "Regression\n\n\n\nfrom sklearn.neural_network import MLPRegressor\nmlp_relu = MLPRegressor(solver=\"lbfgs\").fit(X, y)\nmlp_tanh = MLPRegressor(solver=\"lbfgs\", activation='tanh').fit(X, y)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#complexity-control",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#complexity-control",
    "title": "Neural Networks",
    "section": "Complexity Control",
    "text": "Complexity Control\n\nNumber of parameters\nRegularization\nEarly Stopping\nDropout"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#grid-searching-neural-nets",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#grid-searching-neural-nets",
    "title": "Neural Networks",
    "section": "Grid-Searching Neural Nets",
    "text": "Grid-Searching Neural Nets\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, stratify=data.target, random_state=0)\n\nfrom sklearn.model_selection import GridSearchCV\npipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\"lbfgs\", random_state=1))\nparam_grid = {'mlpclassifier__alpha': np.logspace(-3, 3, 7)}\ngrid = GridSearchCV(pipe, param_grid)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#searching-hidden-layer-sizes",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#searching-hidden-layer-sizes",
    "title": "Neural Networks",
    "section": "Searching Hidden Layer Sizes",
    "text": "Searching Hidden Layer Sizes\nfrom sklearn.model_selection import GridSearchCV\npipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\"lbfgs\", random_state=1))\nparam_grid = {'mlpclassifier__hidden_layer_sizes':\n              [(10,), (50,), (100,), (500,), (10, 10), (50, 50), (100, 100), (500, 500)]}\ngrid = GridSearchCV(pipe, param_grid)\ngrid.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#write-your-own-neural-networks",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#write-your-own-neural-networks",
    "title": "Neural Networks",
    "section": "Write Your Own Neural Networks",
    "text": "Write Your Own Neural Networks\nclass NeuralNetwork(object):\n    def __init__(self):\n        # initialize coefficients and biases\n        pass\n    def forward(self, x):\n        activation = x\n        for coef, bias in zip(self.coef_, self.bias_):\n            activation = self.nonlinearity(np.dot(activation, coef) + bias)\n        return activation\n    def backward(self, x):\n        # compute gradient of stuff in forward pass\n        pass"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#autodiff",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#autodiff",
    "title": "Neural Networks",
    "section": "Autodiff",
    "text": "Autodiff\nclass array(object) :\n    \"\"\"Simple Array object that support autodiff.\"\"\"\n    def __init__(self, value, name=None):\n        self.value = value\n        if name:\n            self.grad = lambda g : {name : g}\n    def __add__(self, other):\n        assert isinstance(other, int)\n        ret = array(self.value + other)\n        ret.grad = lambda g : self.grad(g)\n        return ret\n    def __mul__(self, other):\n        assert isinstance(other, array)\n        ret = array(self.value * other.value)\n        def grad(g):\n            x = self.grad(g * other.value)\n            x.update(other.grad(g * self.value))\n            return x\n        ret.grad = grad\n        return ret"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#autodiff-example",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#autodiff-example",
    "title": "Neural Networks",
    "section": "Autodiff Example",
    "text": "Autodiff Example\na = array(np.array([1, 2]), 'a')\nb = array(np.array([3, 4]), 'b')\nc = b * a\nd = c + 1\nprint(d.value)\nprint(d.grad(1))\n[4 9]\n{'b': array([1, 2]), 'a': array([3, 4])}\n\nAutomatic differentiation avoids writing gradients manually\nKeep track of computation while executing forward pass\nHard-code derivative for each operation (no symbolic differentiation)\nBuild computation graph automatically"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#gpu-support",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#gpu-support",
    "title": "Neural Networks",
    "section": "GPU Support",
    "text": "GPU Support\n\n\n\n\nImportant limitation: GPUs have much less memory than RAM\nMemory copies between RAM and GPU are expensive"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#computation-graph",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#computation-graph",
    "title": "Neural Networks",
    "section": "Computation Graph",
    "text": "Computation Graph\n\n\n\n\nStore different intermediate results depending on derivatives needed\nGiven limited GPU memory, important to know what to cache/discard\nHelps with visual debugging and understanding network structure"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#deep-learning-framework-requirements",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#deep-learning-framework-requirements",
    "title": "Neural Networks",
    "section": "Deep Learning Framework Requirements",
    "text": "Deep Learning Framework Requirements\n\nAutodiff\nGPU support\nOptimization and inspection of computation graph\nOn-the-fly generation of computation graph (optional)\nDistribution over multiple GPUs and/or cluster (optional)\n\nCurrent choices: TensorFlow, PyTorch / Torch, Chainer"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#deep-learning-libraries",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#deep-learning-libraries",
    "title": "Neural Networks",
    "section": "Deep Learning Libraries",
    "text": "Deep Learning Libraries\n\nKeras (TensorFlow, CNTK, Theano)\nPyTorch (torch)\nChainer (chainer)\nMXNet (MXNet)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#quick-look-at-tensorflow",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#quick-look-at-tensorflow",
    "title": "Neural Networks",
    "section": "Quick Look at TensorFlow",
    "text": "Quick Look at TensorFlow\n\n“Down to the metal” - don’t use for everyday tasks\nThree steps for learning:\n\nBuild computation graph (using array operations and functions)\nCreate Optimizer (gradient descent, adam, etc.) attached to graph\nRun actual computation\n\nEager mode (default in TensorFlow 2.0): write imperative code directly"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#pytorch-example",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#pytorch-example",
    "title": "Neural Networks",
    "section": "PyTorch Example",
    "text": "PyTorch Example\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n\nN = 100\nx = torch.randn(N, 1, device=device, dtype=dtype)\ny = torch.randn(N, 1, device=device, dtype=dtype)\nw = torch.randn(D_in, H, device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    y_pred = x.mm(w1)\n    loss = (y_pred - y).pow(2).sum().item()\n    loss.backward()\n    w1 -= learning_rate * w1.grad\n    w1.grad.zero_()"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#best-practices",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#best-practices",
    "title": "Neural Networks",
    "section": "Best Practices",
    "text": "Best Practices\n\nDon’t go down to the metal unless you have to!\nDon’t write TensorFlow, write Keras!\nDon’t write PyTorch, write pytorch.nn or FastAI (or Skorch or ignite)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_neural_networks.html#questions",
    "href": "sessions/W05_neural_networks/aml_neural_networks.html#questions",
    "title": "Neural Networks",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#last-week",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#last-week",
    "title": "Tree-Based Methods",
    "section": "Last week",
    "text": "Last week\n\nAnalysis workflow of supervised learning\nModel evaluation methods: train-test split, cross validation (and extensions)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#objectives-of-this-week",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#objectives-of-this-week",
    "title": "Tree-Based Methods",
    "section": "Objectives of this week",
    "text": "Objectives of this week\n\nUnderstand decision trees and tree-based ensemble methods (random forests, gradient boosting)\nCan apply tree-based methods to real-world problems"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#decision-trees-dt",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#decision-trees-dt",
    "title": "Tree-Based Methods",
    "section": "Decision Trees (DT)",
    "text": "Decision Trees (DT)\n\nMany types of decision trees: classification and regression trees (CART), C4.5, ID3, CHAID, etc.\nFocus on CART: binary trees for classification and regression\nBinary splits on features; leaves store predictions"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#why-dt",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#why-dt",
    "title": "Tree-Based Methods",
    "section": "Why DT?",
    "text": "Why DT?\n\nNon-linear, strong models for classification and regression\nMinimal preprocessing: scale and distributions matter little\nSmall trees can be explained; large trees power strong ensembles"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#decision-trees-for-classification",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#decision-trees-for-classification",
    "title": "Tree-Based Methods",
    "section": "Decision Trees for Classification",
    "text": "Decision Trees for Classification\n\nAsk a sequence of binary questions on features\nAxis-parallel splits; thresholds on single features\nLeaves store class distributions"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#building-trees",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#building-trees",
    "title": "Tree-Based Methods",
    "section": "Building Trees",
    "text": "Building Trees\n\n\n\nSearch all features and thresholds\nChoose split that most reduces impurity of data on a node\nRecurse on each child until stopping rule"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#criteria-classification",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#criteria-classification",
    "title": "Tree-Based Methods",
    "section": "Criteria (Classification)",
    "text": "Criteria (Classification)\n\nGini: \\(H_{\\text{gini}}(X_m) = \\sum_{k \\in \\mathcal{Y}} p_{mk}(1-p_{mk})\\)\nCross-Entropy: \\(H_{\\text{CE}}(X_m) = - \\sum_{k \\in \\mathcal{Y}} p_{mk}\\log p_{mk}\\)\nHere, \\(p_{mk}\\) is class \\(k\\) proportion in node \\(m\\)\nExample: a dataset with 10 samples in three classes (0, 1, 2) with proportions (0.2, 0.5, 0.3).\nGini = 0.2 × 0.8 + 0.5 × 0.5 + 0.3 × 0.7 = 0.66\nCross-Entropy = −(0.2 log 0.2 + 0.5 log 0.5 + 0.3 log 0.3) ≈ 1.0296"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#prediction",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#prediction",
    "title": "Tree-Based Methods",
    "section": "Prediction",
    "text": "Prediction\n\n\n\n\nGiven a new sample, start from the top\nTraverse splits; follow feature tests\nPredict majority class in the reached leaf"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#regression-trees",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#regression-trees",
    "title": "Tree-Based Methods",
    "section": "Regression Trees",
    "text": "Regression Trees\n\nPredict mean in leaf: \\(\\bar{y}_m = \\frac{1}{N_m}\\sum_{i \\in N_m} y_i\\)\nTwo impurity metrics\n\nMSE: \\(H(X_m) = \\frac{1}{N_m}\\sum_{i \\in N_m}(y_i-\\bar{y}_m)^2\\)\nMAE: \\(\\frac{1}{N_m}\\sum_{i \\in N_m}|y_i-\\bar{y}_m|\\)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#visualizing-trees-sklearn",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#visualizing-trees-sklearn",
    "title": "Tree-Based Methods",
    "section": "Visualizing Trees (sklearn)",
    "text": "Visualizing Trees (sklearn)\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=0)\n\ntree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#visualizing-trees-plot_tree",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#visualizing-trees-plot_tree",
    "title": "Tree-Based Methods",
    "section": "Visualizing Trees (plot_tree)",
    "text": "Visualizing Trees (plot_tree)\nfrom sklearn.tree import plot_tree\ntree_dot = plot_tree(tree, feature_names=cancer.feature_names)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#parameter-tuning",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#parameter-tuning",
    "title": "Tree-Based Methods",
    "section": "Parameter Tuning",
    "text": "Parameter Tuning\n\nPre-pruning (limit growth)\n\nmax_depth\nmax_leaf_nodes\nmin_samples_split\nmin_impurity_decrease\n\nPost-pruning (cost-complexity) after full growth"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#no-pruning",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#no-pruning",
    "title": "Tree-Based Methods",
    "section": "No Pruning",
    "text": "No Pruning"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#max_depth-4",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#max_depth-4",
    "title": "Tree-Based Methods",
    "section": "max_depth = 4",
    "text": "max_depth = 4"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#max_leaf_nodes-8",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#max_leaf_nodes-8",
    "title": "Tree-Based Methods",
    "section": "max_leaf_nodes = 8",
    "text": "max_leaf_nodes = 8"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#min_samples_split-50",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#min_samples_split-50",
    "title": "Tree-Based Methods",
    "section": "min_samples_split = 50",
    "text": "min_samples_split = 50"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#grid-search-max_depth",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#grid-search-max_depth",
    "title": "Tree-Based Methods",
    "section": "Grid Search: max_depth",
    "text": "Grid Search: max_depth\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth': range(1, 7)}\ngrid = GridSearchCV(DecisionTreeClassifier(random_state=0),\n                    param_grid=param_grid, cv=10)\ngrid.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#grid-search-max_leaf_nodes",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#grid-search-max_leaf_nodes",
    "title": "Tree-Based Methods",
    "section": "Grid Search: max_leaf_nodes",
    "text": "Grid Search: max_leaf_nodes\nparam_grid = {'max_leaf_nodes': range(2, 20)}"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#cost-complexity-pruning",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#cost-complexity-pruning",
    "title": "Tree-Based Methods",
    "section": "Cost Complexity Pruning",
    "text": "Cost Complexity Pruning\n\nObjective: \\(R_\\alpha(T) = R(T) + \\alpha |T|\\)\n\\(R(T)\\) = total leaf impurity; \\(|T|\\) = number of leaves; tune \\(\\alpha\\)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#efficient-pruning-path",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#efficient-pruning-path",
    "title": "Tree-Based Methods",
    "section": "Efficient Pruning Path",
    "text": "Efficient Pruning Path\nclf = DecisionTreeClassifier(random_state=0)\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#post--vs-pre-pruning",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#post--vs-pre-pruning",
    "title": "Tree-Based Methods",
    "section": "Post- vs Pre-Pruning",
    "text": "Post- vs Pre-Pruning\n\n\n\nCost-complexity pruning result\n\n\n\n\n\n\nmax_leaf_nodes search result"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#feature-importance",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#feature-importance",
    "title": "Tree-Based Methods",
    "section": "Feature Importance",
    "text": "Feature Importance\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, stratify=iris.target, random_state=0)\ntree = DecisionTreeClassifier(max_leaf_nodes=6).fit(X_train, y_train)\ntree.feature_importances_\n\n\n\n\nSum of impurity decreases per feature; magnitude only (no sign)\nUnstable with correlated features or different splits"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#categorical-data",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#categorical-data",
    "title": "Tree-Based Methods",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nTrees can split categories into subsets; many possible splits\nExact search costly; efficient for binary classification + Gini\nIn sklearn, one-hot encoding is needed today"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#predicting-probabilities",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#predicting-probabilities",
    "title": "Tree-Based Methods",
    "section": "Predicting Probabilities",
    "text": "Predicting Probabilities\n\nLeaf probability = class fraction in leaf\nDeep, unpruned trees give overconfident (100%) probabilities\nPruning helps but calibration may still be poor"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#flexible-split-functions",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#flexible-split-functions",
    "title": "Tree-Based Methods",
    "section": "Flexible Split Functions",
    "text": "Flexible Split Functions\n\n\n\nSource: Shotton et al., Real-Time Human Pose Recognition (Kinect v1)\n\n\n\nCan compare pixels, regions, or other engineered tests\nLinear models inside nodes possible when extrapolation needed"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#limitations-of-trees",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#limitations-of-trees",
    "title": "Tree-Based Methods",
    "section": "Limitations of Trees",
    "text": "Limitations of Trees\n\nCannot extrapolate beyond training data range\nInstability/overfitting: small data changes can alter splits. (Emsembles help)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-limits",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-limits",
    "title": "Tree-Based Methods",
    "section": "Extrapolation Limits",
    "text": "Extrapolation Limits\n\n\n\n\nTrees behave like nearest neighbors; cannot extrapolate beyond observed range"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-train-on-data-before-2000",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-train-on-data-before-2000",
    "title": "Tree-Based Methods",
    "section": "Extrapolation: train on data before 2000",
    "text": "Extrapolation: train on data before 2000"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-test-on-data-after-2000",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#extrapolation-test-on-data-after-2000",
    "title": "Tree-Based Methods",
    "section": "Extrapolation: test on data after 2000",
    "text": "Extrapolation: test on data after 2000\n\nTree predictions flatten outside training support; linear models can extrapolate"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#instability",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#instability",
    "title": "Tree-Based Methods",
    "section": "Instability",
    "text": "Instability\n\n\n\n\n\n\n\n\n\n\n\nSmall data changes can alter splits"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#poor-mans-ensemble",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#poor-mans-ensemble",
    "title": "Tree-Based Methods",
    "section": "Poor man’s ensemble",
    "text": "Poor man’s ensemble\n\nCombine multiple models to reduce variance / improve accuracy\nTrain several models with different seeds; average predictions\nOwen Zhang (long time kaggle 1st): build XGBoosting models with different random seeds.\nWorks across model families (e.g., tree + linear + RF + NN)\nKey to success: diversity among models\nsklearn: VotingClassifier\n\nsoft: average the probabilities of all models and take the arg max (need models provide calibrated probabilities)\nhard: let each model make a prediction and take majority vote"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#votingclassifier-example",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#votingclassifier-example",
    "title": "Tree-Based Methods",
    "section": "VotingClassifier Example",
    "text": "VotingClassifier Example\nfrom sklearn.ensemble import VotingClassifier\nvoting = VotingClassifier(\n    [('logreg', LogisticRegression(C=100)),\n     ('tree', DecisionTreeClassifier(max_depth=3, random_state=0))],\n    voting='soft')\nvoting.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#tree-ensembles-two-types",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#tree-ensembles-two-types",
    "title": "Tree-Based Methods",
    "section": "Tree ensembles: two types",
    "text": "Tree ensembles: two types\n\nBagging (Bootstrap Aggregation): random forests\nBoosting: gradient boosting machines (GBM), XGBoost, LightGBM, CatBoost, etc."
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#bagging-bootstrap-aggregation",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#bagging-bootstrap-aggregation",
    "title": "Tree-Based Methods",
    "section": "Bagging (Bootstrap Aggregation)",
    "text": "Bagging (Bootstrap Aggregation)\n\n\n\nSample with replacement (same size as dataset)\nTrain a model on each bootstrap sample\nAverage predictions to cut variance"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#bias-and-variance",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#bias-and-variance",
    "title": "Tree-Based Methods",
    "section": "Bias and Variance",
    "text": "Bias and Variance\n\n\n\n\nAim for low bias + low variance; averaging high-variance models can lower variance"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#ensembles-bias-vs-variance",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#ensembles-bias-vs-variance",
    "title": "Tree-Based Methods",
    "section": "Ensembles: Bias vs Variance",
    "text": "Ensembles: Bias vs Variance\n\nGeneralization improves with strong base learners and low correlation\nDiversifying models (or data/features) helps more than sheer count"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#random-forests",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#random-forests",
    "title": "Tree-Based Methods",
    "section": "Random Forests",
    "text": "Random Forests\n\n\n\n\nBagging + feature subsampling at each split"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#randomise-in-two-ways",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#randomise-in-two-ways",
    "title": "Tree-Based Methods",
    "section": "Randomise in Two Ways",
    "text": "Randomise in Two Ways\n\n\n\nFor each tree: bootstrap sample of rows\nFor each split: sample features without replacement\nMore trees → lower variance (diminishing returns)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#tuning-random-forests",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#tuning-random-forests",
    "title": "Tree-Based Methods",
    "section": "Tuning Random Forests",
    "text": "Tuning Random Forests\n\nmax_features: ~\\(\\sqrt{p}\\) for classification, ~\\(p\\) for regression\nn_estimators: use 100+; more reduces variance\nPre-pruning (max_depth, max_leaf_nodes, min_samples_split) can cut size/time"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#extremely-randomized-trees",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#extremely-randomized-trees",
    "title": "Tree-Based Methods",
    "section": "Extremely Randomized Trees",
    "text": "Extremely Randomized Trees\n\nAdd randomness: draw split thresholds at random per feature\nOften no bootstrap; faster (no threshold search)\nCan yield smoother boundaries; less common than standard RF"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#warm-starts",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#warm-starts",
    "title": "Tree-Based Methods",
    "section": "Warm-Starts",
    "text": "Warm-Starts\nrf = RandomForestClassifier(warm_start=True)\nfor n in range(1, 100, 5):\n    rf.n_estimators = n\n    rf.fit(X_train, y_train)\n\n\n\n\nIncrease trees incrementally; stop when scores stabilize"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#out-of-bag-estimates",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#out-of-bag-estimates",
    "title": "Tree-Based Methods",
    "section": "Out-of-Bag Estimates",
    "text": "Out-of-Bag Estimates\n\nEach tree trains on ~66% of data; predict remaining ~34%\nAverage OOB predictions as a free validation score\n\nrf = RandomForestClassifier(max_features=m, oob_score=True,\n                            n_estimators=200, random_state=0)\nrf.fit(X_train, y_train)\noob_scores.append(rf.oob_score_)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#variable-importance-rf",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#variable-importance-rf",
    "title": "Tree-Based Methods",
    "section": "Variable Importance (RF)",
    "text": "Variable Importance (RF)\nrf = RandomForestClassifier().fit(X_train, y_train)\nrf.feature_importances_\n\n\n\n\nMore stable than single-tree importances; still magnitude-only"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#trees-takeaways",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#trees-takeaways",
    "title": "Tree-Based Methods",
    "section": "Trees: Takeaways",
    "text": "Trees: Takeaways\n\nNon-linear without heavy preprocessing\nSingle small trees interpretable; forests robust baselines\nBeware extrapolation limits and instability"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-descent",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-descent",
    "title": "Tree-Based Methods",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\n\n\nOptimise \\(\\arg\\min_w F(w)\\) by stepping along \\(-\\nabla F(w)\\)\nUpdate: \\(w_{i+1} = w_i - \\eta_i \\nabla F(w_i)\\)\nConverges to a local minimum (global for convex losses)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#stochastic-gradient-descent",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#stochastic-gradient-descent",
    "title": "Tree-Based Methods",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nLogistic regression objective: log-loss + regularizer\nSGD uses one (or a mini-batch of) example(s) per step to approximate the gradient\nFaster on large data; noisier updates\n\nfrom sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier().fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-idea",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-idea",
    "title": "Tree-Based Methods",
    "section": "Gradient Boosting Idea",
    "text": "Gradient Boosting Idea\n\nTrain a small tree to predict \\(y\\)\nTrain next tree on residuals of previous model (or on points poorly predicted)\nRepeat; sum scaled predictions: \\(f(x)=\\sum \\gamma g_k(x)\\) with learning rate \\(\\gamma\\)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-algorithm",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-algorithm",
    "title": "Tree-Based Methods",
    "section": "Gradient Boosting Algorithm",
    "text": "Gradient Boosting Algorithm\n$ f_{1}(x) y $ $ f_{2}(x) y - f_{1}(x) $ $ f_{3}(x) y - f_{1}(x) - f_{2}(x) $\n\nEach new tree fixes remaining error\nSmall \\(\\gamma\\) (e.g., 0.1) for smoother learning"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-as-gradient-descent",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#gradient-boosting-as-gradient-descent",
    "title": "Tree-Based Methods",
    "section": "Gradient Boosting as Gradient Descent",
    "text": "Gradient Boosting as Gradient Descent\n\n\n\nLinear regression minimizes \\(\\sum (y - w^T x - b)^2\\)\nGradient descent updates weights\n\n\n\nGradient boosting minimises same loss over predictions \\(\\hat{y}\\)\nUpdate \\(\\hat{y}\\) by adding trees along negative gradient"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#regression-example",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#regression-example",
    "title": "Tree-Based Methods",
    "section": "Regression Example",
    "text": "Regression Example\n\n\n\n\nShallow trees fit residuals sequentially until residuals shrink"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#classification-example",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#classification-example",
    "title": "Tree-Based Methods",
    "section": "Classification Example",
    "text": "Classification Example\n\n\n\n\nProbability surfaces become sharper as trees accumulate\nMulticlass: one regression tree per class per step"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#early-stopping",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#early-stopping",
    "title": "Tree-Based Methods",
    "section": "Early Stopping",
    "text": "Early Stopping\n\nMore trees can overfit\nStop when validation metric stops improving\nEither fix \\(n_{\\text{estimators}}\\) and tune \\(\\gamma\\), or fix \\(\\gamma\\) and stop early"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#tuning-gradient-boosting",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#tuning-gradient-boosting",
    "title": "Tree-Based Methods",
    "section": "Tuning Gradient Boosting",
    "text": "Tuning Gradient Boosting\n\nUse shallow trees (strong pruning via max_depth)\nTune learning rate, n_estimators, subsampling of rows/columns\nOptional regularisation (min_samples_split, min_impurity_decrease)"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#ensuring-uncorrelated-trees-aggressive-subsampling",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#ensuring-uncorrelated-trees-aggressive-subsampling",
    "title": "Tree-Based Methods",
    "section": "Ensuring uncorrelated trees: Aggressive Subsampling",
    "text": "Ensuring uncorrelated trees: Aggressive Subsampling\n\nRow and column subsampling reduce correlation and overfitting\nCommon in XGBoost/LightGBM"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#xgboost",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#xgboost",
    "title": "Tree-Based Methods",
    "section": "XGBoost",
    "text": "XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier().fit(X_train, y_train)\n\nFast, supports missing values, GPU/cluster training\nL1/L2 on leaves; fast approximate splits; sparse data friendly"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#lightgbm",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#lightgbm",
    "title": "Tree-Based Methods",
    "section": "LightGBM",
    "text": "LightGBM\nfrom lightgbm.sklearn import LGBMClassifier\nlgbm = LGBMClassifier().fit(X_train, y_train)\n\nNative categorical handling; missing values; GPU/cluster support"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#catboost",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#catboost",
    "title": "Tree-Based Methods",
    "section": "CatBoost",
    "text": "CatBoost\nfrom catboost.sklearn import CatBoostClassifier\ncatb = CatBoostClassifier().fit(X_train, y_train)\n\nStrong on categorical data; symmetric trees; target-encoding tricks; GPU"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#advantages-of-gradient-boosting",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#advantages-of-gradient-boosting",
    "title": "Tree-Based Methods",
    "section": "Advantages of Gradient Boosting",
    "text": "Advantages of Gradient Boosting\n\nOften more accurate than random forests with tuning\nSmall models; fast prediction\nHist/XGB/LightGBM implementations are very fast"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#when-to-use-tree-ensembles",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#when-to-use-tree-ensembles",
    "title": "Tree-Based Methods",
    "section": "When to Use Tree Ensembles",
    "text": "When to Use Tree Ensembles\n\nFor tabular data\nNeed non-linear relationships and minimal preprocessing\nSingle small tree for interpretability;\nRandom forests very robust, good benchmark\nGradient boosting for best accuracy when tuned"
  },
  {
    "objectID": "sessions/W04_tree_based_methods/tree_based_methods.html#questions",
    "href": "sessions/W04_tree_based_methods/tree_based_methods.html#questions",
    "title": "Tree-Based Methods",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#framework",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#framework",
    "title": "Supervised learning",
    "section": "Framework",
    "text": "Framework\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in\n\\begin{cases}\n\\mathbb{R}, & \\text{(regression)} \\\\\n\\mathcal{Y} \\text{ (finite set)}, & \\text{(classification)}\n\\end{cases} \\\\\n\\text{learn } f(x_i) &\\approx y_i \\\\\n\\text{such that } f(x) &\\approx y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#regression-vs.-classification",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#regression-vs.-classification",
    "title": "Supervised learning",
    "section": "Regression vs. classification",
    "text": "Regression vs. classification\n\n\n\n\n\n\n\n\n\nRegression\nClassification\n\n\n\n\nTarget variable\nContinuous value (y )\nDiscrete label (y ) (finite set)\n\n\nTask\nPredict “how much” / “how many”\nPredict “which class” / “which category”\n\n\nIntuition\nFind a ‘line’ close to all points\nFind a ‘boundary’ between classis\n\n\nExample\nPredicting house prices from features\nPredicting spam vs. not spam for an email"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#example---linear-regression",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#example---linear-regression",
    "title": "Supervised learning",
    "section": "Example - linear regression",
    "text": "Example - linear regression\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndata = fetch_california_housing()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.2f}\")\nprint(f\"Intercept: {model.intercept_:.2f}\")\n\nMedInc: 0.45\nHouseAge: 0.01\nAveRooms: -0.12\nAveBedrms: 0.78\nPopulation: -0.00\nAveOccup: -0.00\nLatitude: -0.42\nLongitude: -0.43\nIntercept: -37.02"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#terms",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#terms",
    "title": "Supervised learning",
    "section": "Terms",
    "text": "Terms\n\n\n\n\n\nTerm\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\nAlgorithm\n\n\nprocedure that runs on data to create a model\n\n\nlinear regression\n\n\n\n\nModel\n\n\nan output by algorithm and data\n\n\n\\(\\hat{y}_i = \\sum_k \\beta_k x_{ik} + \\beta_0\\)\n\n\n\n\nMetric\n\n\nto evalute the model performace\n\n\nSquared error\n\n\n\n\nHyperparameter\n\n\nalgorithm settings, predefined by user instead of learned from data\n\n\nNone\n\n\n\n\nParameter\n\n\nmodel components learned from data\n\n\ncoefficients & intercept\n\n\n\n\nModel training\n\n\nprocess of estimating parameters from data\n\n\nusing maximum likelihood estimation"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#classification",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#classification",
    "title": "Supervised learning",
    "section": "Classification",
    "text": "Classification\n\nMost classification algorithms follow two steps:\n\n\npredict probabilities for each class\nassign class with highest probability\n\n\nWe can talk about common challenges for supervised learning (despite their differences)"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#common-challenges",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#common-challenges",
    "title": "Supervised learning",
    "section": "Common Challenges",
    "text": "Common Challenges\n\nTo select evaluation metrics (so that the model solves the right problem)\nTo design workflow (so that the model generalises well and avoids overfitting)"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression",
    "title": "Supervised learning",
    "section": "Metrics for regression",
    "text": "Metrics for regression\n\n\n\n\n\n\n\nformula\n\n\nunit\n\n\nnotes\n\n\n\n\n\n\nRMSE\n\n\n\\(\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\)\n\n\nSame unit as target \\(y\\)\n\n\nPenalises large errors more; sensitive to outliers\n\n\n\n\nR²\n\n\n\\(1 - \\dfrac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\\)\n\n\nDimensionless (between 0 and 1 for most cases)\n\n\nMeasures proportion of variance explained by the model; can be negative if model is worse than \\(y_i=\\bar{y}\\)\n\n\n\n\nMAE\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^{n}\\lvert y_i - \\hat{y}_i\\rvert\\)\n\n\nSame unit as target \\(y\\)\n\n\nMore robust to outliers than RMSE; interpretable as average absolute error\n\n\n\n\nMAPE\n\n\n\\(\\frac{100}{n}\\sum_{i=1}^{n}\\left\\lvert \\frac{y_i - \\hat{y}_i}{y_i} \\right\\rvert\\)\n\n\nPercent (%)\n\n\nsensitive to very small \\(y_i\\); relative error"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression-1",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-regression-1",
    "title": "Supervised learning",
    "section": "Metrics for regression",
    "text": "Metrics for regression\n\nUsing RMSE or squared error in most cases\nBy default, regressors in sklearn and XGBoost use Squared Error as loss function\nDifference between R2 for OLS (within [0,1]) vs. R2 for regression (&lt;=1, can be negative)"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-classification-binary",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#metrics-for-classification-binary",
    "title": "Supervised learning",
    "section": "Metrics for classification (binary)",
    "text": "Metrics for classification (binary)\n\n\n\nImage Credit: COMS4995-s20\n\n\n\\(Accuracy=\\frac{TP+TN}{TP+TN+FP+FN}\\)\n\nSelecting Positive label: often the minority class"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#example-using-breast-cancer-dataset",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#example-using-breast-cancer-dataset",
    "title": "Supervised learning",
    "section": "Example using Breast Cancer dataset",
    "text": "Example using Breast Cancer dataset\n\n\nclass malignant: 212, 37.258%\nclass benign: 357, 62.742%\nPredictive accuracy: 0.930"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#limitation-of-accuracy-accuracy-paradox",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#limitation-of-accuracy-accuracy-paradox",
    "title": "Supervised learning",
    "section": "Limitation of accuracy (Accuracy paradox)",
    "text": "Limitation of accuracy (Accuracy paradox)\n\nScenario: Data with 90% negatives (imbalanced data)\nA majority strategy that predicts all as negative gets 90% accuracy, but this is useless.\nDifferent models can have the same accuracy (0.9) but make very different types of errors.\n\ny_pred_1: Predicts all negative (90 TN, 0 TP)\ny_pred_2: Predicts some positives correctly but misses others\ny_pred_3: A mix of errors"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#precision-recall-f1-score-auc",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#precision-recall-f1-score-auc",
    "title": "Supervised learning",
    "section": "Precision, Recall, F1-score, AUC",
    "text": "Precision, Recall, F1-score, AUC\n\nPrecision (Positive Predicted Value): \\(\\frac{TP}{TP+FP}\\). Among predicted positives, how many are actually positive.\nRecall (Sensitivity, True Positive Rate): \\(\\frac{TP}{TP+FN}\\). Among actual positives, how many are correctly predicted.\nF1-score (Harmonic mean of precision & recall): \\(F=2\\frac{\\text{precision}\\cdot \\text{recall}}{\\text{precision}+\\text{recall}}\\)"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#addressing-accuracy-paradox",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#addressing-accuracy-paradox",
    "title": "Supervised learning",
    "section": "Addressing accuracy paradox",
    "text": "Addressing accuracy paradox"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#trade-off-between-precision-and-recall",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#trade-off-between-precision-and-recall",
    "title": "Supervised learning",
    "section": "Trade-off between precision and recall",
    "text": "Trade-off between precision and recall\n\nWhen precision increases, recall decreases. Vice versa.\nBy changing classification threshold (default=0.5, from 0 to 1), we can see the balance between precision and recall.\n\n\n\n\n\n\nPrecision–Recall curve for LR and Random Forest"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#roc-curve",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#roc-curve",
    "title": "Supervised learning",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nReceiver Operating Characteristic (ROC) curve plots True Positive Rate vs. False Positive Rate. Similar to precision-recall curve.\nThe identity line y=x represents random classifier (e.g. tossing a coin)."
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#auc-area-under-roc-curve",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#auc-area-under-roc-curve",
    "title": "Supervised learning",
    "section": "AUC (Area under ROC Curve)",
    "text": "AUC (Area under ROC Curve)\n\nThe integral of the ROC curve (or the area size under the curve)\nAUC is always 0.5 for random predictions\nMaximum AUC is 1.0 for perfect predictions"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#aggregating-metrics-across-classes",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#aggregating-metrics-across-classes",
    "title": "Supervised learning",
    "section": "Aggregating metrics across classes",
    "text": "Aggregating metrics across classes\n\nMacro average: \\(\\frac{1}{|L|}\\sum_{l\\in L}R(y_{l},\\hat{y}_{l})\\)\nUnweighted mean of per-class scores. Each class contributes equally, regardless of sample size.\nMicro average: \\(\\frac{1}{n}\\sum_{i=1}^{n}R(y_{i},\\hat{y}_{i})\\)\nSum individual TP, FP, FN, TN across all classes, then compute metric. Equivalent to accuracy for multiclass.\nWeighted average: \\(\\frac{1}{n}\\sum_{l\\in L}n_{l}R(y_{l},\\hat{y}_{l})\\)\nWeighted by support (number of samples in each class). Balances class sizes in the final score."
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#classification-report",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#classification-report",
    "title": "Supervised learning",
    "section": "Classification report",
    "text": "Classification report\n\nprint(classification_report(y_test, rf.predict(X_test), target_names=data.target_names))\n\n              precision    recall  f1-score   support\n\n   malignant       0.92      0.92      0.92        53\n      benign       0.96      0.96      0.96        90\n\n    accuracy                           0.94       143\n   macro avg       0.94      0.94      0.94       143\nweighted avg       0.94      0.94      0.94       143"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#picking-a-metric",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#picking-a-metric",
    "title": "Supervised learning",
    "section": "Picking a metric",
    "text": "Picking a metric\n\nReal-world problems are rarely balanced.\nAccuracy is rarely what you want.\nFind the right criterion for the specific task.\nDecide between emphasis on recall or precision.\nIdentify which classes are important."
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#metric-for-breast-cancer-detection",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#metric-for-breast-cancer-detection",
    "title": "Supervised learning",
    "section": "Metric for breast cancer detection",
    "text": "Metric for breast cancer detection\n\n“1” indicates malignant/cancer, “0” indicates benign/no cancer.\nMissing a cancer (FN) is much worse than a false alarm (FP)\nSo, we care more about recall than precision or accuracy.\nA model with high recall is preferred, even if it has lower precision."
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#next-question-how-can-i-optimise-for-a-specific-metric",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#next-question-how-can-i-optimise-for-a-specific-metric",
    "title": "Supervised learning",
    "section": "Next question: how can I optimise for a specific metric?",
    "text": "Next question: how can I optimise for a specific metric?\n\nRandomForestClassifier (and other classifiers) in sklearn by default optimises for accuracy and have no direct way to optimise for recall.\nWe have workarounds (topics for next week)\n\nUse recall metric during hyperparameter tuning and cross-validation\nAdjust classification threshold after training\nUse class weights to penalise misclassifications of the positive class more heavily during training"
  },
  {
    "objectID": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#generalisation-to-multi-class",
    "href": "sessions/W02_supervised_learning_metrics/supervised_learning_metrics.html#generalisation-to-multi-class",
    "title": "Supervised learning",
    "section": "Generalisation to multi-class",
    "text": "Generalisation to multi-class\n\nMost metrics can be generalised to multi-class using macro, micro, or weighted averaging.\nROC curve and AUC can be computed using one-vs-rest approach for each class and then averaged."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#what-we-learnt-in-term-1",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#what-we-learnt-in-term-1",
    "title": "Introduction to machine learning",
    "section": "What we learnt in Term 1",
    "text": "What we learnt in Term 1\n\nPython programming\nData types\nVisualisation\nRegression (Ordinary Least Square; Linear Mixed Effects; multicollinearity)\nDimensionality reduction\nClustering"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#learning-objectives",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#learning-objectives",
    "title": "Introduction to machine learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand the basics and classifications of machine learning\nUnderstand the differences between statistical methods and machine learning (estimation vs. prediction)\nAppreciate GIGO theorems in machine learning and data science"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#ml-as-subset-of-ai",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#ml-as-subset-of-ai",
    "title": "Introduction to machine learning",
    "section": "ML as subset of AI",
    "text": "ML as subset of AI\n\n\n\nMachine learning (decision tree, random forest, k-means, etc.)\n\nDeep learning (deep neural networks)\n\nOther AI tools: graphical models, symbolic AI\n\nNote: we don’t distinguish ML/DL and consider NN as part of ML\n\n\n\n\n\nImage Credit: Lecture slide (ML is a subset of AI)"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#definition-of-machine-learning",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#definition-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Definition of machine learning",
    "text": "Definition of machine learning\n\nArthur Samuel (1959): (Machine learning is the) field of study that gives computers the ability to learn without being explicitly programmed.\n\n\nTom Mitchell (1997): A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#what-is-machine-learning",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#what-is-machine-learning",
    "title": "Introduction to machine learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nExtracting knowledge from data\nRelying on data and algorithms\nClosely related to statistics but distinct from linear models\nFocus on prediction rather than estimating relationships or interpretation"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-facebook",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-facebook",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook",
    "text": "Examples: Facebook\n\n\n\n\nMachine learning in news feed ranking\nContent selection and targeting\nAds and recommendations"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nFace detection and recognition\nPhoto organization"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.-1",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.-1",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nPhoto selection and layout"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-amazon",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-amazon",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon",
    "text": "Examples: Amazon\n\n\n\n\nProduct ranking\nPersonalised recommendations\nAds selection"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-amazon-cont.",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-amazon-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon (cont.)",
    "text": "Examples: Amazon (cont.)\n\n\n\nSeller selection\nDefault choices\nRelated products"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#science-applications",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#science-applications",
    "title": "Introduction to machine learning",
    "section": "Science Applications",
    "text": "Science Applications\n\n\n\nPersonalised cancer treatment\nMedical diagnosis\nDrug discovery\nHiggs boson discovery\nExoplanet detection"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#types-of-machine-learning",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#types-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nSupervised Learning: Learn from input-output pairs (with labelled data)\nUnsupervised Learning: Discover structure in data (without labelled data)\nReinforcement Learning: Learn through interaction with environment (with an actual/simulated environment)"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#supervised-learning",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in \\mathbb{R} \\\\\nf(x_i) &\\approx y_i\n\\end{aligned}\n\\]\nLearn a function \\(f\\) from input-output pairs to predict on new data."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#generalisation-to-unseen-data",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#generalisation-to-unseen-data",
    "title": "Introduction to machine learning",
    "section": "Generalisation to unseen data",
    "text": "Generalisation to unseen data\n\nGoal: \\(f(x_i) \\approx y_i\\) on training data\nMore important: \\(f(x) \\approx y\\) on new data\nCore distinction: not just function approximation, but prediction on unseen data\nAvoiding overfitting to training data is key"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-of-supervised-learning",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#examples-of-supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Examples of Supervised Learning",
    "text": "Examples of Supervised Learning\n\nSpam detection\nMedical diagnosis\nAd click prediction"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#unsupervised-learning",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\\[\nx_i \\sim p(x) \\text{ i.i.d.}\n\\]\nLearn about the distribution \\(p\\):\n\nClustering\nDimensionality reduction\nTopic modeling\nOutlier detection"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#other-types-of-learning",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#other-types-of-learning",
    "title": "Introduction to machine learning",
    "section": "Other Types of Learning",
    "text": "Other Types of Learning\n\nSemi-supervised\nActive Learning\nForecasting\nTransfer learning\nIf you understand the basics of supervised, unsupervised, and reinforcement learning, others are easier to grasp"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name-classification-of-neighbourhoods",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name-classification-of-neighbourhoods",
    "title": "Introduction to machine learning",
    "section": "Don’t read a book by its name: classification of neighbourhoods",
    "text": "Don’t read a book by its name: classification of neighbourhoods\n\nClassifications of neighbourhoods (e.g. London output area classification) that are actually clustering ::: {style=“text-align:center;”}  :::"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name-anomaly-detection",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name-anomaly-detection",
    "title": "Introduction to machine learning",
    "section": "Don’t read a book by its name: anomaly detection",
    "text": "Don’t read a book by its name: anomaly detection\n\nAnomaly detection methods can be unsupervised, semi-supervised, or supervised learning\n\n\n\n\nImage Credit: https://pub.towardsai.net/anomaly-detection-a-comprehensive-guide-9d4d7e320242"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name-forecasting",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name-forecasting",
    "title": "Introduction to machine learning",
    "section": "Don’t read a book by its name: forecasting",
    "text": "Don’t read a book by its name: forecasting\n\nForecasting can be done with lagged features (via supervised learning), or with time series data (via time series analysis), or both\n\n\n\n\nImage Credit: gemini.com"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#what-does-llm-belong-to",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#what-does-llm-belong-to",
    "title": "Introduction to machine learning",
    "section": "What does LLM belong to?",
    "text": "What does LLM belong to?\n\n\n\nImage Credit: medium.com"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#classification-vs.-regression",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#classification-vs.-regression",
    "title": "Introduction to machine learning",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression\n\n\nClassification\n\nTarget \\(y\\) is discrete\nExample: Is this patient sick?\n\n\nRegression\n\nTarget \\(y\\) is continuous\nExample: How long to recover?"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#relationship-to-statistics",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#relationship-to-statistics",
    "title": "Introduction to machine learning",
    "section": "Relationship to Statistics",
    "text": "Relationship to Statistics\n\n\nStatistics\n\nModel first\nEstimation emphasis\nYes/no questions\nWith many assumptions, need to test\nInterpretation is key\ne.g. Does smoking lead to lung cancer?\n\n\nMachine Learning\n\nData first\nPrediction emphasis\nFuture predictions\nFew assumptions (but not assumption-free)\nInterpretation isn’t primary\ne.g. Predict tomorrow’s weather"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#be-careful-with-the-following-statements--",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#be-careful-with-the-following-statements--",
    "title": "Introduction to machine learning",
    "section": "Be careful with the following statements -",
    "text": "Be careful with the following statements -\n\n“Linear regression is not needed anymore because of machine learning.”\n“Machine learning is just a fad; statistics is more important.”\n“Machine learning models are black boxes; we can’t interpret them at all.”\n“Machine learning models are as interpretable as linear models.”\n“Machine learning models have no assumptions.”"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#guiding-principles-goal-considerations",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#guiding-principles-goal-considerations",
    "title": "Introduction to machine learning",
    "section": "Guiding Principles: Goal Considerations",
    "text": "Guiding Principles: Goal Considerations\n\nDefine the goal clearly\nDefine how to measure success\nThink about context and baseline\nAsk: what’s the benefit?"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#thinking-in-context",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#thinking-in-context",
    "title": "Introduction to machine learning",
    "section": "Thinking in Context",
    "text": "Thinking in Context\n\nWhat do you want to achieve?\nWhat’s the baseline and its performance?\nWhat improvement over baseline do you need?"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#good-and-bad-substitutes",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#good-and-bad-substitutes",
    "title": "Introduction to machine learning",
    "section": "Good and Bad Substitutes",
    "text": "Good and Bad Substitutes\n\nChoose metrics carefully\nSubstitute metrics can be misleading\nOptimize for the right goal\nUnderstand side effects"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#communicating-results",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#communicating-results",
    "title": "Introduction to machine learning",
    "section": "Communicating Results",
    "text": "Communicating Results\n\nExplain why your approach works\nCommunicate uncertainty\nShow impact and limitations\nConvince stakeholders"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#explainable-results",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#explainable-results",
    "title": "Introduction to machine learning",
    "section": "Explainable Results",
    "text": "Explainable Results\n\n\n\n\nUsers want to know why recommendations are made\nExplainability improves engagement\nImportant for trust and transparency"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#ethical-considerations",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#ethical-considerations",
    "title": "Introduction to machine learning",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\n\n\n\nBias in risk assessments\nFairness in automated decisions\nTransparency and accountability"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#ethics-its-in-the-application",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#ethics-its-in-the-application",
    "title": "Introduction to machine learning",
    "section": "Ethics: It’s in the Application!",
    "text": "Ethics: It’s in the Application!\n\nUnderstand biases in your system\nConsider the impact of predictions\nUse algorithms responsibly\nSame algorithm, different outcomes"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#data-and-data-collection",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#data-and-data-collection",
    "title": "Introduction to machine learning",
    "section": "Data and Data Collection",
    "text": "Data and Data Collection\n\nCritical component of ML\nMore data usually helps (if from right source)\nConsider marginal cost vs. marginal benefit"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#free-vs-expensive-data",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#free-vs-expensive-data",
    "title": "Introduction to machine learning",
    "section": "Free vs Expensive Data",
    "text": "Free vs Expensive Data\n\n\nFree Data\n\nOpen data from gov and census, often aggregated (e.g. ONS, London Fire Brigade, NASA)\nOpen data from companies (e.g. Google Street View) (Read the license first)\nSynthetic data\nWeb scraping (be careful with legality and ethics)\n\n\nExpensive Data\n\nIndividual data (e.g. health records)\nMobile phone data (high cost)\nUser survey\nHigh-resolution imagery (e.g. satellite, aerial, drone)"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#big-data-considerations",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#big-data-considerations",
    "title": "Introduction to machine learning",
    "section": "Big Data Considerations",
    "text": "Big Data Considerations\n\nMore data can be more expensive to work with\nSubsample to RAM when possible (512GB available in cloud)\nRuntime and analyst time matter\nAlways try with a small sample first"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#theorem-garbage-in-garbage-out-gigo",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#theorem-garbage-in-garbage-out-gigo",
    "title": "Introduction to machine learning",
    "section": "Theorem: Garbage in, garbage out (GIGO)",
    "text": "Theorem: Garbage in, garbage out (GIGO)\n\nGreat algorithms + bad data = bad results\n\n\n\n\nModel performance is constrained by data quality.\nBiased, noisy, or incomplete data leads to misleading predictions.\n\n\n\n\n\nImage Credit: x.com/xschelling/status/954936528555429888"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#good-data-large-size-high-quality.",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#good-data-large-size-high-quality.",
    "title": "Introduction to machine learning",
    "section": "Good data = large size + high quality.",
    "text": "Good data = large size + high quality.\n\n\n\nSufficient sample size to capture variability in the problem\n\nHigh-quality labels and accurate measurements.\n\nRepresentative of the population and application context.\n\n\n\n\n\nImage Credit: Internet"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#data-size-and-performance",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#data-size-and-performance",
    "title": "Introduction to machine learning",
    "section": "Data size and performance",
    "text": "Data size and performance\n\n\nThe performance of ML/DL increases rapidly with the size of the data:\n\nLarge neural nets benefit the most from big data.\n\nMedium and small neural nets also improve with more data.\n\nTraditional ML algorithms (e.g. random forest, SVM) may saturate earlier.\n\n\n\n\n\nPerformance vs data size for ML/DL models"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#feature-engineering-eda-90-time",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#feature-engineering-eda-90-time",
    "title": "Introduction to machine learning",
    "section": "Feature engineering & EDA (~90% time)",
    "text": "Feature engineering & EDA (~90% time)\n\nMostly manual rather than automated (no automated methods for EDA)\nDomain knowledge from expertise is desirable\nUsing visualisation to identify patterns & data relationship\nRemoving noisy or erroneous data\n\nDealing with missing data\n\nGenerating new features by combining existing ones"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning.html#example-representation-of-geospatial-locations-in-ml-models",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning.html#example-representation-of-geospatial-locations-in-ml-models",
    "title": "Introduction to machine learning",
    "section": "Example: Representation of geospatial locations in ML models",
    "text": "Example: Representation of geospatial locations in ML models\n\nRaw coordinates (e.g. long/lat) may not be directly useful\nNeed to engineer features that capture spatial relationships\nLong/lat\nDistance to POIs (train stations/schools).\nUsing adjacency matrix between spatial units"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "The Data Science for Spatial Systems (DSSS, CASA0006) module is a optional element of CASA’s MSc/MRes USS Course and is intended provide an introduction to advanced computational techniques in spatial analysis, or advanced spatial data sicence.\nAs with most computational analysis, spatial data science builds on two pillars: methods and data. In recent year, as AI (represented by deep learning and large-language models) has developed rapidly, various computational techniques have been invented and deployed, which enables complicated analysis of large-scale and heterougenous data. For example, we are now able to combine census data, remote sensing data, street-view imagery, and text data for understanding urban security or urban heat island effect on a laptop. Notably, (geospatial) foundation models, which are large, pre-trained AI models, are expected to become versatile bases for many diffferent tasks. It seems like we could model and predict almost everything in spatial analysis, even if we don’t understand what foundation models are doing. However, this is not entirely true. So far, AI is not yet silver bullet and AI produces halluciations. We need to understand the principle and limitations of various computational models; we need to be critical about methods and results; we need to ask critical questions for any task; and we need to link modelling with applications so as to inform policy and practice.\nThis module aims to provide students with a solid foundation in (geospatial) machine learning, focusing on supervised learning techniques and their applications to real-world problems. We acknowledge that this is not a full coverage of machine learning or data science, but we believe that mastering supervised learning is the natural foundation and crucial step for students to embark on their journey in (spatial) data science.\nThe module is structured progressively across three core parts. The first part (W1-3) introduces machinea learning (in general) and the metrics/workflow of supervised learning. The second part (W4-6) covers three two key types of supervised learning and their derivatives, including tree-based methods and neural networks. The third part (W7-10) focuses on advanced topics and applications, including model interpretation and feature selection, handling imbalanced data, machine learning operations (MLOps), and testing of machine learning systems."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe really appreciate the support from various people:\n\nJon for setting up this wonderful CASA-themed quarto template.\nOllie and Andy for inspiring us to pursue a Quarto-based module website."
  },
  {
    "objectID": "assessments/index.html",
    "href": "assessments/index.html",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "",
    "text": "This individual assignment aims to test your ability to conduct in-depth spatial data analysis using methods from this module. You are required to submit a single Python Notebook which contains both the code required to conduct the data analysis and accompanying text which provides context and interpretation.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#overview",
    "href": "assessments/index.html#overview",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "",
    "text": "This individual assignment aims to test your ability to conduct in-depth spatial data analysis using methods from this module. You are required to submit a single Python Notebook which contains both the code required to conduct the data analysis and accompanying text which provides context and interpretation.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#assessment-steps",
    "href": "assessments/index.html#assessment-steps",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "Assessment Steps",
    "text": "Assessment Steps\n\n1. Select a Main Dataset\nYou are recommended to choose one of the following datasets and define a research question that relates to urban or spatial processes. Feel free to use a subset of the chosen dataset, such as focusing on a specific crime type or a type of traffic incidents.\nIf you wish to use a different dataset, you are welcome to do so, as long as the dataset is relevant to urban or spatial processes and is appropriate for the analysis.\n\n\n\nDataset\nLink\nDescription\n\n\n\n\nLondon Crime Records\ndata.london.gov.uk\nCrime data across London\n\n\nChildhood Obesity Prevalence\ndata.london.gov.uk\nObesity rates by borough, ward, and MSOA\n\n\nRoad Safety Data (UK)\ndata.gov.uk\nIncident location, severity, weather, road conditions, vehicle types, etc.\n\n\n\n\n\n2. Define Your Research Question\nYour research question should be specific and clear. Examples: - “What is the relationship between crime rates and local deprivation in London?” - “Is it possible to predict childhood obesity prevalence using socio-demographic variables?” - “How do road accident rates vary by location and weather conditions in the UK?”\n\n\n3. Augment Your Data (Optional)\nYou can enhance your main dataset with additional datasets from: - Census and demographic data - Social indicators - Economic factors - Environmental variables\n\n\n4. Structure Your Analysis\nYou need to use the notebook template for this assessment.\nThe analysis should be captured in a single Python notebook containing: - All code for data analysis - Full documentation of the analysis process - Interpretation of results - Narrative text (max 1500 words, code/comments not included)\n\n\n5. Choose Your Methods\nYou can use up to 4 methods that are appropriate for your research question.\n\n\n6. Include Required Notebook Sections\nYour notebook must include these sections:\n\nIntroduction\n\nInclude at least 3 relevant studies from credible sources (Google Scholar, CrossRef, etc.)\nSet the context for your research\n\nResearch Questions\n\nState explicitly, ending with a question mark\nExample: “What is the relationship between Covid-19 mortality rate and local deprivation in the UK?”\n\nData\n\nDescribe your datasets\nExplain any preprocessing or cleaning\n\nMethodology\n\nExplain the methods you chose and why\nConnect them to your research question\n\nResults and Discussion\n\nPresent findings clearly\nInterpret results in context\n\nConclusion\n\nSummarise key findings\nDiscuss implications\n\nReferences\n\nUse credible sources only",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#marking-scheme",
    "href": "assessments/index.html#marking-scheme",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "Marking Scheme",
    "text": "Marking Scheme\nYour work will be assessed across these categories:\n\n\n\nCategory\nWeight\n\n\n\n\nAnalysis context and research questions\n15%\n\n\nData collection, handling, and presentation\n15%\n\n\nCorrectness, depth, and scope of data analysis\n35%\n\n\nVisualisation\n10%\n\n\nQuality of writing\n15%\n\n\nCreativity of analytical work\n10%\n\n\n\nA more details marking scheme is below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriterion\nA+ (80-100%)\nA (70-79%)\nB (60-69%)\nC (50-59%)\nFail (near pass) (40-49%)\nFail (1-39%)\n\n\n\n\n0\nAnalysis context and research questions (15%)\nThe report focuses on an analysis and interpretation of the dataset chosen. Each component of analysis, visualisation, database structure is detailed in the context of the overall project. The report overall conveys, excellently, the rationale of the methodology undertaken and the visualisations created compliment the story behind the data analysis extremely well. The project’s background references are framed extensively to wider projects/academic literature, far beyond that on the prescribed reading list. The research question is very interesting and insightful.\nThe report focuses on an analysis and interpretation of the dataset chosen. The report conveys the story behind the analysis well and the dataset chosen compliments the story and analysis undertaken. The project’s background references are framed well to wider projects/academic literature, with few to no inaccurate bibliography or citations. The research question is well thought out.\nThe report focuses on the analysis. The wider context of the work is clearly defined and how the work places in the wider context of research. The report conveys the story behind the analysis but some flaws in execution of the analysis distract from the overall narrative of the submission. The project’s background references are framed broadly to wider projects/academic literature. There may be some inaccuracies bibliography or citations. The research question is adequate and reflects some critical thinking of the topic.\nThe report shows a basic understanding of analysis and the data set chosen is limited in analytical scope. The story of the analysis is broken and the work raises more questions of the data then the work answers. The work satisfies the techniques that were taught during lectures but the wrong analytical techniques have been applied to the dataset. The project’s background references are framed but may be limited to the prescribed reading list/datasets. There may be obvious inaccuracies bibliography or citations. A fair research question is proposed but is not interesting.\nThe report shows a basic understanding of an analysis, but the data set chosen is unsuitable for an analysis. The story of the analysis is broken and incoherent. The work raises more questions of the data then the work answers. The report details the wrong techniques to analyse the data and provides no insight to the data being analysed. Very little background research is evident beyond use of the datasets. Bibliography may be incomplete or inconsistent and citations poor. A research question is proposed but needs substantial improvements.\nA poor attempt at the problem using an unsuitable dataset. The story behind the analysis is not present and the report has been rushed and thrown together haphazardly. There is no evidence of wider background research at all. No research questions.\n\n\n1\nData collection handling, and presentation (15%)\nA detailed description of the data is present. Data has been extensively treated and justified in preparation for analysis. There is a table that describes the variables selected for analysis so that the readers can easily understand the variables used for analysis.\nA detailed description of the data is present. Data has been extensively treated and justified in preparation for analysis. There is a table that describes the variables selected for analysis.\nA detailed description of the data is present. Data has been treated and justified in preparation for analysis. There are no obvious errors in variable selection.\nA detailed description of the data is present, including data provider, country, and URL (if the URL exists). Data has been treated in preparation for analysis. There are obvious errors in variable selection, for example, including identification columns for analysis, or incorrectly treating categorical variable as numerical ones.\nThere is some description of the dataset but essential information is missing, such as provider, country, or URL (if the URL exists). Data is incomplete and is not compatible for data analysis. The data is unstructured and no obvious work has been carried out on the dataset.\nNo data source has been identified or collected, or data source incomplete and unavailable.\n\n\n2\nCorrectness, depth and scope of data analysis (35%)\nThe work shows extensive knowledge of the topic chosen and an in-depth understanding of the links between the problem, data and method(s) employed to solve it. Shows considerable insight into any shortcomings of the analysis and demonstrates critical reflection on the finished product.\nThe work shows good knowledge of the chosen topic and a clear appreciation of the links between the problem, data and method(s) employed to solve it. Critical appraisal of the finished product is thoughtful and relevant. No errors in the notebook.\nThe work shows good knowledge of the chosen topic although there may be some minor issues with the choice of data and/or methods at the lower end of the scale. At the upper end of the scale, matching data and/or method to the problem is sound. Some evidence of critical reflection is displayed. Some minor errors in the code.\nThe work shows a dequate knowledge of the chosen topic but choice of data and/or methods could have been better. Critical reflection may be poor, or lacking entirely at the lower end of the scale. There are a few errors in the code but they don’t impact the presentation of the work.\nThe work shows poor knowledge of the chosen topic and the choice of data and/or methods is ill logical. Critical reflection is an alien concept. There are existing errors in the code which prevent the code from executing, or the results of coding are largely different from the discussion in the text.\nThe work shows that methods taught were not understood at all.\n\n\n3\nVisualisation (10%)\nFigures used to convey the visualisation of data analysed are of publishable quality; they are clear, well labelled and convey the intended information expertly.\nFigures used to convey the visualisation of data analysed are excellent; they are clear, well labelled and wholly appropriate. Visuals complement the report and display an understanding of the methodology executed within the report.\nFigures used to convey the visualisation of data analysed are overall good and well chosen, but at the lower end of the scale may contain minor errors. Visuals are lacking in clarity.\nFigures used to convey the visualisation of data analysed are adequate but errors detract from their usefulness. Visuals are lacking in context. The graphics may not be referenced within the document.\nFigures used to convey the visualisation of data analysed are poor and do not aid understanding in any way. The graphics are not referenced within the document and are out of order/place.\nFigures used to convey the visualisation of data analysed are bad or missing entirely.\n\n\n4\nQuality of writing (15%)\nThe final piece of work is presented to a professional standard. Exceptionally well written; stylish with no errors in spelling, punctuation or grammar. The piece of work is clearly and logically structured and enjoyable to read.\nThe final piece of work is presented very well and would only require minor editing. Very well written with virtually no errors in spelling, punctuation or grammar. The piece of work is clearly and logically structured and flows well.\nA good, well written piece of work with few errors in spelling, punctuation or grammar. The work shows structure and is clearly presented. The work may require the odd edit.\nThe final piece of work may lack polish and would need attention. A more-or-less competent piece of work but may contain some errors in spelling, punctuation or grammar. The may lack structure and presentation could be improved.\nThe final piece is poorly presented and would need serious editing. A more-or-less weak piece of work containing a number of errors in spelling, punctuation or grammar. The piece may lack any kind of structure.\nThe final piece is very badly presented. Only a complete re-write would bring it up to standard. A poor piece of work. Written English is bad, with numerous errors in spelling, punctuation and grammar.\n\n\n5\nCreativity of analytical work (10%)\nThe work is outstandingly creative and approaches the problem within a clear and analytical mind set. The execution of the analysis is also excellent and shows meticulous planning and understanding to the problem being addressed. The execution of the analysis is sound and the student has demonstrated understanding to the analytical process. At each stage the narrative is inferred by the analysis of the previous stage and the analysis has been planned from the outset.\nThe work is creative in the execution and shows that the student has thoroughly about the problem being tackled. At each stage the narrative is inferred by the analysis of the previous stage and the analysis has been planned from the outset.\nThe work is creative in nature, both in the execution and the thought process is apparent within the narrative. The execution is sound and the student has demonstrated understanding to the analytical process.\nThe works shows some creativity which is satisfactory, but some flaws in the execution or assertions are made within the work.\nThe work shows little to no creativity but falls short in the execution and understanding.\nThe work shows no creativity or relevance in research questions or methods, and there are major problems in the execution.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#key-considerations",
    "href": "assessments/index.html#key-considerations",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "Key Considerations",
    "text": "Key Considerations\n\n1. Use Rate, Not Count\nIn most analysis, rate is more appropriate than count because it normalises data by accounting for population size or scale.\nExample: In road safety research, calculate the rate of traffic incidents by normalising the count of incidents by traffic volume or population size.\n\n\n2. Use Credible References\n\nReferences must be relevant and credible (Google Scholar, CrossRef)\nDo not use unverified references generated by LLM - this is BAD ACADEMIC PRACTICE and will be penalised\n\n\n\n3. Choose Methods Wisely\n\nEnsure all methods are directly relevant to your research question\nAvoid including irrelevant or excessive methods; this reduces submission quality",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#major-problems-to-avoid",
    "href": "assessments/index.html#major-problems-to-avoid",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "Major Problems to Avoid",
    "text": "Major Problems to Avoid\nA high-quality submission (distinction/merit level) should NOT contain these issues:\n\n❌ ID Columns - Including identification columns for analysis without justification\n❌ Categorical as Numerical - Treating categorical variables as numerical ones incorrectly\n❌ Code-Discussion Mismatch - Inconsistency between code results and discussion (e.g. model accuracy differs)\n❌ Code Errors - Code in the notebook is not run or contains errors\n❌ Notebook-PDF Mismatch - Python notebook and PDF file are largely different\n❌ Excess printing dataframe - Printing large dataframes frequently or after each step is not necessary",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#submission",
    "href": "assessments/index.html#submission",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "Submission",
    "text": "Submission\n\nWhat to Submit\nPart 1: Python notebook (or .zip file with notebook + dataset files)\nPart 2: PDF file exported from the notebook\nSubmit both parts separately in two tabs on Moodle. The submission timestamp is based on the later of the two parts.\n\n\nCritical Rules\nYou will receive a mark of 0 if:\n\nYou fail to submit either Part 1 or Part 2\nThe content of Part 2 is largely different from Part 1\n\n\n\nDataset Requirements\n\nShare your dataset in a GitHub repository and remotely read it in the notebook (e.g. using read_csv)\nIf data size exceeds 100 MB (GitHub limit), submit a .zip file with the notebook and data\n\n\n\nLibrary Requirements\n\nIf possible, use only libraries from the recommended computing environment (Podman/Docker/Anaconda)\nIf you must use other libraries (including fastai):\n\nClearly state the library names and version numbers\n\n\n\n\nExecution Requirements\n\nNotebook must execute within 1 hour at submission; please remove unnecessary coding from the notebook\nUse Jupyter’s ‘Restart & Rerun all’ before submission to verify viability\nIf data preprocessing requires significant time, you may provide preprocessed data with a detailed description\n\n\n\n\n\n\n\nWarning\n\n\n\nPenalty will apply if the code is not run or results contain errors or are not clearly presented.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#exporting-to-pdf",
    "href": "assessments/index.html#exporting-to-pdf",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "Exporting to PDF",
    "text": "Exporting to PDF\n\nSteps to Export\n\nIn the web browser running your notebook, right-click\nSelect ‘Print’\nIn the printer dialog, select ‘Save as PDF’\nChoose your save location\n\nAlternative methods are acceptable as long as the PDF is text-selectable.\n\n\nMoodle Warning\nYou may see a warning about supported file types when uploading your notebook (see below). You can safely ignore this.\nYou must upload a supported file type for this assignment. \n\nAccepted file types are; .doc, .docx, .ppt, .pptx, .pps, .ppsx, .pdf, .txt, .htm, .html, .hwp, .odt, .wpd, .ps and .rtf",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assessments/index.html#final-checklist",
    "href": "assessments/index.html#final-checklist",
    "title": "CASA0006 Assessment Guidelines 2025-2026",
    "section": "Final Checklist",
    "text": "Final Checklist\nBefore submitting, ensure:\n\nYour notebook runs completely in under 1 hour. Don’t include lots of data pre-processing in the notebook.\nYou’ve run ‘Restart & Rerun all’ and all cells execute successfully\nYour research question is clearly stated\nAll methods are appropriate for your research question\nYour visualisations are clear and meaningful\nYour writing is clear and accessible\nYour PDF is exported and text-selectable\nYou’re submitting both notebook and PDF to Moodle\nYour notebook and PDF contain consistent content",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Getting Help",
    "section": "",
    "text": "We all need help from time to time, and while we will always do our best to support you because we know that this module is hard for students who are new to quantitative modules or statistics, the best way to ‘get help’ will also always be taking steps to ‘help yourself’ first."
  },
  {
    "objectID": "help.html#how-to-help-yourself",
    "href": "help.html#how-to-help-yourself",
    "title": "Getting Help",
    "section": "How to Help Yourself",
    "text": "How to Help Yourself\nHere are at least six things that you can do to ‘help yourself’:\n\nMake use of practical sessions–we can’t help you if we don’t know that you’re struggling. Please talk to the lecturer or TAs during the pratical sessions.\nUse the dedicated #casa0007_qm channel on Slack –this provides a much richer experience than the Moodle Forum and should be your primary means of requesting help outside of scheduled teaching hours.\nDo the readings–regardless of whether we ask you questions in class about them (or not), the readings are designed to support the module’s learning outcomes, so if you are struggling with a concept or an idea then please look to the week’s readings! You should also review the full bibliography while developing your thinking for the final project.\nUse Google or Stack Overflow–as you become a better programmer you’ll start to understand how to frame your question in ways that produce the right answer right away, but whether you’re a beginner or an expert Stack Overflow is your friend.\nSign up for online classes–there are lots of plausible online classes on LinkedIn course or Coursera. Please check the reviews before you take an online module."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "This page contains all articles/books from the weekly readings and some more that may be useful.\nPlease check the reading list on UCL Reading list (UCL users only).",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "",
    "text": "This week is focussed on ensuring that you’re able to access the teaching materials and to run Jupyter notebooks locally, as well as describing a dataset in Python."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#learning-outcomes",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#learning-outcomes",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nYou have familiarised yourself with how to access the lecture notes and Python notebook of this module.\nYou have familiarised yourself with running the Python notebooks locally.\nYou have familiarised yourself with using sklearn for supervised learning."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#set-up-the-tools",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#set-up-the-tools",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "Set up the tools",
    "text": "Set up the tools\nPlease follow the Setup page of CASA0013 to install and configure the computing platform, and this page to get started on using the container & JupyterLab."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#download-the-notebook",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#download-the-notebook",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "Download the Notebook",
    "text": "Download the Notebook\nSo for this week, visit the Week 1 of DSSS page, you’ll see that there is a ‘preview’ link and a a ‘download’ link. If you click the preview link you will be taken to the GitHub page for the notebook where it has been ‘rendered’ as a web page, which is not editable. To make the notebook useable on your computer, you need to download the IPYNB file.\nSo now:\n\nClick on the Download link.\nThe file should download automatically, but if you see a page of raw code, select File then Save Page As....\nMake sure you know where to find the file (e.g. Downloads or Desktop).\nMove the file to your Git repository folder (e.g. ~/Documents/CASA/DSSS/)\nCheck to see if your browser has added .txt to the file name:\n\nIf no, then you can move to adding the file.\nIf yes, then you can either fix the name in the Finder/Windows Explore, or you can do this in the Terminal using mv &lt;name_of_practical&gt;.ipynb.txt &lt;name_of_practical&gt;.ipynb (you can even do this in JupyterLab’s terminal if it’s already running)."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#running-notebooks-on-jupyterlab",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#running-notebooks-on-jupyterlab",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "Running notebooks on JupyterLab",
    "text": "Running notebooks on JupyterLab\nI am assuming that most of you are already running JupyterLab via Podman (or Docker) using the command.\nIf you are a bit confused with container, JupyterLab, terminal, or Git, please feel free to ask any questions.\nIn the following, we will introduce how to represent data and train a regression model using sklearn."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#load-libraries",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#load-libraries",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "Load libraries",
    "text": "Load libraries\n\nimport sklearn\nimport sklearn.datasets\nimport sklearn.metrics\n%matplotlib inline\nimport seaborn as sns; sns.set_theme()\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\n\nIt is important to check the version of sklearn and use this version when you search on the online documentation or ask sklearn questions. When reading the sklearn documentation online, please ensure that you choose the correct version in the dropdown box on the top-right corner.\n\nprint(sklearn.__version__)\n\n1.7.2"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#data-representation-in-scikit-learn",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#data-representation-in-scikit-learn",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "Data Representation in Scikit-Learn",
    "text": "Data Representation in Scikit-Learn\nMachine learning is about creating and training models from data: for that reason, we’ll start by discussing how data can be represented in order to be understood by the computer. The best way to think about data within Scikit-Learn is in terms of tables of data.\n\nData as table\nA basic table is a two-dimensional grid of data, in which the rows represent individual elements of the dataset, and the columns represent quantities related to each of these elements.\nHere, we will use the Iris dataset, which was created by Ronald Fisher in 1936 and is a classic multi-class classification dataset. Below is a picture showing the flower structure (source):\n\nThe sklearn.datasets module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It provides a list of classic datasets, including iris.\nThere are a few parameters in the funciton of load_iris. If return_X_y is set as False (by default), then it returns a Bunch object consisting of x and y variables. Otherwise, it returns (data, target) (aka, x and y objects) separately. If as_frame is True, the returned data is a pandas DataFrame.\n\niris = sklearn.datasets.load_iris(return_X_y=True, as_frame=True)\nprint(iris.__class__)\n\n&lt;class 'tuple'&gt;\n\n\nThe iris object is a tuple that consists of two DataFrames, namely features (or X_iris) and target (or y_iris).\nWe will separate these two DataFrames and rename the columns.\n\n# features DataFrame called X_iris\nX_iris = iris[0]\nX_iris = X_iris.rename(columns={\"sepal length (cm)\": \"sepal_length\", \n\"sepal width (cm)\": \"sepal_width\", \n\"petal length (cm)\": \"petal_length\",\n\"petal width (cm)\": \"petal_width\"\n})\n# target DataFrame called y_iris\ny_iris = iris[1]\n\nExplore the features DataFrame.\nHere, each row of the data refers to a single observed flower, and the number of rows is the total number of flowers in the dataset. In general, we will refer to the rows of the matrix as samples, and the number of rows as n_samples. Each column of the data refers to a particular quantitative piece of information that describes each sample. In general, we will refer to the columns of the matrix as features, and the number of columns as n_features.\n\nprint(X_iris.__class__)\nprint(\"Number of samples:{}\".format(X_iris.shape[0]))\nprint(\"Number of features:{}\".format(X_iris.shape[1]))\nprint(X_iris.head())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nNumber of samples:150\nNumber of features:4\n   sepal_length  sepal_width  petal_length  petal_width\n0           5.1          3.5           1.4          0.2\n1           4.9          3.0           1.4          0.2\n2           4.7          3.2           1.3          0.2\n3           4.6          3.1           1.5          0.2\n4           5.0          3.6           1.4          0.2\n\n\nExplore the target DataFrame.\n\nprint(y_iris.__class__)\nprint(y_iris.head())\n\n&lt;class 'pandas.core.series.Series'&gt;\n0    0\n1    0\n2    0\n3    0\n4    0\nName: target, dtype: int64\n\n\nBelow are some reflections on Features and Target.\n\nFeatures matrix\nThis table layout makes clear that the information can be thought of as a two-dimensional numerical array or matrix, which we will call the features matrix. By convention, this features matrix is often stored in a variable named X. The features matrix is assumed to be two-dimensional, with shape [n_samples, n_features], and is most often contained in a NumPy array or a Pandas DataFrame, though some Scikit-Learn models also accept SciPy sparse matrices.\nThe samples (i.e., rows) always refer to the individual objects described by the dataset. For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements.\nThe columns can be ofd different types, including real-value, string, dates, etc.\n\n\nTarget array\nIn addition to the feature matrix X, we also generally work with a label or target array, which by convention we will usually call y. The target array is usually one dimensional, with length n_samples, and is generally contained in a NumPy array or Pandas Series. The target array may have continuous numerical values, or discrete classes/labels. While some Scikit-Learn estimators do handle multiple target values in the form of a two-dimensional, [n_samples, n_targets] target array, we will primarily be working with the common case of a one-dimensional target array.\nOften one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to predict from the data: in statistical terms, it is the dependent variable. For example, in the preceding data we may wish to construct a model that can predict the species of flower based on the other measurements; in this case, the species column would be considered the target array.\nWe can use Seaborn to conveniently visualise the data:\n\n# will combine X and y into a DataFrame before plotting\niris_Xy = X_iris.assign(species = y_iris)\nprint(iris_Xy.columns)\nsns.pairplot(iris_Xy, hue='species', size=1.5)\n\nIndex(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n       'species'],\n      dtype='object')\n\n\n/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/seaborn/axisgrid.py:2100: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\n\n\nExtra data manipulation\nIn some projects, you might be given a single DataFrame that combines X and y instead of two separate DataFrame. Then you can use pandas operations to separate X and y. For example:\n\nX_iris = iris_Xy.drop('species', axis=1)\nX_iris.shape\n\n(150, 4)\n\n\n\ny_iris = iris_Xy['species']\ny_iris.shape\n\n(150,)\n\n\nTo summarise, the expected layout of features and target values is visualised in the following diagram:\n\n\n\nFigure\n\n\nWith this data properly formatted, we can move on to consider the estimator API of Scikit-Learn:"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#scikit-learns-estimator-api",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#scikit-learns-estimator-api",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "Scikit-Learn’s Estimator API",
    "text": "Scikit-Learn’s Estimator API\nThe Scikit-Learn API is designed with the following guiding principles in mind, as outlined in the Scikit-Learn API paper:\n\nConsistency: All objects share a common interface drawn from a limited set of methods, with consistent documentation.\nInspection: All specified parameter values are exposed as public attributes.\nLimited object hierarchy: Only algorithms are represented by Python classes; datasets are represented in standard formats (NumPy arrays, Pandas DataFrames, SciPy sparse matrices) and parameter names use standard Python strings.\nComposition: Many machine learning tasks can be expressed as sequences of more fundamental algorithms, and Scikit-Learn makes use of this wherever possible.\nSensible defaults: When models require user-specified parameters, the library defines an appropriate default value.\n\nIn practice, these principles make Scikit-Learn very easy to use, once the basic principles are understood. Every machine learning algorithm in Scikit-Learn is implemented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications.\n\nBasics of the API\nMost commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow).\n\nChoose a class of model by importing the appropriate estimator class from Scikit-Learn.\nChoose model hyperparameters by instantiating this class with desired values.\nArrange data into a features matrix and target vector following the discussion above.\nFit the model to your data by calling the fit() method of the model instance.\nApply the Model to new data:\n\nFor supervised learning, often we predict labels for unknown data using the predict() method.\nFor unsupervised learning, we often transform or infer properties of the data using the transform() or predict() method.\n\n\nWe will now step through several simple examples of applying supervised and unsupervised learning methods.\n\n\nSupervised learning example: Simple linear regression\nAs an example of this process, let’s consider a simple linear regression—that is, the common case of fitting a line to \\((x, y)\\) data.\nWe want to explore the relationship between Sepal length and Petal length.\n\nX_iris.plot.scatter(x='sepal_length',y='petal_length')\n\n\n\n\n\n\n\n\nFor convenience, we will use x and y to store these two variables.\n\nx = X_iris.sepal_length\ny = X_iris.petal_length\n\nWith this data in place, we can use the recipe outlined earlier. Let’s walk through the process:\n\n1. Choose a class of model\nIn Scikit-Learn, every class of model is represented by a Python class. If we would like to compute a simple linear regression model, we can import the linear regression class:\nfrom sklearn.linear_model import LinearRegression\nNote that other more general linear regression models exist as well; you can read more about them in the sklearn.linear_model module documentation.\n\n\n2. Choose model hyperparameters\nAn important point is that a class of model is not the same as an instance of a model.\nOnce we have decided on our model class, there are still some options open to us. Depending on the model class we are working with, we might need to answer one or more questions like the following:\n\nWould we like to fit for the offset (i.e., y-intercept)?\nWould we like the model to be normalized?\nWould we like to preprocess our features to add model flexibility?\nWhat degree of regularization would we like to use in our model?\nHow many model components would we like to use?\n\nThese are examples of the important choices that must be made once the model class is selected. These choices are often represented as hyperparameters, or parameters that must be set before the model is fit to data. In Scikit-Learn, hyperparameters are chosen by passing values at model instantiation. We will explore how you can motivate and justify the choice of hyperparameters in the later weeks.\nFor our linear regression example, we can instantiate the LinearRegression class and specify that we would like to fit the intercept using the fit_intercept hyperparameter:\n\nmodel = LinearRegression()\nmodel\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nKeep in mind that when the model is instantiated, the only action is the storing of these hyperparameter values. In particular, we have not yet applied the model to any data: the Scikit-Learn API makes very clear the distinction between choice of model and application of model to data.\n\n\n3. Arrange data into a features matrix and target vector\nPreviously we detailed the Scikit-Learn data representation, which requires a two-dimensional features matrix and a one-dimensional target array. Here our target variable y is already in the correct form (a length-n_samples array), but we need to massage the data x to make it a matrix of size [n_samples, n_features]. In this case, this amounts to a simple reshaping of the one-dimensional array:\n\nprint(\"Shape of the original x object: {}\".format(x.shape))\nX = x.to_numpy()[:, np.newaxis]\nprint(\"Shape of the X object: {}\".format(X.shape))\n\nShape of the original x object: (150,)\nShape of the X object: (150, 1)\n\n\nThe difference between two shapes, (150,) and (150,1) is noteworthy, as it has caused many problems when I used sklearn.\nWhat is np.newaxis? Simply put, it is used to increase the dimension of the existing array by one more dimension, when used once.\n\n\n\n4. Fit the model to your data\nNow it is time to apply our model to data. This can be done with the fit() method of the model:\n\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nThis fit() command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore.\nIn Scikit-Learn, by convention all model parameters that were learned during the fit() process have trailing underscores; for example in this linear model, we have the following:\n\nprint(\"Coef: {}\".format(model.coef_))\nprint(\"Slope: {}\".format(model.intercept_))\nprint(\"Formula: petal_length = {} * sepal_length + {}\".format(np.round(model.coef_[0],2), np.round(model.intercept_, 2)))\n\nCoef: [1.85843298]\nSlope: -7.101443369602455\nFormula: petal_length = 1.86 * sepal_length + -7.1\n\n\nTo get the goodness-of-fit or R square of this model, use the sklearn.metrics module.\n\ny_pred = model.predict(X)\nprint(\"The R^2 of this model is: {}\".format(sklearn.metrics.r2_score(y, y_pred)))\n\nThe R^2 of this model is: 0.759954645772515\n\n\nAnother question that frequently comes up regards the uncertainty or standard deviation in such internal model parameters.\nIn general, Scikit-Learn does not provide tools to easily draw conclusions from internal model parameters themselves: interpreting model parameters is much more a statistical modeling question than a machine learning question. Machine learning rather focuses on what the model predicts.\nIf you would like to build a linear regression model that estimates the uncertainty of model parameters, you can have a look at Statsmodels package.\n\n\n5. Predict labels for unknown data\nOnce the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set.\nIn Scikit-Learn, this can be done using the predict() method. For the sake of this example, our “new data” will be a grid of x values, and we will ask what y values the model predicts:\n\n# the xfit is a list of x values between 4.5 and 8.0, with step length of 0.5\nxfit = np.arange(4.5, 8, 0.5)\n\nAs before, we need to coerce these x values into a [n_samples, n_features] features matrix, after which we can feed it to the model:\n\nXfit = xfit[:, np.newaxis]\nyfit = model.predict(Xfit)\n\nFinally, let’s visualize the results by plotting first the raw data, and then this model fit:\n\nplt.scatter(x, y)\nplt.plot(xfit, yfit)"
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#youre-done",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#youre-done",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "You’re Done!",
    "text": "You’re Done!\nIn this session, we have covered the essential features of the sklearn data representation, and the estimator API.\nRegardless of the type of estimator, the same import -&gt; instantiate -&gt; fit -&gt; predict workflow holds.\nArmed with this information about the estimator API, you can explore the Scikit-Learn documentation and begin trying out various models on your data."
  },
  {
    "objectID": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#references-and-recommendations",
    "href": "sessions/W01_intro_machine_learning/intro_machine_learning_practical.html#references-and-recommendations",
    "title": "Practical 1: Introduction to supervised learning in sklearn",
    "section": "References and recommendations:",
    "text": "References and recommendations:\n\nThe introduction to sklearn is heavily based on this notebook, which is part of the online repo of the book “Python Data Science Handbook”.\nIf you want to learn a bit more about sklearn, see Introduction to Machine Learning with Scikit-Learn."
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#last-week",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#last-week",
    "title": "Supervised learning workflow",
    "section": "Last week",
    "text": "Last week\n\nFramework of supervised learning\nEvaluation metrics for regression and classification"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#objectives-of-this-week",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#objectives-of-this-week",
    "title": "Supervised learning workflow",
    "section": "Objectives of this week",
    "text": "Objectives of this week\n\nUnderstand different workflow of supervised learning.\nUnderstand train-test split and train-validation-test split.\nUnderstand cross validation and its extensions\nKnow when to use different model evaluation methods."
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#supervised-learning",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#supervised-learning",
    "title": "Supervised learning workflow",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in\n\\begin{cases}\n\\mathbb{R}, & \\text{(regression)} \\\\\n\\mathcal{Y} \\text{ (finite set)}, & \\text{(classification)}\n\\end{cases} \\\\\n\\text{learn } f(x_i) &\\approx y_i \\\\\n\\text{such that } f(x) &\\approx y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#challenges-of-training-supervised-models",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#challenges-of-training-supervised-models",
    "title": "Supervised learning workflow",
    "section": "Challenges of training supervised models",
    "text": "Challenges of training supervised models\n\nGeneralise to new data\nAvoid overfitting/underfitting\nModel selection (hyperparameter tuning)"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-7525",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-7525",
    "title": "Supervised learning workflow",
    "section": "Train-Test Split (75/25)",
    "text": "Train-Test Split (75/25)"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#why-train-test-split",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#why-train-test-split",
    "title": "Supervised learning workflow",
    "section": "Why Train-Test Split?",
    "text": "Why Train-Test Split?\n\n\nStatistics\n\nNo train-test split\nTrain and evaluate on whole data\nEstimation is key\nLow model complexity\n\n\nMachine Learning\n\nTrain-test split\nTrain on part, evaluate on held-out part\nPrediction (Generalisation) is key\nHigh model complexity; overfitting risk"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#influence-of-n_neighbors-k3",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#influence-of-n_neighbors-k3",
    "title": "Supervised learning workflow",
    "section": "Influence of n_neighbors (k=3)",
    "text": "Influence of n_neighbors (k=3)\n\nLarger k → smoother boundary\nSmaller k → complex boundary"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting",
    "title": "Supervised learning workflow",
    "section": "Overfitting vs Underfitting",
    "text": "Overfitting vs Underfitting"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting-2",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting-2",
    "title": "Supervised learning workflow",
    "section": "Overfitting vs Underfitting",
    "text": "Overfitting vs Underfitting"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#so-far-happy-ending",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#so-far-happy-ending",
    "title": "Supervised learning workflow",
    "section": "So far: Happy ending?",
    "text": "So far: Happy ending?\n\nReport: best k=19, test accuracy=0.77\nGood for choosing k\nBut: overly optimistic for generalisation\nProblem: test set used for both choosing k and final evaluation"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#why-overfitting-the-validation-set",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#why-overfitting-the-validation-set",
    "title": "Supervised learning workflow",
    "section": "Why? overfitting the Validation Set",
    "text": "Why? overfitting the Validation Set\n\n\n\n\nInteresting reading Preventing Overfitting in cross-validation - Ng 1997"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-the-validation-set",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overfitting-the-validation-set",
    "title": "Supervised learning workflow",
    "section": "Overfitting the Validation Set",
    "text": "Overfitting the Validation Set"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#threefold-split-code",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#threefold-split-code",
    "title": "Supervised learning workflow",
    "section": "Threefold Split (Code)",
    "text": "Threefold Split (Code)\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=0)\n\nval_scores = []\nneighbors = np.arange(1, 15, 2)\nfor i in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    val_scores.append(knn.score(X_val, y_val))\nprint(f\"best validation score: {np.max(val_scores):.3}\\n\")\nbest_n_neighbors = neighbors[np.argmax(val_scores)]\nprint(f\"best n_neighbors:{best_n_neighbors}\\n\")\n\nknn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\nknn.fit(X_trainval, y_trainval)\nprint(f\"test-set score: {knn.score(X_test, y_test):.3f}\")\nbest validation score: 0.991 best n_neighbors: 11 test-set score: 0.951\n\nHere is an implementation of the three-fold split for selecting the number of neighbors. For each number of neighbors that we want to try, we build a model on the training set, and evaluate it on the validation set. We then pick the best validation set score, here that’s 99.1%, achieved when using 11 neighbors. We then retrain the model with this parameter, and evaluate on the test set. The retraining step is somewhat optional. We could also just use the best model. But retraining allows us to make better use of all the data.\nStill, our results depend on how exactly we split the datasets. So how can we make this more robust?"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#new-problem-with-threefold-split",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#new-problem-with-threefold-split",
    "title": "Supervised learning workflow",
    "section": "New problem with threefold split",
    "text": "New problem with threefold split\n\nFixed train/val/test split → results depend on split\nHigh variance in best k and test score, not robust\n\n\n\n   random_seed  best_validation_score  best_k  test_set_score\n0            0                    0.7       5        0.846154\n1            1                    0.7       1        0.538462\n2            2                    1.0      13        0.692308\n3            3                    0.7       1        0.846154\n4            4                    0.9       5        0.769231\n5            5                    0.8      11        0.769231"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-cv-test-set",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-cv-test-set",
    "title": "Supervised learning workflow",
    "section": "Cross-Validation (CV) + Test Set",
    "text": "Cross-Validation (CV) + Test Set"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#n_neighbors-search-results",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#n_neighbors-search-results",
    "title": "Supervised learning workflow",
    "section": "n_neighbors Search Results",
    "text": "n_neighbors Search Results"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-strategies",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-strategies",
    "title": "Supervised learning workflow",
    "section": "Cross-Validation Strategies",
    "text": "Cross-Validation Strategies"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#how-many-cv-folds",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#how-many-cv-folds",
    "title": "Supervised learning workflow",
    "section": "How many CV folds?",
    "text": "How many CV folds?\n\nRecommend to run 5-fold or 10-fold CV multiple times, while shuffling the dataset\nMore folds → more training data per fold → better generlisation performance estimate, but slower\nExtreme: LeaveOneOut CV (one fold per sample)"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#repeated-kfold-and-leaveoneout",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#repeated-kfold-and-leaveoneout",
    "title": "Supervised learning workflow",
    "section": "Repeated KFold and LeaveOneOut",
    "text": "Repeated KFold and LeaveOneOut\n\nLeaveOneOut: high variance, slow\nShuffleSplit: repeated random splits\nRepeatedKFold: multiple shuffled KFold runs"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#shuffle-split",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#shuffle-split",
    "title": "Supervised learning workflow",
    "section": "Shuffle Split",
    "text": "Shuffle Split"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#standard-cv-not-preserving-class-distribution",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#standard-cv-not-preserving-class-distribution",
    "title": "Supervised learning workflow",
    "section": "Standard CV not preserving class distribution",
    "text": "Standard CV not preserving class distribution\n\nStandard CV leads to folds with different class distributions"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#stratified-cv-for-multiclass-classification-or-imbalanced-data",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#stratified-cv-for-multiclass-classification-or-imbalanced-data",
    "title": "Supervised learning workflow",
    "section": "Stratified CV: for multiclass classification or imbalanced data",
    "text": "Stratified CV: for multiclass classification or imbalanced data\n\nPreserve class distribution in each fold"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cv-in-scikit-learn",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cv-in-scikit-learn",
    "title": "Supervised learning workflow",
    "section": "CV in scikit-learn",
    "text": "CV in scikit-learn\n\n5-fold CV (default)\nClassification CV is stratified by default\ntrain_test_split(..., stratify=y) to stratify\nNo shuffle by default (repeatable results)"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cv-for-grouped-data",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#cv-for-grouped-data",
    "title": "Supervised learning workflow",
    "section": "CV for grouped data?",
    "text": "CV for grouped data?\n\nCV is more complicated when data are grouped\nData points within a group are correlated (e.g., city, patient, user)\ne.g. The task is to if a patient has a disease based on medical records from 9 cities\nHow CV should be done depends on the application scenario"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#scenario-1-to-predict-new-data-from-existing-cities-i.i.d.",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#scenario-1-to-predict-new-data-from-existing-cities-i.i.d.",
    "title": "Supervised learning workflow",
    "section": "Scenario 1: To predict new data from existing cities (i.i.d.)",
    "text": "Scenario 1: To predict new data from existing cities (i.i.d.)\n\nStandard CV (e.g. KFold, RepeatedKFold) can be used\nGroup information can be ignored"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#scenario-2-to-predict-new-data-from-unknown-cities-not-i.i.d.",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#scenario-2-to-predict-new-data-from-unknown-cities-not-i.i.d.",
    "title": "Supervised learning workflow",
    "section": "Scenario 2: To predict new data from unknown cities (not i.i.d.)",
    "text": "Scenario 2: To predict new data from unknown cities (not i.i.d.)\n\nGroupKFold should be used; ensure each group is contained in exactly one fold (either train or test)\nData from 9 cities; 5-fold CV; GroupKFold as below."
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#standard-train-test-split-or-cv-not-suitable-for-time-series",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#standard-train-test-split-or-cv-not-suitable-for-time-series",
    "title": "Supervised learning workflow",
    "section": "Standard train-test split or CV not suitable for time series",
    "text": "Standard train-test split or CV not suitable for time series"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-for-time-series",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-for-time-series",
    "title": "Supervised learning workflow",
    "section": "Train-Test Split for Time Series",
    "text": "Train-Test Split for Time Series\n\nUsing past data to train, future data to test"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#timeseriessplit",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#timeseriessplit",
    "title": "Supervised learning workflow",
    "section": "TimeSeriesSplit",
    "text": "TimeSeriesSplit"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#time-series-cv",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#time-series-cv",
    "title": "Supervised learning workflow",
    "section": "Time Series CV",
    "text": "Time Series CV"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overview",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#overview",
    "title": "Supervised learning workflow",
    "section": "Overview",
    "text": "Overview\nWe’ve covered:\n\nTrain-test split, threefold split\nCross-validation and its extensions, especially RepeatedKFold\nStratified CV (StratifiedKFold) for multiple classes and imbalanced data\nCV for grouped data and time series data"
  },
  {
    "objectID": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#questions",
    "href": "sessions/W03_supervised_learning_workflow/supervised_learning_workflow.html#questions",
    "title": "Supervised learning workflow",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#introduction-to-keras",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#introduction-to-keras",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Introduction to Keras",
    "text": "Introduction to Keras"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#keras-sequential",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#keras-sequential",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Keras Sequential",
    "text": "Keras Sequential\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential([\n    Dense(32, input_shape=(784,)),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax')])\n\n# or\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\n\n# or\nmodel = Sequential([\n    Dense(32, input_shape=(784,), activation='relu'),\n    Dense(10, activation='softmax')])"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#model-summary",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#model-summary",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Model Summary",
    "text": "Model Summary\nmodel.summary()"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#setting-optimizer",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#setting-optimizer",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Setting Optimizer",
    "text": "Setting Optimizer\n\n\n\nmodel.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#training-the-model",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#training-the-model",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Training the Model",
    "text": "Training the Model"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#preparing-mnist-data",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#preparing-mnist-data",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Preparing MNIST Data",
    "text": "Preparing MNIST Data\nfrom keras.datasets import mnist\nimport keras\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#fit-model",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#fit-model",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Fit Model",
    "text": "Fit Model\nmodel.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#fit-with-validation",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#fit-with-validation",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Fit with Validation",
    "text": "Fit with Validation\nmodel.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1,\n          validation_split=.1)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#evaluating-on-test-set",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#evaluating-on-test-set",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Evaluating on Test Set",
    "text": "Evaluating on Test Set\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Test loss: {:.3f}\".format(score[0]))\nprint(\"Test Accuracy: {:.3f}\".format(score[1]))\nTest loss: 0.120\nTest Accuracy: 0.966"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#loggers-and-callbacks",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#loggers-and-callbacks",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Loggers and Callbacks",
    "text": "Loggers and Callbacks\nhistory_callback = model.fit(X_train, y_train, batch_size=128,\n                             epochs=100, verbose=1, validation_split=.1)\npd.DataFrame(history_callback.history).plot()"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#wrappers-for-sklearn",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#wrappers-for-sklearn",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Wrappers for sklearn",
    "text": "Wrappers for sklearn\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nfrom sklearn.model_selection import GridSearchCV\n\ndef make_model(optimizer=\"adam\", hidden_size=32):\n    model = Sequential([\n        Dense(hidden_size, input_shape=(784,)),\n        Activation('relu'),\n        Dense(10),\n        Activation('softmax'),\n    ])\n    model.compile(optimizer=optimizer,loss=\"categorical_crossentropy\",\n                  metrics=['accuracy'])\n    return model\n\nclf = KerasClassifier(make_model)\nparam_grid = {'epochs': [1, 5, 10],\n              'hidden_size': [32, 64, 256]}\ngrid = GridSearchCV(clf, param_grid=param_grid)\ngrid.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#grid-search-results",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#grid-search-results",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Grid Search Results",
    "text": "Grid Search Results\nres = pd.DataFrame(grid.cv_results_)\nres.pivot_table(index=[\"param_epochs\", \"param_hidden_size\"],\n                values=['mean_train_score', \"mean_test_score\"])"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutional-neural-networks",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutional-neural-networks",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#idea-behind-cnns",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#idea-behind-cnns",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Idea Behind CNNs",
    "text": "Idea Behind CNNs\n\nTranslation invariance\nWeight sharing"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#definition-of-convolution",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#definition-of-convolution",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Definition of Convolution",
    "text": "Definition of Convolution\n\\[(f*g)[n] = \\sum\\limits_{m=-\\infty}^\\infty f[m]g[n-m]\\]\n\\[= \\sum\\limits_{m=-\\infty}^\\infty f[n-m]g[m]\\]"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-example-gaussian-smoothing",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-example-gaussian-smoothing",
    "title": "Keras & Convolutional Neural Nets",
    "section": "1D Example: Gaussian Smoothing",
    "text": "1D Example: Gaussian Smoothing"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutions-in-2d",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutions-in-2d",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Convolutions in 2D",
    "text": "Convolutions in 2D\n\n\n\nsource: Arden Dertat"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-convolution-animation",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-convolution-animation",
    "title": "Keras & Convolutional Neural Nets",
    "section": "2D Convolution Animation",
    "text": "2D Convolution Animation\n\n\n\nsource: Arden Dertat"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-smoothing",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-smoothing",
    "title": "Keras & Convolutional Neural Nets",
    "section": "2D Smoothing",
    "text": "2D Smoothing"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-gradients",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#d-gradients",
    "title": "Keras & Convolutional Neural Nets",
    "section": "2D Gradients",
    "text": "2D Gradients"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#max-pooling",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#max-pooling",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Max Pooling",
    "text": "Max Pooling\n\n\n\n\nNeed to remember position of maximum for back-propagation\nAgain not differentiable → subgradient descent"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutional-neural-networks-1",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#convolutional-neural-networks-1",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\n\n\n\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner: Gradient-based learning applied to document recognition"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#other-architectures",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#other-architectures",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Other Architectures",
    "text": "Other Architectures"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#conv-nets-with-keras",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#conv-nets-with-keras",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Conv-nets with Keras",
    "text": "Conv-nets with Keras"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#preparing-data",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#preparing-data",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Preparing Data",
    "text": "Preparing Data\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\nimg_rows, img_cols = 28, 28\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nX_train_images = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nX_test_images = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#create-tiny-network",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#create-tiny-network",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Create Tiny Network",
    "text": "Create Tiny Network\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\n\nnum_classes = 10\ncnn = Sequential()\ncnn.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Conv2D(32, (3, 3), activation='relu'))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Flatten())\ncnn.add(Dense(64, activation='relu'))\ncnn.add(Dense(num_classes, activation='softmax'))"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#number-of-parameters",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#number-of-parameters",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Number of Parameters",
    "text": "Number of Parameters\n\n\nConvolutional Network for MNIST\n\n\n\n\nDense Network for MNIST"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#train-and-evaluate",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#train-and-evaluate",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Train and Evaluate",
    "text": "Train and Evaluate\ncnn.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])\nhistory_cnn = cnn.fit(X_train_images, y_train,\n                      batch_size=128, epochs=20, verbose=1, validation_split=.1)\ncnn.evaluate(X_test_images, y_test)\n 9952/10000 [============================&gt;.] - ETA: 0s\n [0.089020583277629253, 0.98429999999999995]"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#visualize-filters",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#visualize-filters",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Visualize Filters",
    "text": "Visualize Filters\nweights, biases = cnn_small.layers[0].get_weights()\nweights2, biases2 = cnn_small.layers[2].get_weights()\nprint(weights.shape)\nprint(weights2.shape)\n(3,3,1,8)\n(3,3,8,8)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#learned-features",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#learned-features",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Learned Features",
    "text": "Learned Features"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#convnets-vs-fully-connected-nets-illustrated",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#convnets-vs-fully-connected-nets-illustrated",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Convnets vs Fully Connected Nets Illustrated",
    "text": "Convnets vs Fully Connected Nets Illustrated"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#mnist-and-permuted-mnist",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#mnist-and-permuted-mnist",
    "title": "Keras & Convolutional Neural Nets",
    "section": "MNIST and Permuted MNIST",
    "text": "MNIST and Permuted MNIST\n\n\n\nrng = np.random.RandomState(42)\nperm = rng.permutation(784)\nX_train_perm = X_train.reshape(-1, 784)[:, perm].reshape(-1, 28, 28)\nX_test_perm = X_test.reshape(-1, 784)[:, perm].reshape(-1, 28, 28)"
  },
  {
    "objectID": "sessions/W05_neural_networks/aml_convolutional_nets.html#questions",
    "href": "sessions/W05_neural_networks/aml_convolutional_nets.html#questions",
    "title": "Keras & Convolutional Neural Nets",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#history",
    "href": "sessions/W05_neural_networks/neural_networks.html#history",
    "title": "Neural Networks",
    "section": "History",
    "text": "History\n\nNearly everything we talk about today existed ~1990\nWhat changed?\n\nMore data\nFaster computers (GPUs)\nSome improvements: relu, dropout, adam, batch-normalization, residual networks"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#logistic-regression-as-neural-net",
    "href": "sessions/W05_neural_networks/neural_networks.html#logistic-regression-as-neural-net",
    "title": "Neural Networks",
    "section": "Logistic Regression as Neural Net",
    "text": "Logistic Regression as Neural Net"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#basic-architecture",
    "href": "sessions/W05_neural_networks/neural_networks.html#basic-architecture",
    "title": "Neural Networks",
    "section": "Basic Architecture",
    "text": "Basic Architecture\n\n\n\n\\(h(x) = f(W_1x+b_1)\\)\n\\(o(x) = g(W_2h(x) + b_2)\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#more-layers",
    "href": "sessions/W05_neural_networks/neural_networks.html#more-layers",
    "title": "Neural Networks",
    "section": "More Layers",
    "text": "More Layers\n\n\n\n\nHidden layers usually all have the same non-linear function\nMany layers → “deep learning”\nMultilayer perceptron, feed-forward neural network, vanilla feed-forward neural network\nRegression: single output neuron with linear activation\nClassification: one-hot-encoding of classes, n_classes output variables with softmax"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#nonlinear-activation-functions",
    "href": "sessions/W05_neural_networks/neural_networks.html#nonlinear-activation-functions",
    "title": "Neural Networks",
    "section": "Nonlinear Activation Functions",
    "text": "Nonlinear Activation Functions\n\n\n\n\nStandard choices: tanh or rectified linear unit (relu)\nTanh squashes between -1 and 1; saturates towards infinities\nReLU is constant zero for negative numbers, then identity"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#supervised-neural-networks",
    "href": "sessions/W05_neural_networks/neural_networks.html#supervised-neural-networks",
    "title": "Neural Networks",
    "section": "Supervised Neural Networks",
    "text": "Supervised Neural Networks\n\nNon-linear models for classification and regression\nWork well for very large datasets\nNon-convex optimization\nNotoriously slow to train – need for GPUs\nUse dot products; require preprocessing similar to SVM or linear models, unlike trees\nMany variants: Convolutional nets, GRUs, LSTMs, recursive networks, VAEs, GANs, deep RL"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#training-objective",
    "href": "sessions/W05_neural_networks/neural_networks.html#training-objective",
    "title": "Neural Networks",
    "section": "Training Objective",
    "text": "Training Objective\n\\(h(x) = f(W_1x+b_1)\\)\n\\(o(x) = g(W_2h(x)+b_2) = g(W_2f(W_1x + b_1) + b_2)\\)\n\\(\\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,o(x_i))\\)\n\\(= \\min_{W_1,W_2,b_1,b_2} \\sum\\limits_{i=1}^N l(y_i,g(W_2f(W_1x+b_1)+b_2))\\)\n\n\\(l\\) = Squared loss for regression; Cross-entropy loss for classification"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#backpropagation",
    "href": "sessions/W05_neural_networks/neural_networks.html#backpropagation",
    "title": "Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nNeed \\(\\frac{\\partial l(y, o)}{\\partial W_i}\\) and \\(\\frac{\\partial l(y, o)}{\\partial b_i}\\)\n\n\\(\\text{net}(x) := W_1x + b_1\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#gradient-computation",
    "href": "sessions/W05_neural_networks/neural_networks.html#gradient-computation",
    "title": "Neural Networks",
    "section": "Gradient Computation",
    "text": "Gradient Computation\n\nBackpropagation is clever application of chain rule for derivatives\nSingle backward pass from output to input computes derivatives\nNot an optimization algorithm, just a way to compute gradients"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#relu-differentiability",
    "href": "sessions/W05_neural_networks/neural_networks.html#relu-differentiability",
    "title": "Neural Networks",
    "section": "ReLU Differentiability",
    "text": "ReLU Differentiability\n\n\n\n\nReLU not differentiable at zero\nUse subgradient descent; any gradient below function works\nIn practice, never hit zero with floating point numbers"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#optimizing-w-b",
    "href": "sessions/W05_neural_networks/neural_networks.html#optimizing-w-b",
    "title": "Neural Networks",
    "section": "Optimizing W, b",
    "text": "Optimizing W, b\nBatch \\(W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=1}^N \\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)\nOnline/Stochastic \\(W_i \\leftarrow W_i - \\eta\\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)\nMinibatch \\(W_i \\leftarrow W_i - \\eta\\sum\\limits_{j=k}^{k+m} \\frac{\\partial l(x_j,y_j)}{\\partial W_i}\\)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#learning-heuristics",
    "href": "sessions/W05_neural_networks/neural_networks.html#learning-heuristics",
    "title": "Neural Networks",
    "section": "Learning Heuristics",
    "text": "Learning Heuristics\n\nConstant \\(\\eta\\) not good\nCan decrease \\(\\eta\\) over time\nBetter: adaptive \\(\\eta\\) for each entry of \\(W_i\\)\nState-of-the-art: adam (with magic numbers)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#picking-optimization-algorithms",
    "href": "sessions/W05_neural_networks/neural_networks.html#picking-optimization-algorithms",
    "title": "Neural Networks",
    "section": "Picking Optimization Algorithms",
    "text": "Picking Optimization Algorithms\n\nSmall dataset: off the shelf like l-bfgs\nBig dataset: adam / rmsprop\nHave time & nerve: tune the schedule"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#neural-nets-with-sklearn",
    "href": "sessions/W05_neural_networks/neural_networks.html#neural-nets-with-sklearn",
    "title": "Neural Networks",
    "section": "Neural Nets with sklearn",
    "text": "Neural Nets with sklearn\n\n\n\nmlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\nprint(mlp.score(X_train, y_train))\nprint(mlp.score(X_test, y_test))\n\nDon’t use sklearn for anything but toy problems in neural nets"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#random-state",
    "href": "sessions/W05_neural_networks/neural_networks.html#random-state",
    "title": "Neural Networks",
    "section": "Random State",
    "text": "Random State\n\n\n\n\nNetwork is way over capacity and can overfit in many ways\nRegularization might make it less dependent on initialization"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#hidden-layer-size",
    "href": "sessions/W05_neural_networks/neural_networks.html#hidden-layer-size",
    "title": "Neural Networks",
    "section": "Hidden Layer Size",
    "text": "Hidden Layer Size\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,), random_state=10)\nmlp.fit(X_train, y_train)\n\n\n\n\nSingle hidden layer with 5 units\nEach unit corresponds to different part of decision boundary"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#multiple-hidden-layers",
    "href": "sessions/W05_neural_networks/neural_networks.html#multiple-hidden-layers",
    "title": "Neural Networks",
    "section": "Multiple Hidden Layers",
    "text": "Multiple Hidden Layers\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10), random_state=0)\nmlp.fit(X_train, y_train)\n\n\n\n\n3 hidden layers each of size 10\nMain way to control complexity"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#activation-functions",
    "href": "sessions/W05_neural_networks/neural_networks.html#activation-functions",
    "title": "Neural Networks",
    "section": "Activation Functions",
    "text": "Activation Functions\nmlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10, 10, 10),\n                    activation='tanh', random_state=0)\nmlp.fit(X_train, y_train)\n\n\n\n\nUsing tanh gives smoother boundaries\nReLU doesn’t work as well with l-bfgs on small networks\nFor large networks, relu is preferred"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#regression",
    "href": "sessions/W05_neural_networks/neural_networks.html#regression",
    "title": "Neural Networks",
    "section": "Regression",
    "text": "Regression\n\n\n\nfrom sklearn.neural_network import MLPRegressor\nmlp_relu = MLPRegressor(solver=\\\"lbfgs\\\").fit(X, y)\nmlp_tanh = MLPRegressor(solver=\\\"lbfgs\\\", activation='tanh').fit(X, y)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#complexity-control",
    "href": "sessions/W05_neural_networks/neural_networks.html#complexity-control",
    "title": "Neural Networks",
    "section": "Complexity Control",
    "text": "Complexity Control\n\nNumber of parameters\nRegularization\nEarly Stopping\nDropout"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#grid-searching-neural-nets",
    "href": "sessions/W05_neural_networks/neural_networks.html#grid-searching-neural-nets",
    "title": "Neural Networks",
    "section": "Grid-Searching Neural Nets",
    "text": "Grid-Searching Neural Nets\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, stratify=data.target, random_state=0)\n\nfrom sklearn.model_selection import GridSearchCV\npipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\\\"lbfgs\\\", random_state=1))\nparam_grid = {'mlpclassifier__alpha': np.logspace(-3, 3, 7)}\ngrid = GridSearchCV(pipe, param_grid)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#searching-hidden-layer-sizes",
    "href": "sessions/W05_neural_networks/neural_networks.html#searching-hidden-layer-sizes",
    "title": "Neural Networks",
    "section": "Searching Hidden Layer Sizes",
    "text": "Searching Hidden Layer Sizes\nfrom sklearn.model_selection import GridSearchCV\npipe = make_pipeline(StandardScaler(), MLPClassifier(solver=\\\"lbfgs\\\", random_state=1))\nparam_grid = {'mlpclassifier__hidden_layer_sizes':\n              [(10,), (50,), (100,), (500,), (10, 10), (50, 50), (100, 100), (500, 500)]}\ngrid = GridSearchCV(pipe, param_grid)\ngrid.fit(X_train, y_train)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#write-your-own-neural-networks",
    "href": "sessions/W05_neural_networks/neural_networks.html#write-your-own-neural-networks",
    "title": "Neural Networks",
    "section": "Write Your Own Neural Networks",
    "text": "Write Your Own Neural Networks\nclass NeuralNetwork(object):\n    def __init__(self):\n        # initialize coefficients and biases\n        pass\n    def forward(self, x):\n        activation = x\n        for coef, bias in zip(self.coef_, self.bias_):\n            activation = self.nonlinearity(np.dot(activation, coef) + bias)\n        return activation\n    def backward(self, x):\n        # compute gradient of stuff in forward pass\n        pass"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#autodiff",
    "href": "sessions/W05_neural_networks/neural_networks.html#autodiff",
    "title": "Neural Networks",
    "section": "Autodiff",
    "text": "Autodiff\nclass array(object) :\n    \\\"\\\"\\\"Simple Array object that support autodiff.\\\"\\\"\\\"\n    def __init__(self, value, name=None):\n        self.value = value\n        if name:\n            self.grad = lambda g : {name : g}\n    def __add__(self, other):\n        assert isinstance(other, int)\n        ret = array(self.value + other)\n        ret.grad = lambda g : self.grad(g)\n        return ret\n    def __mul__(self, other):\n        assert isinstance(other, array)\n        ret = array(self.value * other.value)\n        def grad(g):\n            x = self.grad(g * other.value)\n            x.update(other.grad(g * self.value))\n            return x\n        ret.grad = grad\n        return ret"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#autodiff-example",
    "href": "sessions/W05_neural_networks/neural_networks.html#autodiff-example",
    "title": "Neural Networks",
    "section": "Autodiff Example",
    "text": "Autodiff Example\na = array(np.array([1, 2]), 'a')\nb = array(np.array([3, 4]), 'b')\nc = b * a\nd = c + 1\nprint(d.value)\nprint(d.grad(1))\n[4 9]\n{'b': array([1, 2]), 'a': array([3, 4])}\n\nAutomatic differentiation avoids writing gradients manually\nKeep track of computation while executing forward pass\nHard-code derivative for each operation (no symbolic differentiation)\nBuild computation graph automatically"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#gpu-support",
    "href": "sessions/W05_neural_networks/neural_networks.html#gpu-support",
    "title": "Neural Networks",
    "section": "GPU Support",
    "text": "GPU Support\n\n\n\n\nImportant limitation: GPUs have much less memory than RAM\nMemory copies between RAM and GPU are expensive"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#computation-graph",
    "href": "sessions/W05_neural_networks/neural_networks.html#computation-graph",
    "title": "Neural Networks",
    "section": "Computation Graph",
    "text": "Computation Graph\n\n\n\n\nStore different intermediate results depending on derivatives needed\nGiven limited GPU memory, important to know what to cache/discard\nHelps with visual debugging and understanding network structure"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#deep-learning-framework-requirements",
    "href": "sessions/W05_neural_networks/neural_networks.html#deep-learning-framework-requirements",
    "title": "Neural Networks",
    "section": "Deep Learning Framework Requirements",
    "text": "Deep Learning Framework Requirements\n\nAutodiff\nGPU support\nOptimization and inspection of computation graph\nOn-the-fly generation of computation graph (optional)\nDistribution over multiple GPUs and/or cluster (optional)\n\nCurrent choices: TensorFlow, PyTorch / Torch, Chainer"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#deep-learning-libraries",
    "href": "sessions/W05_neural_networks/neural_networks.html#deep-learning-libraries",
    "title": "Neural Networks",
    "section": "Deep Learning Libraries",
    "text": "Deep Learning Libraries\n\nKeras (TensorFlow, CNTK, Theano)\nPyTorch (torch)\nChainer (chainer)\nMXNet (MXNet)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#quick-look-at-tensorflow",
    "href": "sessions/W05_neural_networks/neural_networks.html#quick-look-at-tensorflow",
    "title": "Neural Networks",
    "section": "Quick Look at TensorFlow",
    "text": "Quick Look at TensorFlow\n\n\"Down to the metal\" - don’t use for everyday tasks\nThree steps for learning:\n\nBuild computation graph (using array operations and functions)\nCreate Optimizer (gradient descent, adam, etc.) attached to graph\nRun actual computation\n\nEager mode (default in TensorFlow 2.0): write imperative code directly"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#pytorch-example",
    "href": "sessions/W05_neural_networks/neural_networks.html#pytorch-example",
    "title": "Neural Networks",
    "section": "PyTorch Example",
    "text": "PyTorch Example\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n\nN = 100\nx = torch.randn(N, 1, device=device, dtype=dtype)\ny = torch.randn(N, 1, device=device, dtype=dtype)\nw = torch.randn(D_in, H, device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    y_pred = x.mm(w1)\n    loss = (y_pred - y).pow(2).sum().item()\n    loss.backward()\n    w1 -= learning_rate * w1.grad\n    w1.grad.zero_()"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#best-practices",
    "href": "sessions/W05_neural_networks/neural_networks.html#best-practices",
    "title": "Neural Networks",
    "section": "Best Practices",
    "text": "Best Practices\n\nDon’t go down to the metal (i.e. write low-level code) unless you have to!\nDon’t write TensorFlow, write Keras!\nDon’t write PyTorch, write pytorch.nn or FastAI (or Skorch or ignite)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#convolutional-neural-networks",
    "href": "sessions/W05_neural_networks/neural_networks.html#convolutional-neural-networks",
    "title": "Neural Networks",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#idea-behind-cnns",
    "href": "sessions/W05_neural_networks/neural_networks.html#idea-behind-cnns",
    "title": "Neural Networks",
    "section": "Idea Behind CNNs",
    "text": "Idea Behind CNNs\n\nTranslation invariance\nWeight sharing"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#definition-of-convolution",
    "href": "sessions/W05_neural_networks/neural_networks.html#definition-of-convolution",
    "title": "Neural Networks",
    "section": "Definition of Convolution",
    "text": "Definition of Convolution\n\\[(f*g)[n] = \\sum\\limits_{m=-\\infty}^\\infty f[m]g[n-m]\\]\n\\[= \\sum\\limits_{m=-\\infty}^\\infty f[n-m]g[m]\\]"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#d-example-gaussian-smoothing",
    "href": "sessions/W05_neural_networks/neural_networks.html#d-example-gaussian-smoothing",
    "title": "Neural Networks",
    "section": "1D Example: Gaussian Smoothing",
    "text": "1D Example: Gaussian Smoothing"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#convolutions-in-2d",
    "href": "sessions/W05_neural_networks/neural_networks.html#convolutions-in-2d",
    "title": "Neural Networks",
    "section": "Convolutions in 2D",
    "text": "Convolutions in 2D\n\n\n\nsource: Arden Dertat"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#d-convolution-animation",
    "href": "sessions/W05_neural_networks/neural_networks.html#d-convolution-animation",
    "title": "Neural Networks",
    "section": "2D Convolution Animation",
    "text": "2D Convolution Animation\n\n\n\nsource: Arden Dertat"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#d-smoothing",
    "href": "sessions/W05_neural_networks/neural_networks.html#d-smoothing",
    "title": "Neural Networks",
    "section": "2D Smoothing",
    "text": "2D Smoothing"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#d-gradients",
    "href": "sessions/W05_neural_networks/neural_networks.html#d-gradients",
    "title": "Neural Networks",
    "section": "2D Gradients",
    "text": "2D Gradients"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#max-pooling",
    "href": "sessions/W05_neural_networks/neural_networks.html#max-pooling",
    "title": "Neural Networks",
    "section": "Max Pooling",
    "text": "Max Pooling\n\n\n\n\nNeed to remember position of maximum for back-propagation\nAgain not differentiable → subgradient descent"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#convolutional-neural-networks-1",
    "href": "sessions/W05_neural_networks/neural_networks.html#convolutional-neural-networks-1",
    "title": "Neural Networks",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\n\n\n\n\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner: Gradient-based learning applied to document recognition"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#other-architectures",
    "href": "sessions/W05_neural_networks/neural_networks.html#other-architectures",
    "title": "Neural Networks",
    "section": "Other Architectures",
    "text": "Other Architectures"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#conv-nets-with-keras",
    "href": "sessions/W05_neural_networks/neural_networks.html#conv-nets-with-keras",
    "title": "Neural Networks",
    "section": "Conv-nets with Keras",
    "text": "Conv-nets with Keras"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#preparing-data",
    "href": "sessions/W05_neural_networks/neural_networks.html#preparing-data",
    "title": "Neural Networks",
    "section": "Preparing Data",
    "text": "Preparing Data\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\nimg_rows, img_cols = 28, 28\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nX_train_images = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\nX_test_images = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\ninput_shape = (img_rows, img_cols, 1)\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#create-tiny-network",
    "href": "sessions/W05_neural_networks/neural_networks.html#create-tiny-network",
    "title": "Neural Networks",
    "section": "Create Tiny Network",
    "text": "Create Tiny Network\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\n\nnum_classes = 10\ncnn = Sequential()\ncnn.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Conv2D(32, (3, 3), activation='relu'))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Flatten())\ncnn.add(Dense(64, activation='relu'))\ncnn.add(Dense(num_classes, activation='softmax'))"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#number-of-parameters",
    "href": "sessions/W05_neural_networks/neural_networks.html#number-of-parameters",
    "title": "Neural Networks",
    "section": "Number of Parameters",
    "text": "Number of Parameters\n\n\nConvolutional Network for MNIST\n\n\n\n\nDense Network for MNIST"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#train-and-evaluate",
    "href": "sessions/W05_neural_networks/neural_networks.html#train-and-evaluate",
    "title": "Neural Networks",
    "section": "Train and Evaluate",
    "text": "Train and Evaluate\ncnn.compile(\\\"adam\\\", \\\"categorical_crossentropy\\\", metrics=['accuracy'])\nhistory_cnn = cnn.fit(X_train_images, y_train,\n                      batch_size=128, epochs=20, verbose=1, validation_split=.1)\ncnn.evaluate(X_test_images, y_test)\n 9952/10000 [============================&gt;.] - ETA: 0s\n [0.089020583277629253, 0.98429999999999995]"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#visualize-filters",
    "href": "sessions/W05_neural_networks/neural_networks.html#visualize-filters",
    "title": "Neural Networks",
    "section": "Visualize Filters",
    "text": "Visualize Filters\nweights, biases = cnn_small.layers[0].get_weights()\nweights2, biases2 = cnn_small.layers[2].get_weights()\nprint(weights.shape)\nprint(weights2.shape)\n(3,3,1,8)\n(3,3,8,8)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#learned-features",
    "href": "sessions/W05_neural_networks/neural_networks.html#learned-features",
    "title": "Neural Networks",
    "section": "Learned Features",
    "text": "Learned Features"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#convnets-vs-fully-connected-nets-illustrated",
    "href": "sessions/W05_neural_networks/neural_networks.html#convnets-vs-fully-connected-nets-illustrated",
    "title": "Neural Networks",
    "section": "Convnets vs Fully Connected Nets Illustrated",
    "text": "Convnets vs Fully Connected Nets Illustrated"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#mnist-and-permuted-mnist",
    "href": "sessions/W05_neural_networks/neural_networks.html#mnist-and-permuted-mnist",
    "title": "Neural Networks",
    "section": "MNIST and Permuted MNIST",
    "text": "MNIST and Permuted MNIST\n\n\n\nrng = np.random.RandomState(42)\nperm = rng.permutation(784)\nX_train_perm = X_train.reshape(-1, 784)[:, perm].reshape(-1, 28, 28)\nX_test_perm = X_test.reshape(-1, 784)[:, perm].reshape(-1, 28, 28)"
  },
  {
    "objectID": "sessions/W05_neural_networks/neural_networks.html#questions",
    "href": "sessions/W05_neural_networks/neural_networks.html#questions",
    "title": "Neural Networks",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#types-of-explanations",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#types-of-explanations",
    "title": "Model Interpretation and Feature Selection",
    "section": "Types of Explanations",
    "text": "Types of Explanations"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#explain-model-globally",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#explain-model-globally",
    "title": "Model Interpretation and Feature Selection",
    "section": "Explain model globally",
    "text": "Explain model globally\n\nHow does the output depend on the input?\nOften: some form of marginals (e.g., feature importance)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#explain-model-locally",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#explain-model-locally",
    "title": "Model Interpretation and Feature Selection",
    "section": "Explain model locally",
    "text": "Explain model locally\n\nWhy did it classify this point this way?\nExplanation could look like a “global” one but be different for each point\n“What is the minimum change to classify it differently?”"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#methods",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#methods",
    "title": "Model Interpretation and Feature Selection",
    "section": "Methods",
    "text": "Methods\n\n\nGlobal: - Coefficients / feature importances - Drop-feature importance - Permutation importance - Partial dependence plots\n\nLocal: - LIME - SHAP values"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#explaining-the-model-explaining-the-data",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#explaining-the-model-explaining-the-data",
    "title": "Model Interpretation and Feature Selection",
    "section": "Explaining the Model != Explaining the Data",
    "text": "Explaining the Model != Explaining the Data\n\nModel inspection only tells you about the model\nThe model might not accurately reflect the data\nDon’t explain a model with low accuracy"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#features-important-to-the-model",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#features-important-to-the-model",
    "title": "Model Interpretation and Feature Selection",
    "section": "“Features Important to the Model”?",
    "text": "“Features Important to the Model”?\n\nNaive: coef_ for linear models (abs value or norm for multi-class)\nfeature_importances_ for tree-based models\n\nUse with care!"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#linear-model-coefficients",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#linear-model-coefficients",
    "title": "Model Interpretation and Feature Selection",
    "section": "Linear Model Coefficients",
    "text": "Linear Model Coefficients\n\nLarge absolute coef value != important feature\nRelative importance only meaningful after scaling\nCorrelation among features might make coefficients completely uninterpretable\nL1 regularization will pick one at random from a correlated group\nAny penalty will invalidate usual interpretation of linear coefficients"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#example-correlated-features",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#example-correlated-features",
    "title": "Model Interpretation and Feature Selection",
    "section": "Example: Correlated Features",
    "text": "Example: Correlated Features\n\nCorrelation among features might make coefficients completely uninterpretable\n\n\n\nCorrelations (should be &lt; 0.1):\n[[ 1.         -0.12070741 -0.04863585]\n [-0.12070741  1.         -0.12376394]\n [-0.04863585 -0.12376394  1.        ]]\n\nModel 1 (3 features): y = 1.25 + 28.32*X1 + 73.99*X2 + 18.80*X3\nR² = 0.985\n\nCorrelation X3-X4: 0.994\n\nModel 2 (4 features): y = 0.99 + 28.28*X1 + 73.87*X2 + 35.28*X3 + -18.30*X4\nR² = 0.986"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#example-linear-regression-with-penalisation",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#example-linear-regression-with-penalisation",
    "title": "Model Interpretation and Feature Selection",
    "section": "Example: linear regression with penalisation",
    "text": "Example: linear regression with penalisation\n\nLasso regression performs feature selection by driving some coefficients to zero\n\\(\\text{Lasso Regression: } \\min_{\\beta} \\left( \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - X_i \\beta)^2 + \\lambda \\|\\beta\\|_1 \\right)\\)\nOn the same data, Lasso regression will pick one of the correlated features (X3 or X4)\nbut it doesn’t mean the dropped feature is unimportant!\n\n\n\nY = 5.94 + 23.15*X1 + 81.83*X2 + 18.22*X3"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#feature_importances_-in-sklearn-tree-based-models",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#feature_importances_-in-sklearn-tree-based-models",
    "title": "Model Interpretation and Feature Selection",
    "section": "feature_importances_ in sklearn tree-based Models",
    "text": "feature_importances_ in sklearn tree-based Models\n\nFor a tree: FI = total reduction of the criterion (e.g., Gini, MSE) brought by that feature\nFor a forest: average FI over all trees\nProblems:\n\nBiased towards high-cardinality features (cardinality = number of unique values)\nCan be misleading when features are correlated"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#drop-feature-importance-not-recommended",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#drop-feature-importance-not-recommended",
    "title": "Model Interpretation and Feature Selection",
    "section": "Drop Feature Importance (Not recommended)",
    "text": "Drop Feature Importance (Not recommended)\n\\[I_i^\\text{drop} = \\text{Acc}(f, X, y) - \\text{Acc}(f', X_{-i}, y)\\]\n\n\nIt refits a new model for each feature removal\nDoesn’t really explain model (refits for each feature)\nCan’t deal with correlated features well\nBe cautious!"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#permutation-importance",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#permutation-importance",
    "title": "Model Interpretation and Feature Selection",
    "section": "Permutation Importance",
    "text": "Permutation Importance\nIdea: measure marginal influence of one feature by permuting it\n\\[I_i^\\text{perm} = \\text{Acc}(f, X, y) - \\mathbb{E}_{x_i}\\left[\\text{Acc}(f(x_i, X_{-i}), y)\\right]\\]\ndef permutation_importance(est, X, y, n_repeat=100):\n  baseline_score = estimator.score(X, y)\n  for f_idx in range(X.shape[1]):\n      for repeat in range(n_repeat):\n          X_new = X.copy()\n          X_new[:, f_idx] = np.random.shuffle(X[:, f_idx])\n          feature_score = estimator.score(X_new, y)\n          scores[f_idx, repeat] = baseline_score - feature_score\n\nStay with the same trained model\nApplied on validation set given trained estimator\nCan deal with correlated features better\nCan run slow (n_features * n_repeats model evaluations)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#lime",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#lime",
    "title": "Model Interpretation and Feature Selection",
    "section": "LIME",
    "text": "LIME\n\nBuild sparse linear local model around each data point\nExplain prediction for each point locally\nPaper: “Why Should I Trust You?” Explaining the Predictions of Any Classifier\nImplementation: ELI5, https://github.com/marcotcr/lime"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shap",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shap",
    "title": "Model Interpretation and Feature Selection",
    "section": "SHAP",
    "text": "SHAP\n\nBuild around idea of Shapley values (from game theory)\nVery roughly: does drop-out importance for every subset of features\nIntractable, sampling approximations exists\nFast implementation for linear and tree-based models\nAlso work for deep learning models (DeepSHAP)\nAwesome vis and tools: https://github.com/slundberg/shap\nAllows local / per sample explanations, and global explanations (by averaging)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shapley-value-game-theory",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shapley-value-game-theory",
    "title": "Model Interpretation and Feature Selection",
    "section": "Shapley value (game theory)",
    "text": "Shapley value (game theory)\n\nA fair way to distribute “payout” among players N={1, …, p}\nDefine characteristic function v(S) giving payout for any subset of players S ⊆ N\nShapley value for player i: \\[\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (p - |S| - 1)!}{p!} [v(S \\cup \\{i\\}) - v(S)]\\]"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shapley-value-game-theory-1",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shapley-value-game-theory-1",
    "title": "Model Interpretation and Feature Selection",
    "section": "Shapley value (game theory)",
    "text": "Shapley value (game theory)\n\nAssume 4 players, to compute Shapley value for player 1:\n\nConsider all subsets of players not including player 1: {}, {2}, {3}, {4}, {2,3}, {2,4}, {3,4}\nFor each subset S, compute marginal contribution of player 1: v(S ∪ {1}) - v(S)\nWeight each marginal contribution by the number of ways to arrange players in S and N  S  {1}\nSum weighted contributions to get φ₁(v)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shapley-value-game-theory-2",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shapley-value-game-theory-2",
    "title": "Model Interpretation and Feature Selection",
    "section": "Shapley value (game theory)",
    "text": "Shapley value (game theory)\n\nShapley value is the only attribution method that satisfies following properties\nEfficiency: sum of attributions = difference between actual output and average output\nSymmetry: if two features contribute equally, they get same attribution\nDummy: if a feature does not affect the output, its attribution is zero\nAdditivity: for two models, attributions add up. (Think about a random forest with many trees, the Shapley values of the forest is the avereage of SHAP values of each tree)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shap-shapley-values-applied-to-ml-model-explanations",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shap-shapley-values-applied-to-ml-model-explanations",
    "title": "Model Interpretation and Feature Selection",
    "section": "SHAP (Shapley values applied to ML model explanations)",
    "text": "SHAP (Shapley values applied to ML model explanations)\n\nSHAP is an application of Shapley values to explain ML model predictions\nTreat features as players in a game\nmodel prediction over average as the payout\nDefine v(S) that uses dataset mean for missing featuers\nSHAP value for feature i:\n\n\\(f(x)-E[f(X)]=\\sum_{j=1}^{p}\\phi_j(x)\\)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#three-methods-for-shap-estimation",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#three-methods-for-shap-estimation",
    "title": "Model Interpretation and Feature Selection",
    "section": "Three methods for SHAP estimation",
    "text": "Three methods for SHAP estimation\n\nKernelSHAP: model-agnostic, uses sampling to estimate SHAP values\nTreeSHAP: efficient exact computation for tree-based models\nPermutation SHAP: approximate method using permutations, similar to permutation importance"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#case-study",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#case-study",
    "title": "Model Interpretation and Feature Selection",
    "section": "Case Study",
    "text": "Case Study"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#toy-data",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#toy-data",
    "title": "Model Interpretation and Feature Selection",
    "section": "Toy Data",
    "text": "Toy Data\nX.shape\n(100000, 8)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#models-on-lots-of-data",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#models-on-lots-of-data",
    "title": "Model Interpretation and Feature Selection",
    "section": "Models on Lots of Data",
    "text": "Models on Lots of Data\nlasso = LassoCV().fit(X_train, y_train)\nlasso.score(X_test, y_test)\n0.545\nridge = RidgeCV().fit(X_train, y_train)\nridge.score(X_test, y_test)\n0.545\nlr = LinearRegression().fit(X_train, y_train)\nlr.score(X_test, y_test)\n0.545\nparam_grid = {'max_leaf_nodes': range(5, 40, 5)}\ngrid = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=10, n_jobs=3)\ngrid.fit(X_train, y_train)\ngrid.score(X_test, y_test)\n0.545\nrf = RandomForestRegressor(min_samples_leaf=5).fit(X_train, y_train)\nrf.score(X_test, y_test)\n0.542"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#coefficients-and-default-feature-importance",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#coefficients-and-default-feature-importance",
    "title": "Model Interpretation and Feature Selection",
    "section": "Coefficients and Default Feature Importance",
    "text": "Coefficients and Default Feature Importance"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#permutation-importances",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#permutation-importances",
    "title": "Model Interpretation and Feature Selection",
    "section": "Permutation Importances",
    "text": "Permutation Importances"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shap-values",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#shap-values",
    "title": "Model Interpretation and Feature Selection",
    "section": "SHAP Values",
    "text": "SHAP Values"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#more-model-inspection",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#more-model-inspection",
    "title": "Model Interpretation and Feature Selection",
    "section": "More Model Inspection",
    "text": "More Model Inspection"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#partial-dependence-plots",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#partial-dependence-plots",
    "title": "Model Interpretation and Feature Selection",
    "section": "Partial Dependence Plots",
    "text": "Partial Dependence Plots\n\nMarginal dependence of prediction on one (or two features)\n\n\\[f_i^{\\text{pdp}}(x_i) = \\mathbb{E}_{X_{-i}}\\left[f(x_i, x_{-i})\\right]\\]\n\nIdea: Get marginal predictions given feature\nHow? “integrate out” other features using validation data\nFast methods available for tree-based models (doesn’t require validation data)\nNonsensical for linear models"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#partial-dependence",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#partial-dependence",
    "title": "Model Interpretation and Feature Selection",
    "section": "Partial Dependence",
    "text": "Partial Dependence\nfrom sklearn.inspection import plot_partial_dependence\nboston = load_boston()\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,random_state=0)\n\ngbrt = GradientBoostingRegressor().fit(X_train, y_train)\n\nfig, axs = plot_partial_dependence(gbrt, X_train, np.argsort(gbrt.feature_importances_)[-6:], feature_names=boston.feature_names)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#bivariate-partial-dependence-plots",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#bivariate-partial-dependence-plots",
    "title": "Model Interpretation and Feature Selection",
    "section": "Bivariate Partial Dependence Plots",
    "text": "Bivariate Partial Dependence Plots\nplot_partial_dependence(\n    gbrt, X_train, [np.argsort(gbrt.feature_importances_)[-2:]],\n    feature_names=boston.feature_names, n_jobs=3, grid_resolution=50)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#partial-dependence-for-classification",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#partial-dependence-for-classification",
    "title": "Model Interpretation and Feature Selection",
    "section": "Partial Dependence for Classification",
    "text": "Partial Dependence for Classification\nfrom sklearn.inspection import plot_partial_dependence\nfor i in range(3):\n    fig, axs = plot_partial_dependence(gbrt, X_train, range(4), n_cols=4,\n                                       feature_names=iris.feature_names, grid_resolution=50, label=i)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#pdp-caveats",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#pdp-caveats",
    "title": "Model Interpretation and Feature Selection",
    "section": "PDP Caveats",
    "text": "PDP Caveats"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#ice-box",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#ice-box",
    "title": "Model Interpretation and Feature Selection",
    "section": "Ice Box",
    "text": "Ice Box\n\nLike partial dependence plots, without the .mean(axis=0)\n\n\n\n\n\nhttps://pdpbox.readthedocs.io/en/latest/\nhttps://github.com/AustinRochford/PyCEbox\nhttps://github.com/scikit-learn/scikit-learn/pull/16164"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#why-select-features",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#why-select-features",
    "title": "Model Interpretation and Feature Selection",
    "section": "Why Select Features?",
    "text": "Why Select Features?\n\nAvoid overfitting\nFaster prediction and training\nLess storage cost for model and dataset\nMore interpretable model"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#types-of-feature-selection",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#types-of-feature-selection",
    "title": "Model Interpretation and Feature Selection",
    "section": "Types of Feature Selection",
    "text": "Types of Feature Selection\n\nUnsupervised vs Supervised\nUnivariate vs Multivariate\nModel-based or not"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#unsupervised-feature-selection",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#unsupervised-feature-selection",
    "title": "Model Interpretation and Feature Selection",
    "section": "Unsupervised Feature Selection",
    "text": "Unsupervised Feature Selection\n\nMay discard important information\nVariance-based: 0 variance or mostly constant\nCovariance-based: remove correlated features\nPCA: remove linear subspaces"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#covariance",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#covariance",
    "title": "Model Interpretation and Feature Selection",
    "section": "Covariance",
    "text": "Covariance\nfrom sklearn.preprocessing import scale\n\nboston = load_boston()\nX, y = boston.data, boston.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nX_train_scaled = scale(X_train)\n\ncov = np.cov(X_train_scaled, rowvar=False)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#supervised-feature-selection",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#supervised-feature-selection",
    "title": "Model Interpretation and Feature Selection",
    "section": "Supervised Feature Selection",
    "text": "Supervised Feature Selection"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#univariate-statistics",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#univariate-statistics",
    "title": "Model Interpretation and Feature Selection",
    "section": "Univariate Statistics",
    "text": "Univariate Statistics\n\nPick statistic, check p-values\nf_regression, f_classsif, chi2 in scikit-learn\n\nfrom sklearn.feature_selection import f_regression\nf_values, p_values = f_regression(X, y)"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#selectkbest-example",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#selectkbest-example",
    "title": "Model Interpretation and Feature Selection",
    "section": "SelectKBest Example",
    "text": "SelectKBest Example\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, SelectFpr\nfrom sklearn.linear_model import RidgeCV\n\nselect = SelectKBest(k=2, score_func=f_regression)\nselect.fit(X_train, y_train)\nprint(X_train.shape)\nprint(select.transform(X_train).shape)\n(379, 13)\n(379, 2)\nall_features = make_pipeline(StandardScaler(), RidgeCV())\nnp.mean(cross_val_score(all_features, X_train, y_train, cv=10))\n0.718\nselect_2 = make_pipeline(StandardScaler(),\n                         SelectKBest(k=2, score_func=f_regression), RidgeCV())\nnp.mean(cross_val_score(select_2, X_train, y_train, cv=10))\n0.624"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#mutual-information",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#mutual-information",
    "title": "Model Interpretation and Feature Selection",
    "section": "Mutual Information",
    "text": "Mutual Information\nfrom sklearn.feature_selection import mutual_info_regression\nscores = mutual_info_regression(X_train, y_train,\n                                discrete_features=[3])"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#model-based-feature-selection",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#model-based-feature-selection",
    "title": "Model Interpretation and Feature Selection",
    "section": "Model-Based Feature Selection",
    "text": "Model-Based Feature Selection\n\nGet best fit for a particular model\nIdeally: exhaustive search over all possible combinations\nBut, exhaustive is infeasible (and has multiple testing issues)\nUse heuristics in practice"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#model-based-single-fit",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#model-based-single-fit",
    "title": "Model Interpretation and Feature Selection",
    "section": "Model Based (Single Fit)",
    "text": "Model Based (Single Fit)\n\nBuild a model, select “features important to model”\nUsually using Lasso or random forest\nMultivariate - linear models assume linear relation\n\nfrom sklearn.linear_model import LassoCV\nX_train_scaled = scale(X_train)\nlasso = LassoCV().fit(X_train_scaled, y_train)\nprint(lasso.coef_)\n[-0.881 0.951 -0.082 0.59 -1.69 2.639 -0.146 -2.796 1.695 -1.614 -2.133 0.729 -3.615]"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#changing-lasso-alpha",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#changing-lasso-alpha",
    "title": "Model Interpretation and Feature Selection",
    "section": "Changing Lasso Alpha",
    "text": "Changing Lasso Alpha\nfrom sklearn.linear_model import Lasso\nX_train_scaled = scale(X_train)\nlasso = Lasso().fit(X_train_scaled, y_train)\nprint(lasso.coef_)\n[-0. 0. -0. 0. -0. 2.529 -0. -0. -0. -0.228 -1.701 0.132 -3.606]"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#selectfrommodel",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#selectfrommodel",
    "title": "Model Interpretation and Feature Selection",
    "section": "SelectFromModel",
    "text": "SelectFromModel\nfrom sklearn.feature_selection import SelectFromModel\nselect_lassocv = SelectFromModel(LassoCV(), threshold=1e-5)\nselect_lassocv.fit(X_train, y_train)\nprint(select_lassocv.transform(X_train).shape)\n(379,11)\npipe_lassocv = make_pipeline(StandardScaler(), select_lassocv, RidgeCV())\nnp.mean(cross_val_score(pipe_lassocv, X_train, y_train, cv=10))\nnp.mean(cross_val_score(all_features, X_train, y_train, cv=10))\n0.717\n0.718\n# could grid-search alpha in lasso\nselect_lasso = SelectFromModel(Lasso())\npipe_lasso = make_pipeline(StandardScaler(), select_lasso, RidgeCV())\nnp.mean(cross_val_score(pipe_lasso, X_train, y_train, cv=10))\n0.671"
  },
  {
    "objectID": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#questions",
    "href": "sessions/W07_model_interpretation_feature_selection/model_interpretation_feature_selection.html#questions",
    "title": "Model Interpretation and Feature Selection",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#recap-on-imbalanced-data",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#recap-on-imbalanced-data",
    "title": "Imbalanced Data",
    "section": "Recap on Imbalanced Data",
    "text": "Recap on Imbalanced Data\n\nClassification often has asymmetric costs or data imbalance\nNeed metrics and models that respect imbalance"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#two-sources-of-imbalance",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#two-sources-of-imbalance",
    "title": "Imbalanced Data",
    "section": "Two Sources of Imbalance",
    "text": "Two Sources of Imbalance\n\nAsymmetric cost between errors\nAsymmetric data prevalence"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#why-do-we-care",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#why-do-we-care",
    "title": "Imbalanced Data",
    "section": "Why Do We Care?",
    "text": "Why Do We Care?\n\nReal-world costs rarely symmetric\nData often heavily imbalanced; rare event detection common"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#methods",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#methods",
    "title": "Imbalanced Data",
    "section": "Methods",
    "text": "Methods\n\nAdjust evaluation metrics (What do you want to optimise?)\nChange decision thresholds\nChange class-weights\nRessampling data"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#adjust-evaluation-metrics",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#adjust-evaluation-metrics",
    "title": "Imbalanced Data",
    "section": "Adjust evaluation metrics",
    "text": "Adjust evaluation metrics\n\nAccuracy paradox for imbalanced data\nUse precision or recall\nIf a method does not optimise your metric directly, use the metric in cross-validation"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#changing-thresholds",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#changing-thresholds",
    "title": "Imbalanced Data",
    "section": "Changing Thresholds",
    "text": "Changing Thresholds\nDon't think this example is good. Change it\n\nAdjust probability threshold to trade precision/recall\n\ny_pred = lr.predict_proba(X_test)[:, 1] &gt; 0.85\nclassification_report(y_test, y_pred)\n\nChoose threshold to minimise given cost; can tune threshold using cross-validation"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#roc-curve",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#roc-curve",
    "title": "Imbalanced Data",
    "section": "ROC Curve",
    "text": "ROC Curve\n\n\n\n\nEvaluates all thresholds via TPR vs FPR"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#remedies-for-the-model",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#remedies-for-the-model",
    "title": "Imbalanced Data",
    "section": "Remedies for the Model",
    "text": "Remedies for the Model\n\nBeyond thresholding: modify data or training to address imbalance"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#mammography-data",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#mammography-data",
    "title": "Imbalanced Data",
    "section": "Mammography Data",
    "text": "Mammography Data\n\n\ndata = fetch_openml(\"mammography\", as_frame=True)\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y == \"1\", random_state=0)\n\n\n\n\n\n\nImbalanced dataset: 260 positive of 11183 samples"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#mammography-baselines",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#mammography-baselines",
    "title": "Imbalanced Data",
    "section": "Mammography Baselines",
    "text": "Mammography Baselines\n\nLogisticRegression CV=10: ROC AUC 0.920, AP 0.630\nRandomForest CV=10: ROC AUC 0.939, AP 0.722"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#basic-approaches",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#basic-approaches",
    "title": "Imbalanced Data",
    "section": "Basic Approaches",
    "text": "Basic Approaches\n\n\n\n\n\n\n\nChange the training procedure\nModify data via sampling"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#scikit-learn-vs-resampling",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#scikit-learn-vs-resampling",
    "title": "Imbalanced Data",
    "section": "Scikit-learn vs Resampling",
    "text": "Scikit-learn vs Resampling\n\n\n\n\nStandard pipelines transform X only; cannot resample y without extensions"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#imbalance-learn",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#imbalance-learn",
    "title": "Imbalanced Data",
    "section": "Imbalance-Learn",
    "text": "Imbalance-Learn\n\nLibrary: http://imbalanced-learn.org\npip install -U imbalanced-learn\nExtends sklearn API with samplers and pipelines"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#sampler-api",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#sampler-api",
    "title": "Imbalanced Data",
    "section": "Sampler API",
    "text": "Sampler API\n\ndata_resampled, targets_resampled = sampler.sample(X, y)\nfit_sample convenience to fit and sample\nIn pipelines, sampling only occurs during fit\nMany samplers are binary-only; check multiclass support"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#random-undersampling",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#random-undersampling",
    "title": "Imbalanced Data",
    "section": "Random Undersampling",
    "text": "Random Undersampling\n\nDrop majority samples until balanced\nVery fast; dataset shrinks to ~2x minority\nLoses data but can still perform well\n\nrus = RandomUnderSampler(replacement=False)\nX_sub, y_sub = rus.fit_sample(X_train, y_train)"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#random-undersampling-results",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#random-undersampling-results",
    "title": "Imbalanced Data",
    "section": "Random Undersampling Results",
    "text": "Random Undersampling Results\n\nLogisticRegression: ROC AUC 0.927, AP 0.527 (baseline 0.920, 0.630)\nRandomForest: ROC AUC 0.951, AP 0.629 (baseline 0.939, 0.722)\nOften as accurate with fraction of data; great for large datasets"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#random-oversampling",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#random-oversampling",
    "title": "Imbalanced Data",
    "section": "Random Oversampling",
    "text": "Random Oversampling\n\nRepeat minority samples until balanced\nDataset grows; slower training\n\nros = RandomOverSampler()\nX_over, y_over = ros.fit_sample(X_train, y_train)"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#random-oversampling-results",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#random-oversampling-results",
    "title": "Imbalanced Data",
    "section": "Random Oversampling Results",
    "text": "Random Oversampling Results\n\nLogisticRegression: ROC AUC 0.917, AP 0.585\nRandomForest: ROC AUC 0.926, AP 0.715\nPerformance similar to baseline; heavier compute"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#curves-for-logreg",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#curves-for-logreg",
    "title": "Imbalanced Data",
    "section": "Curves for LogReg",
    "text": "Curves for LogReg"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#curves-for-random-forest",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#curves-for-random-forest",
    "title": "Imbalanced Data",
    "section": "Curves for Random Forest",
    "text": "Curves for Random Forest"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#class-weights",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#class-weights",
    "title": "Imbalanced Data",
    "section": "Class-Weights",
    "text": "Class-Weights\n\nReweight loss instead of resampling\nSame effect as oversampling without data duplication\nSupported by most models"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#class-weights-in-linear-models",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#class-weights-in-linear-models",
    "title": "Imbalanced Data",
    "section": "Class-Weights in Linear Models",
    "text": "Class-Weights in Linear Models\n\nModify loss with per-class weight \\(c_{y_i}\\)\nEquivalent to repeating samples by class weight count"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#class-weights-in-trees",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#class-weights-in-trees",
    "title": "Imbalanced Data",
    "section": "Class-Weights in Trees",
    "text": "Class-Weights in Trees\n\nApply class weights in impurity (Gini or entropy)\nUse weighted votes for prediction"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#using-class-weights",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#using-class-weights",
    "title": "Imbalanced Data",
    "section": "Using Class-Weights",
    "text": "Using Class-Weights\n\nLogisticRegression(class_weight=“balanced”): ROC AUC 0.918, AP 0.587\nRandomForest(class_weight=“balanced”): ROC AUC 0.917, AP 0.701"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#ensemble-resampling",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#ensemble-resampling",
    "title": "Imbalanced Data",
    "section": "Ensemble Resampling",
    "text": "Ensemble Resampling\n\nRandom resampling separately per estimator in ensemble\nExample: Balanced bagging or balanced random forest\nEasy with imblearn; not yet in sklearn core"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#easy-ensemble-with-imblearn",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#easy-ensemble-with-imblearn",
    "title": "Imbalanced Data",
    "section": "Easy Ensemble with imblearn",
    "text": "Easy Ensemble with imblearn\n\n\nfrom imblearn.ensemble import BalancedBaggingClassifier\nbase = DecisionTreeClassifier(max_features=\"auto\")\nresampled_rf = BalancedBaggingClassifier(base_estimator=base, random_state=0)\n\n\nTrains each tree on a different undersampled dataset\nROC AUC 0.957, AP 0.654 (baseline RF 0.939, 0.722)\n\n\n\nAs cheap as undersampling; strong results"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#roc-vs-pr-comparison",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#roc-vs-pr-comparison",
    "title": "Imbalanced Data",
    "section": "ROC vs PR Comparison",
    "text": "ROC vs PR Comparison\n\n\n\n\nEasy ensemble performs well at higher recall and precision regions"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#synthetic-sample-generation",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#synthetic-sample-generation",
    "title": "Imbalanced Data",
    "section": "Synthetic Sample Generation",
    "text": "Synthetic Sample Generation\n\nSMOTE: Synthetic Minority Oversampling Technique\nInterview-friendly method; many variants exist"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#smote",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#smote",
    "title": "Imbalanced Data",
    "section": "SMOTE",
    "text": "SMOTE\n\nAdd synthetic points for minority class\nFor each minority sample: pick random neighbor, interpolate on line segment\nLeads to larger datasets; can combine with undersampling"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#smote-illustration",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#smote-illustration",
    "title": "Imbalanced Data",
    "section": "SMOTE Illustration",
    "text": "SMOTE Illustration"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#smote-results",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#smote-results",
    "title": "Imbalanced Data",
    "section": "SMOTE Results",
    "text": "SMOTE Results\n\nLogisticRegression with SMOTE: ROC AUC 0.919, AP 0.585\nRandomForest with SMOTE: ROC AUC 0.946, AP 0.688\nSimilar to baseline; tune k_neighbors for best AP"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#smote-tuning",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#smote-tuning",
    "title": "Imbalanced Data",
    "section": "SMOTE Tuning",
    "text": "SMOTE Tuning\n\n\n\n\nGridSearch over smote__k_neighbors; moderate impact on metrics"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#smote-curves",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#smote-curves",
    "title": "Imbalanced Data",
    "section": "SMOTE Curves",
    "text": "SMOTE Curves"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#roc-vs-pr-with-smote",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#roc-vs-pr-with-smote",
    "title": "Imbalanced Data",
    "section": "ROC vs PR with SMOTE",
    "text": "ROC vs PR with SMOTE"
  },
  {
    "objectID": "sessions/W08_imbalanced_data/imbalanced_data.html#summary",
    "href": "sessions/W08_imbalanced_data/imbalanced_data.html#summary",
    "title": "Imbalanced Data",
    "section": "Summary",
    "text": "Summary\n\nInspect both ROC AUC and average precision; review curves\nUndersampling is fast and can help\nUndersampling plus ensembles is powerful\nSMOTE adds synthetic samples; results vary by metric"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#intro-to-code-testing",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#intro-to-code-testing",
    "title": "Testing ML Systems",
    "section": "Intro to code testing",
    "text": "Intro to code testing\n\nThis is different from model testing in ML or in train-test split\nCode testing aims to ensure code correctness, reliability, and maintainability\nCatching code bugs early saves time and effort\nSome bugs are nuanced and hard to spot without tests\nSome systems can run to completion without throwing errors, but produce invalid results"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#targest-of-code-testing",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#targest-of-code-testing",
    "title": "Testing ML Systems",
    "section": "Targest of code testing",
    "text": "Targest of code testing\n\nNot polluting main codebase; tests are in separate files\nAutomated testing; can be integrated into CI/CD pipelines\n100% Coverage of different code and edge cases"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#types-of-tests",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#types-of-tests",
    "title": "Testing ML Systems",
    "section": "Types of tests",
    "text": "Types of tests\n\nUnit tests: test individual functions or classes in isolation (eg, function that filters a list)\nIntegration tests: tests on the combined functionality of individual components (eg, data processing).\nSystem tests: tests on the design of a system for expected outputs given inputs (ex. training, inference, etc.).\nAcceptance tests: tests to verify that requirements have been met, usually referred to as User Acceptance Testing (UAT)."
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#type-of-tests",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#type-of-tests",
    "title": "Testing ML Systems",
    "section": "Type of tests",
    "text": "Type of tests\n\n\n\nImage Credit: https://madewithml.com/courses/mlops/testing/"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#other-types-under-the-system-testing-umbrella",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#other-types-under-the-system-testing-umbrella",
    "title": "Testing ML Systems",
    "section": "Other types (under the system testing umbrella)",
    "text": "Other types (under the system testing umbrella)\n\nRegression tests: ensure that new code changes do not break existing functionality.\nSmoke tests: basic tests to check if the main functionalities of a program work.\nPerformance tests: tests to evaluate the speed, responsiveness, and stability of a program under load.\nSecurity tests: tests to identify vulnerabilities and ensure data protection."
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#testing-tools-in-python",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#testing-tools-in-python",
    "title": "Testing ML Systems",
    "section": "Testing tools in Python",
    "text": "Testing tools in Python\n\nDon’t rely on print() statements for testing\nUse testing tools like unittest, pytest, or nose2\nThese tools provide built-in functionality including parameterisation, filters, etc.\nThese tools can be integrated into CI/CD pipelines for automated testing\nThese tools don’t pollute the main codebase; tests are in separate files"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#how-should-we-test---arrange-act-assert-aaa-methodology",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#how-should-we-test---arrange-act-assert-aaa-methodology",
    "title": "Testing ML Systems",
    "section": "How should we test - Arrange Act Assert (AAA) Methodology",
    "text": "How should we test - Arrange Act Assert (AAA) Methodology\n\nArrange: set up the test data and environment\nAct: execute the code being tested\nAssert: verify that the output is as expected"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#using-pytest",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#using-pytest",
    "title": "Testing ML Systems",
    "section": "Using pytest",
    "text": "Using pytest\n\nBy default, pytest expects tests to be organised under a tests directory, with test files named test_*.py\nEach test function should be named test_*\n\ntests/\n├── code/\n│   ├── conftest.py\n│   ├── test_data.py\n│   ├── test_predict.py\n│   ├── test_train.py\n│   ├── test_tune.py\n│   ├── test_utils.py\n│   └── utils.py\n├── data/\n│   ├── conftest.py\n│   └── test_dataset.py\n└── models/\n│   ├── conftest.py\n│   └── test_behavioral.py"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#example",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#example",
    "title": "Testing ML Systems",
    "section": "example",
    "text": "example\n# predict.py\ndef decode(indices: Iterable[Any], index_to_class: Dict) -&gt; List:\n    return [index_to_class[index] for index in indices]\n# tests/code/test_predict.py \n# tests/code/test_predict.py\ndef test_decode():\ndecoded = predict.decode(\n    indices=[0, 1, 1], # arrange\n    index_to_class={0: \"x\", 1: \"y\"}) ## act\nassert decoded == [\"x\", \"y\", \"y\"] ## assert"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#what-is-assert-in-python",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#what-is-assert-in-python",
    "title": "Testing ML Systems",
    "section": "what is assert in Python?",
    "text": "what is assert in Python?\n\nassert is a keyword in Python used for debugging purposes\nIt tests if a condition is true; if not, it raises an AssertionError"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#three-key-features-of-pytest",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#three-key-features-of-pytest",
    "title": "Testing ML Systems",
    "section": "Three key features of pytest",
    "text": "Three key features of pytest\n\nParameterize\nFixtures\nMarkers\nWhy? Aim to reduce redundancy and to automate test setup"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#parameterize",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#parameterize",
    "title": "Testing ML Systems",
    "section": "Parameterize",
    "text": "Parameterize\n\nUse @pytest.mark.parametrize to run the same test with different inputs\nHelps reduce code redundancy; similar to loops but better reporting\n\n@pytest.mark.parametrize(\n    \"text, sw, clean_text\",\n    [\n        (\"hi\", [], \"hi\"),\n        (\"hi you\", [\"you\"], \"hi\"),\n        (\"hi yous\", [\"you\"], \"hi yous\"),\n    ],\n)\ndef test_clean_text(text, sw, clean_text):\n    assert data.clean_text(text=text, stopwords=sw) == clean_text"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#fixtures",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#fixtures",
    "title": "Testing ML Systems",
    "section": "Fixtures",
    "text": "Fixtures\n\nUse @pytest.fixture to set up reusable test data or state\nReduce redundancy across different test functions\n\n# tests/code/conftest.py\nimport pytest\nfrom madewithml.data import CustomPreprocessor\n\n@pytest.fixture\ndef dataset_loc():\n    return \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\n\n@pytest.fixture\ndef preprocessor():\n    return CustomPreprocessor()"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#fixtures-cont",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#fixtures-cont",
    "title": "Testing ML Systems",
    "section": "Fixtures (cont)",
    "text": "Fixtures (cont)\n\nSo the fixtures of data_loc and preprocessor can be used in any test file under tests/code/\n\ndef test_fit_transform(dataset_loc, preprocessor):\n    ds = data.load_data(dataset_loc=dataset_loc)\n    preprocessor.fit_transform(ds)\n    assert len(preprocessor.class_to_index) == 4"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#markers",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#markers",
    "title": "Testing ML Systems",
    "section": "Markers",
    "text": "Markers\n\nSometimes we want to run only a subset of tests; pytest provides various levels of granularity\n\npython3 -m pytest                                          # all tests\npython3 -m pytest tests/code                               # tests under a directory\npython3 -m pytest tests/code/test_predict.py               # tests for a single file\npython3 -m pytest tests/code/test_predict.py::test_decode  # tests for a single function"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#markers-cont",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#markers-cont",
    "title": "Testing ML Systems",
    "section": "Markers (cont)",
    "text": "Markers (cont)\n\nMore advacend: can use @pytest.mark.&lt;name&gt; to label tests and create groups\n\n@pytest.mark.training\ndef test_train_model(dataset_loc):\n    pass\n\nThen run tests with specific markers\n\npytest -m \"training\"      #  runs all tests marked with `training`\npytest -m \"not training\"  #  runs all tests besides those marked with `training`"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#purpose",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#purpose",
    "title": "Testing ML Systems",
    "section": "Purpose",
    "text": "Purpose\n\nData from various resources: local file systems, databases, APIs\nNeed to test data validity before using it for training/inference\nGreat expectations library allows to create expectations and to compare with data"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#example-1",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#example-1",
    "title": "Testing ML Systems",
    "section": "Example",
    "text": "Example\n\nCreate a fixture to load data\n\n# tests/data/conftest.py\nimport great_expectations as ge\nimport pandas as pd\nimport pytest\n\n@pytest.fixture(scope=\"module\")\ndef df(request):\n    dataset_loc = request.config.getoption(\"--dataset-loc\")\n    df = ge.dataset.PandasDataset(pd.read_csv(dataset_loc))\n    return df"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#create-expectations",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#create-expectations",
    "title": "Testing ML Systems",
    "section": "Create expectations",
    "text": "Create expectations\n# tests/data/test_dataset.py\ndef test_dataset(df):\n    \"\"\"Test dataset quality and integrity.\"\"\"\n    column_list = [\"id\", \"created_on\", \"title\", \"description\", \"tag\"]\n    df.expect_table_columns_to_match_ordered_list(column_list=column_list)  # schema adherence\n    tags = [\"computer-vision\", \"natural-language-processing\", \"mlops\", \"other\"]\n    df.expect_column_values_to_be_in_set(column=\"tag\", value_set=tags)  # expected labels\n    df.expect_compound_columns_to_be_unique(column_list=[\"title\", \"description\"])  # data leaks\n    df.expect_column_values_to_not_be_null(column=\"tag\")  # missing values\n    df.expect_column_values_to_be_unique(column=\"id\")  # unique values\n    df.expect_column_values_to_be_of_type(column=\"title\", type_=\"str\")  # type adherence\n\n    # Expectation suite\n    expectation_suite = df.get_expectation_suite(discard_failed_expectations=False)\n    results = df.validate(expectation_suite=expectation_suite, only_return_failures=True).to_json_dict()\n    assert results[\"success\"]"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#check-expectations",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#check-expectations",
    "title": "Testing ML Systems",
    "section": "Check expectations",
    "text": "Check expectations\n\nCan run these data tests like a code test\n\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\npytest --dataset-loc=$DATASET_LOC tests/data --verbose --disable-warnings"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#other-expectations",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#other-expectations",
    "title": "Testing ML Systems",
    "section": "Other expectations",
    "text": "Other expectations\n\nThis library provides many built-in expectations, e.g.,\nexpect_column_pair_values_a_to_be_greater_than_b\nexpect_column_mean_to_be_between"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#purpose-1",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#purpose-1",
    "title": "Testing ML Systems",
    "section": "Purpose",
    "text": "Purpose\n\nWant to write tests when we develop training pipelines so we can catch errors quickly\nML systems can run to completion without throwing errors, but produce invalid results\nWe want to catch errors quickly to save on time and compute"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#example-of-testing-a-neural-network",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#example-of-testing-a-neural-network",
    "title": "Testing ML Systems",
    "section": "Example of testing a neural network",
    "text": "Example of testing a neural network\n\nCheck shapes and values of model output\n\nassert model(inputs).shape == torch.Size([len(inputs), num_classes])\n\nCheck overfitting on a batch (the logic is - if the model cannot overfit on a small batch, there is likely a bug)\n\naccuracy = train(model, inputs=batches[0])\nassert accuracy == pytest.approx(0.95, abs=0.05) # 0.95 ± 0.05"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#example-of-testing-a-neural-network-cont",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#example-of-testing-a-neural-network-cont",
    "title": "Testing ML Systems",
    "section": "Example of testing a neural network (cont)",
    "text": "Example of testing a neural network (cont)\n\nTrain to completion (tests early stopping, saving, etc.)\n\ntrain(model)\nassert learning_rate &gt;= min_learning_rate\nassert artifacts\n\nOn different devices\n\nassert train(model, device=torch.device(\"cpu\"))\nassert train(model, device=torch.device(\"cuda\"))"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#key-references",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#key-references",
    "title": "Testing ML Systems",
    "section": "Key references",
    "text": "Key references\n\nTesting Machine Learning Systems: Code, Data and Models"
  },
  {
    "objectID": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#summary",
    "href": "sessions/W10_testing_ML_systems/Testing_ML_systems.html#summary",
    "title": "Testing ML Systems",
    "section": "Summary",
    "text": "Summary\n\nCode testing is essential for ensuring code correctness and reliability\nusing testing frameworks including pytest and great_expectations helps organise and automate tests\nKeep testing in mind during development and all stages of the ML lifecycle"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#what-we-learnt-in-term-1",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#what-we-learnt-in-term-1",
    "title": "Introduction to machine learning",
    "section": "What we learnt in Term 1",
    "text": "What we learnt in Term 1\n\nPython programming\nData types\nVisualisation\nRegression (Ordinary Least Square; Linear Mixed Effects; multicollinearity)\nDimensionality reduction\nClustering"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#learning-objectives",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#learning-objectives",
    "title": "Introduction to machine learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand the basics and classifications of machine learning\nUnderstand the differences between statistical methods and machine learning (estimation vs. prediction)\nAppreciate GIGO theorems in machine learning and data science"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#ml-as-subset-of-ai",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#ml-as-subset-of-ai",
    "title": "Introduction to machine learning",
    "section": "ML as subset of AI",
    "text": "ML as subset of AI\n\n\n\nMachine learning (decision tree, random forest, k-means, etc.)\n\nDeep learning (deep neural networks)\n\nOther AI tools: graphical models, symbolic AI\n\nNote: we don’t distinguish ML/DL and consider NN as part of ML\n\n\n\n\n\nImage Credit: Lecture slide (ML is a subset of AI)"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#definition-of-machine-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#definition-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Definition of machine learning",
    "text": "Definition of machine learning\n\nArthur Samuel (1959): (Machine learning is the) field of study that gives computers the ability to learn without being explicitly programmed.\n\n\nTom Mitchell (1997): A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#what-is-machine-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#what-is-machine-learning",
    "title": "Introduction to machine learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nExtracting knowledge from data\nRelying on data and algorithms\nClosely related to statistics but distinct from linear models\nFocus on prediction rather than estimating relationships or interpretation"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook",
    "text": "Examples: Facebook\n\n\n\n\nMachine learning in news feed ranking\nContent selection and targeting\nAds and recommendations"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nFace detection and recognition\nPhoto organization"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.-1",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-facebook-cont.-1",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nPhoto selection and layout"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-amazon",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-amazon",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon",
    "text": "Examples: Amazon\n\n\n\n\nProduct ranking\nPersonalised recommendations\nAds selection"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-amazon-cont.",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-amazon-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon (cont.)",
    "text": "Examples: Amazon (cont.)\n\n\n\n\nSeller selection\nDefault choices\nRelated products"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#science-applications",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#science-applications",
    "title": "Introduction to machine learning",
    "section": "Science Applications",
    "text": "Science Applications\n\n\n\n\nPersonalised cancer treatment\nMedical diagnosis\nDrug discovery\nHiggs boson discovery\nExoplanet detection"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#types-of-machine-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#types-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nSupervised Learning: Learn from input-output pairs (with labelled data)\nUnsupervised Learning: Discover structure in data (without labelled data)\nReinforcement Learning: Learn through interaction with environment (with an actual/simulated environment)"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#supervised-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in \\mathbb{R} \\\\\nf(x_i) &\\approx y_i\n\\end{aligned}\n\\]\nLearn a function \\(f\\) from input-output pairs to predict on new data."
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#generalisation-to-unseen-data",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#generalisation-to-unseen-data",
    "title": "Introduction to machine learning",
    "section": "Generalisation to unseen data",
    "text": "Generalisation to unseen data\n\nGoal: \\(f(x_i) \\approx y_i\\) on training data\nMore important: \\(f(x) \\approx y\\) on new data\nCore distinction: not just function approximation, but prediction on unseen data\nAvoiding overfitting to training data is key"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#examples-of-supervised-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#examples-of-supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Examples of Supervised Learning",
    "text": "Examples of Supervised Learning\n\nSpam detection\nMedical diagnosis\nAd click prediction"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#unsupervised-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\\[\nx_i \\sim p(x) \\text{ i.i.d.}\n\\]\nLearn about the distribution \\(p\\):\n\nClustering\nDimensionality reduction\nTopic modeling\nOutlier detection"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#other-types-of-learning",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#other-types-of-learning",
    "title": "Introduction to machine learning",
    "section": "Other Types of Learning",
    "text": "Other Types of Learning\n\nSemi-supervised\nActive Learning\nForecasting\nTransfer learning\nIf you understand supervised/unsupervised/reinforcement learning, others are easier to grasp"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#dont-read-a-book-by-its-name",
    "title": "Introduction to machine learning",
    "section": "Don’t read a book by its name",
    "text": "Don’t read a book by its name\n\nClassifications of neighbourhoods (e.g. London output area classification) that are actually clustering\nAnomaly detection methods can be unsupervised or supervised learning\nForecasting can be done with lagged features (via supervised learning), or with time series data (via time series analysis), or both"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#what-does-llm-belong-to",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#what-does-llm-belong-to",
    "title": "Introduction to machine learning",
    "section": "What does LLM belong to?",
    "text": "What does LLM belong to?\n\n\n\nImage Credit: medium.com"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#classification-vs.-regression",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#classification-vs.-regression",
    "title": "Introduction to machine learning",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression\n\n\nClassification\n\nTarget \\(y\\) is discrete\nExample: Is this patient sick?\n\n\nRegression\n\nTarget \\(y\\) is continuous\nExample: How long to recover?"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#relationship-to-statistics",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#relationship-to-statistics",
    "title": "Introduction to machine learning",
    "section": "Relationship to Statistics",
    "text": "Relationship to Statistics\n\n\nStatistics\n\nModel first\nEstimation emphasis\nYes/no questions\nWith many assumptions, need to test\nInterpretation is key\ne.g. Does smoking lead to lung cancer?\n\n\nMachine Learning\n\nData first\nPrediction emphasis\nFuture predictions\nFew assumptions (but not assumption-free)\nInterpretation isn’t primary\ne.g. Predict tomorrow’s weather"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#following-statements-are-not-recommended",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#following-statements-are-not-recommended",
    "title": "Introduction to machine learning",
    "section": "Following statements are not recommended",
    "text": "Following statements are not recommended\n\n“Linear regression is not needed anymore because of machine learning.”\n“Machine learning is just a fad; statistics is more important.”\n“Machine learning models are black boxes; we can’t interpret them at all.”\n“Machine learning models are as interpretable as linear models.”\n“Machine learning models have no assumptions.”"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#guiding-principles-goal-considerations",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#guiding-principles-goal-considerations",
    "title": "Introduction to machine learning",
    "section": "Guiding Principles: Goal Considerations",
    "text": "Guiding Principles: Goal Considerations\n\nDefine the goal clearly\nDefine how to measure success\nThink about context and baseline\nAsk: what’s the benefit?"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#thinking-in-context",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#thinking-in-context",
    "title": "Introduction to machine learning",
    "section": "Thinking in Context",
    "text": "Thinking in Context\n\nWhat do you want to achieve?\nWhat’s the baseline and its performance?\nWhat improvement over baseline do you need?"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#good-and-bad-substitutes",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#good-and-bad-substitutes",
    "title": "Introduction to machine learning",
    "section": "Good and Bad Substitutes",
    "text": "Good and Bad Substitutes\n\nChoose metrics carefully\nSubstitute metrics can be misleading\nOptimize for the right goal\nUnderstand side effects"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#communicating-results",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#communicating-results",
    "title": "Introduction to machine learning",
    "section": "Communicating Results",
    "text": "Communicating Results\n\nExplain why your approach works\nCommunicate uncertainty\nShow impact and limitations\nConvince stakeholders"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#explainable-results",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#explainable-results",
    "title": "Introduction to machine learning",
    "section": "Explainable Results",
    "text": "Explainable Results\n\n\n\n\nUsers want to know why recommendations are made\nExplainability improves engagement\nImportant for trust and transparency"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#ethical-considerations",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#ethical-considerations",
    "title": "Introduction to machine learning",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\n\n\n\nBias in risk assessments\nFairness in automated decisions\nTransparency and accountability"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#ethics-its-in-the-application",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#ethics-its-in-the-application",
    "title": "Introduction to machine learning",
    "section": "Ethics: It’s in the Application!",
    "text": "Ethics: It’s in the Application!\n\nUnderstand biases in your system\nConsider the impact of predictions\nUse algorithms responsibly\nSame algorithm, different outcomes"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#data-and-data-collection",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#data-and-data-collection",
    "title": "Introduction to machine learning",
    "section": "Data and Data Collection",
    "text": "Data and Data Collection\n\nCritical component of ML\nMore data usually helps (if from right source)\nConsider marginal cost vs. marginal benefit"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#free-vs-expensive-data",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#free-vs-expensive-data",
    "title": "Introduction to machine learning",
    "section": "Free vs Expensive Data",
    "text": "Free vs Expensive Data\n\n\nFree Data\n\nOpen data from gov and census, often aggregated (e.g. ONS, London Fire Brigade, NASA)\nOpen data from companies (e.g. Google Street View) (Read the license first)\nSynthetic data\nWeb scraping (be careful with legality and ethics)\n\n\nExpensive Data\n\nIndividual data (e.g. health records)\nMobile phone data (high cost)\nUser survey\nHigh-resolution imagery (e.g. satellite, aerial, drone)"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#big-data-considerations",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#big-data-considerations",
    "title": "Introduction to machine learning",
    "section": "Big Data Considerations",
    "text": "Big Data Considerations\n\nMore data can be more expensive to work with\nSubsample to RAM when possible (512GB available in cloud)\nRuntime and analyst time matter\nAlways try with a small sample first"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#theorem-garbage-in-garbage-out-gigo",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#theorem-garbage-in-garbage-out-gigo",
    "title": "Introduction to machine learning",
    "section": "Theorem: Garbage in, garbage out (GIGO)",
    "text": "Theorem: Garbage in, garbage out (GIGO)\n\nGreat algorithms + bad data = bad results\n\n\n\n\nModel performance is constrained by data quality.\nBiased, noisy, or incomplete data leads to misleading predictions.\n\n\n\n\n\nImage Credit: x.com/xschelling/status/954936528555429888"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#good-data-large-size-high-quality.",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#good-data-large-size-high-quality.",
    "title": "Introduction to machine learning",
    "section": "Good data = large size + high quality.",
    "text": "Good data = large size + high quality.\n\n\n\nSufficient sample size to capture variability in the problem\n\nHigh-quality labels and accurate measurements.\n\nRepresentative of the population and application context.\n\n\n\n\n\nImage Credit: Internet"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#data-size-and-performance",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#data-size-and-performance",
    "title": "Introduction to machine learning",
    "section": "Data size and performance",
    "text": "Data size and performance\n\n\nThe performance of ML/DL increases rapidly with the size of the data:\n\nLarge neural nets benefit the most from big data.\n\nMedium and small neural nets also improve with more data.\n\nTraditional ML algorithms (e.g. random forest, SVM) may saturate earlier.\n\n\n\n\n\nPerformance vs data size for ML/DL models"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#feature-engineering-eda-90-time",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#feature-engineering-eda-90-time",
    "title": "Introduction to machine learning",
    "section": "Feature engineering & EDA (~90% time)",
    "text": "Feature engineering & EDA (~90% time)\n\nMostly manual rather than automated (no automated methods for EDA)\nDomain knowledge from expertise is desirable\nUsing visualisation to identify patterns & data relationship\nRemoving noisy or erroneous data\n\nDealing with missing data\n\nGenerating new features by combining existing ones"
  },
  {
    "objectID": "sessions/intro_machine_learning/intro_machine_learning.html#example-representation-of-geospatial-locations-in-ml-models",
    "href": "sessions/intro_machine_learning/intro_machine_learning.html#example-representation-of-geospatial-locations-in-ml-models",
    "title": "Introduction to machine learning",
    "section": "Example: Representation of geospatial locations in ML models",
    "text": "Example: Representation of geospatial locations in ML models\n\nRaw coordinates (e.g. long/lat) may not be directly useful\nNeed to engineer features that capture spatial relationships\nLong/lat\nDistance to POIs (train stations/schools).\nUsing adjacency matrix between spatial units"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#last-week",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#last-week",
    "title": "Supervised learning workflow",
    "section": "Last week",
    "text": "Last week\n\nFramework of supervised learning\nEvaluation metrics for regression and classification"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#objectives-of-this-week",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#objectives-of-this-week",
    "title": "Supervised learning workflow",
    "section": "Objectives of this week",
    "text": "Objectives of this week\n\nUnderstand different workflow of supervised learning.\nUnderstand train-test split and train-validation-test split.\nUnderstand cross validation and its extensions\nKnow when to use different model evaluation methods."
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#supervised-learning",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#supervised-learning",
    "title": "Supervised learning workflow",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in\n\\begin{cases}\n\\mathbb{R}, & \\text{(regression)} \\\\\n\\mathcal{Y} \\text{ (finite set)}, & \\text{(classification)}\n\\end{cases} \\\\\n\\text{learn } f(x_i) &\\approx y_i \\\\\n\\text{such that } f(x) &\\approx y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#challenges-of-training-supervised-models",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#challenges-of-training-supervised-models",
    "title": "Supervised learning workflow",
    "section": "Challenges of training supervised models",
    "text": "Challenges of training supervised models\n\nGeneralise to new data\nAvoid overfitting/underfitting\nModel selection (hyperparameter tuning)"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-7525",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-7525",
    "title": "Supervised learning workflow",
    "section": "Train-Test Split (75/25)",
    "text": "Train-Test Split (75/25)"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#why-train-test-split",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#why-train-test-split",
    "title": "Supervised learning workflow",
    "section": "Why Train-Test Split?",
    "text": "Why Train-Test Split?\n\n\nStatistics\n\nNo train-test split\nTrain and evaluate on whole data\nEstimation is key\nLow model complexity\n\n\nMachine Learning\n\nTrain-test split\nTrain on part, evaluate on held-out part\nPrediction (Generalisation) is key\nHigh model complexity; overfitting risk"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#influence-of-n_neighbors-k3",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#influence-of-n_neighbors-k3",
    "title": "Supervised learning workflow",
    "section": "Influence of n_neighbors (k=3)",
    "text": "Influence of n_neighbors (k=3)\n\nLarger k → smoother boundary\nSmaller k → complex boundary"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting",
    "title": "Supervised learning workflow",
    "section": "Overfitting vs Underfitting",
    "text": "Overfitting vs Underfitting"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting-2",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-vs-underfitting-2",
    "title": "Supervised learning workflow",
    "section": "Overfitting vs Underfitting",
    "text": "Overfitting vs Underfitting"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#so-far-happy-ending",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#so-far-happy-ending",
    "title": "Supervised learning workflow",
    "section": "So far: Happy ending?",
    "text": "So far: Happy ending?\n\nReport: best k=19, test accuracy=0.77\nGood for choosing k\nBut: overly optimistic for generalisation\nProblem: test set used for both choosing k and final evaluation"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#why-overfitting-the-validation-set",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#why-overfitting-the-validation-set",
    "title": "Supervised learning workflow",
    "section": "Why? overfitting the Validation Set",
    "text": "Why? overfitting the Validation Set\n\n\n\n\nInteresting reading Preventing Overfitting in cross-validation - Ng 1997"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-the-validation-set",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overfitting-the-validation-set",
    "title": "Supervised learning workflow",
    "section": "Overfitting the Validation Set",
    "text": "Overfitting the Validation Set"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#threefold-split-code",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#threefold-split-code",
    "title": "Supervised learning workflow",
    "section": "Threefold Split (Code)",
    "text": "Threefold Split (Code)\nX_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=0)\n\nval_scores = []\nneighbors = np.arange(1, 15, 2)\nfor i in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    val_scores.append(knn.score(X_val, y_val))\nprint(f\"best validation score: {np.max(val_scores):.3}\\n\")\nbest_n_neighbors = neighbors[np.argmax(val_scores)]\nprint(f\"best n_neighbors:{best_n_neighbors}\\n\")\n\nknn = KNeighborsClassifier(n_neighbors=best_n_neighbors)\nknn.fit(X_trainval, y_trainval)\nprint(f\"test-set score: {knn.score(X_test, y_test):.3f}\")\nbest validation score: 0.991 best n_neighbors: 11 test-set score: 0.951\n\nHere is an implementation of the three-fold split for selecting the number of neighbors. For each number of neighbors that we want to try, we build a model on the training set, and evaluate it on the validation set. We then pick the best validation set score, here that’s 99.1%, achieved when using 11 neighbors. We then retrain the model with this parameter, and evaluate on the test set. The retraining step is somewhat optional. We could also just use the best model. But retraining allows us to make better use of all the data.\nStill, our results depend on how exactly we split the datasets. So how can we make this more robust?"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#new-problem-with-threefold-split",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#new-problem-with-threefold-split",
    "title": "Supervised learning workflow",
    "section": "New problem with threefold split",
    "text": "New problem with threefold split\n\nFixed train/val/test split → results depend on split\nHigh variance in best k and test score, not robust\n\n\n\n   random_seed  best_validation_score  best_k  test_set_score\n0            0                    0.7       5        0.846154\n1            1                    0.7       1        0.538462\n2            2                    1.0      13        0.692308\n3            3                    0.7       1        0.846154\n4            4                    0.9       5        0.769231\n5            5                    0.8      11        0.769231"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-cv-test-set",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-cv-test-set",
    "title": "Supervised learning workflow",
    "section": "Cross-Validation (CV) + Test Set",
    "text": "Cross-Validation (CV) + Test Set"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#n_neighbors-search-results",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#n_neighbors-search-results",
    "title": "Supervised learning workflow",
    "section": "n_neighbors Search Results",
    "text": "n_neighbors Search Results"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-strategies",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cross-validation-strategies",
    "title": "Supervised learning workflow",
    "section": "Cross-Validation Strategies",
    "text": "Cross-Validation Strategies"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#how-many-cv-folds",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#how-many-cv-folds",
    "title": "Supervised learning workflow",
    "section": "How many CV folds?",
    "text": "How many CV folds?\n\nRecommend to run 5-fold or 10-fold CV multiple times, while shuffling the dataset\nMore folds → more training data per fold → better generlisation performance estimate, but slower\nExtreme: LeaveOneOut CV (one fold per sample)"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#repeated-kfold-and-leaveoneout",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#repeated-kfold-and-leaveoneout",
    "title": "Supervised learning workflow",
    "section": "Repeated KFold and LeaveOneOut",
    "text": "Repeated KFold and LeaveOneOut\n\nLeaveOneOut: high variance, slow\nShuffleSplit: repeated random splits\nRepeatedKFold: multiple shuffled KFold runs"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#shuffle-split",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#shuffle-split",
    "title": "Supervised learning workflow",
    "section": "Shuffle Split",
    "text": "Shuffle Split"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#standard-cv-not-preserving-class-distribution",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#standard-cv-not-preserving-class-distribution",
    "title": "Supervised learning workflow",
    "section": "Standard CV not preserving class distribution",
    "text": "Standard CV not preserving class distribution\n\nStandard CV leads to folds with different class distributions"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#stratified-cv-for-multiclass-classification-or-imbalanced-data",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#stratified-cv-for-multiclass-classification-or-imbalanced-data",
    "title": "Supervised learning workflow",
    "section": "Stratified CV: for multiclass classification or imbalanced data",
    "text": "Stratified CV: for multiclass classification or imbalanced data\n\nPreserve class distribution in each fold"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cv-in-scikit-learn",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cv-in-scikit-learn",
    "title": "Supervised learning workflow",
    "section": "CV in scikit-learn",
    "text": "CV in scikit-learn\n\n5-fold CV (default)\nClassification CV is stratified by default\ntrain_test_split(..., stratify=y) to stratify\nNo shuffle by default (repeatable results)"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cv-for-grouped-data",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#cv-for-grouped-data",
    "title": "Supervised learning workflow",
    "section": "CV for grouped data?",
    "text": "CV for grouped data?\n\nCV is more complicated when data are grouped\nData points within a group are correlated (e.g., city, patient, user)\ne.g. The task is to if a patient has a disease based on medical records from 9 cities\nHow CV should be done depends on the application scenario"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#scenario-1-to-predict-new-data-from-existing-cities-i.i.d.",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#scenario-1-to-predict-new-data-from-existing-cities-i.i.d.",
    "title": "Supervised learning workflow",
    "section": "Scenario 1: To predict new data from existing cities (i.i.d.)",
    "text": "Scenario 1: To predict new data from existing cities (i.i.d.)\n\nStandard CV (e.g. KFold, RepeatedKFold) can be used\nGroup information can be ignored"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#scenario-2-to-predict-new-data-from-unknown-cities-not-i.i.d.",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#scenario-2-to-predict-new-data-from-unknown-cities-not-i.i.d.",
    "title": "Supervised learning workflow",
    "section": "Scenario 2: To predict new data from unknown cities (not i.i.d.)",
    "text": "Scenario 2: To predict new data from unknown cities (not i.i.d.)\n\nGroupKFold should be used; ensure each group is contained in exactly one fold (either train or test)\nData from 9 cities; 5-fold CV; GroupKFold as below."
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#standard-train-test-split-or-cv-not-suitable-for-time-series",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#standard-train-test-split-or-cv-not-suitable-for-time-series",
    "title": "Supervised learning workflow",
    "section": "Standard train-test split or CV not suitable for time series",
    "text": "Standard train-test split or CV not suitable for time series"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-for-time-series",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#train-test-split-for-time-series",
    "title": "Supervised learning workflow",
    "section": "Train-Test Split for Time Series",
    "text": "Train-Test Split for Time Series\n\nUsing past data to train, future data to test"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#timeseriessplit",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#timeseriessplit",
    "title": "Supervised learning workflow",
    "section": "TimeSeriesSplit",
    "text": "TimeSeriesSplit"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#time-series-cv",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#time-series-cv",
    "title": "Supervised learning workflow",
    "section": "Time Series CV",
    "text": "Time Series CV"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overview",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#overview",
    "title": "Supervised learning workflow",
    "section": "Overview",
    "text": "Overview\nWe’ve covered:\n\nTrain-test split, threefold split\nCross-validation and its extensions, especially RepeatedKFold\nStratified CV (StratifiedKFold) for multiple classes and imbalanced data\nCV for grouped data and time series data"
  },
  {
    "objectID": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#questions",
    "href": "sessions/supervised_learning_workflow/supervised_learning_workflow.html#questions",
    "title": "Supervised learning workflow",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "sessions/week10.html",
    "href": "sessions/week10.html",
    "title": "Week 10",
    "section": "",
    "text": "This week will introduce how to conduct various types of testing of code, data, and models in ML systems using Python tools of pytest and great expectations.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "10. Testing Machine Learning Systems"
    ]
  },
  {
    "objectID": "sessions/week10.html#introduction",
    "href": "sessions/week10.html#introduction",
    "title": "Week 10",
    "section": "",
    "text": "This week will introduce how to conduct various types of testing of code, data, and models in ML systems using Python tools of pytest and great expectations.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "10. Testing Machine Learning Systems"
    ]
  },
  {
    "objectID": "sessions/week10.html#learning-objectives",
    "href": "sessions/week10.html#learning-objectives",
    "title": "Week 10",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the principle and types of testing in ML systems.\nCan use pytest and great expectations to test ML code and data.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "10. Testing Machine Learning Systems"
    ]
  },
  {
    "objectID": "sessions/week10.html#lecture",
    "href": "sessions/week10.html#lecture",
    "title": "Week 10",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 3: Advanced Topics",
      "10. Testing Machine Learning Systems"
    ]
  },
  {
    "objectID": "sessions/week10.html#quiz",
    "href": "sessions/week10.html#quiz",
    "title": "Week 10",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "10. Testing Machine Learning Systems"
    ]
  },
  {
    "objectID": "sessions/week10.html#practical",
    "href": "sessions/week10.html#practical",
    "title": "Week 10",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Advanced Topics",
      "10. Testing Machine Learning Systems"
    ]
  },
  {
    "objectID": "sessions/week10.html#further-resources",
    "href": "sessions/week10.html#further-resources",
    "title": "Week 10",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 3: Advanced Topics",
      "10. Testing Machine Learning Systems"
    ]
  },
  {
    "objectID": "sessions/week1_lecture.html#what-we-learnt-in-term-1",
    "href": "sessions/week1_lecture.html#what-we-learnt-in-term-1",
    "title": "Introduction to machine learning",
    "section": "What we learnt in Term 1",
    "text": "What we learnt in Term 1\n\nPython programming\nData types\nVisualisation\nRegression (Ordinary Least Square; Linear Mixed Effects; multicollinearity)\nDimensionality reduction\nClustering"
  },
  {
    "objectID": "sessions/week1_lecture.html#learning-objectives",
    "href": "sessions/week1_lecture.html#learning-objectives",
    "title": "Introduction to machine learning",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture you should:\n\nUnderstand the basics and classifications of machine learning\nUnderstand the differences between statistical methods and machine learning (estimation vs. prediction)\nAppreciate GIGO theorems in machine learning and data science"
  },
  {
    "objectID": "sessions/week1_lecture.html#ml-as-subset-of-ai",
    "href": "sessions/week1_lecture.html#ml-as-subset-of-ai",
    "title": "Introduction to machine learning",
    "section": "ML as subset of AI",
    "text": "ML as subset of AI\n\n\n\nMachine learning (decision tree, random forest, k-means, etc.)\n\nDeep learning (deep neural networks)\n\nOther AI tools: graphical models, symbolic AI\n\nNote: we don’t distinguish ML/DL and consider NN as part of ML\n\n\n\n\n\nImage Credit: Lecture slide (ML is a subset of AI)"
  },
  {
    "objectID": "sessions/week1_lecture.html#definition-of-machine-learning",
    "href": "sessions/week1_lecture.html#definition-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Definition of machine learning",
    "text": "Definition of machine learning\n\nArthur Samuel (1959): (Machine learning is the) field of study that gives computers the ability to learn without being explicitly programmed.\n\n\nTom Mitchell (1997): A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E."
  },
  {
    "objectID": "sessions/week1_lecture.html#what-is-machine-learning",
    "href": "sessions/week1_lecture.html#what-is-machine-learning",
    "title": "Introduction to machine learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nExtracting knowledge from data\nRelying on data and algorithms\nClosely related to statistics but distinct from linear models\nFocus on prediction rather than estimating relationships or interpretation"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-facebook",
    "href": "sessions/week1_lecture.html#examples-facebook",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook",
    "text": "Examples: Facebook\n\n\n\n\nMachine learning in news feed ranking\nContent selection and targeting\nAds and recommendations"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-facebook-cont.",
    "href": "sessions/week1_lecture.html#examples-facebook-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nFace detection and recognition\nPhoto organization"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-facebook-cont.-1",
    "href": "sessions/week1_lecture.html#examples-facebook-cont.-1",
    "title": "Introduction to machine learning",
    "section": "Examples: Facebook (cont.)",
    "text": "Examples: Facebook (cont.)\n\n\n\n\nPhoto selection and layout"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-amazon",
    "href": "sessions/week1_lecture.html#examples-amazon",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon",
    "text": "Examples: Amazon\n\n\n\n\nProduct ranking\nPersonalised recommendations\nAds selection"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-amazon-cont.",
    "href": "sessions/week1_lecture.html#examples-amazon-cont.",
    "title": "Introduction to machine learning",
    "section": "Examples: Amazon (cont.)",
    "text": "Examples: Amazon (cont.)\n\n\n\n\nSeller selection\nDefault choices\nRelated products"
  },
  {
    "objectID": "sessions/week1_lecture.html#science-applications",
    "href": "sessions/week1_lecture.html#science-applications",
    "title": "Introduction to machine learning",
    "section": "Science Applications",
    "text": "Science Applications\n\n\n\n\nPersonalised cancer treatment\nMedical diagnosis\nDrug discovery\nHiggs boson discovery\nExoplanet detection"
  },
  {
    "objectID": "sessions/week1_lecture.html#types-of-machine-learning",
    "href": "sessions/week1_lecture.html#types-of-machine-learning",
    "title": "Introduction to machine learning",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\n\nSupervised Learning: Learn from input-output pairs (with labelled data)\nUnsupervised Learning: Discover structure in data (without labelled data)\nReinforcement Learning: Learn through interaction with environment (with an actual/simulated environment)"
  },
  {
    "objectID": "sessions/week1_lecture.html#supervised-learning",
    "href": "sessions/week1_lecture.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\\[\n\\begin{aligned}\n(x_i, y_i) &\\sim p(x, y) \\text{ i.i.d.} \\\\\nx_i &\\in \\mathbb{R}^n \\\\\ny_i &\\in \\mathbb{R} \\\\\nf(x_i) &\\approx y_i\n\\end{aligned}\n\\]\nLearn a function \\(f\\) from input-output pairs to predict on new data."
  },
  {
    "objectID": "sessions/week1_lecture.html#generalisation-to-unseen-data",
    "href": "sessions/week1_lecture.html#generalisation-to-unseen-data",
    "title": "Introduction to machine learning",
    "section": "Generalisation to unseen data",
    "text": "Generalisation to unseen data\n\nGoal: \\(f(x_i) \\approx y_i\\) on training data\nMore important: \\(f(x) \\approx y\\) on new data\nCore distinction: not just function approximation, but prediction on unseen data\nAvoiding overfitting to training data is key"
  },
  {
    "objectID": "sessions/week1_lecture.html#examples-of-supervised-learning",
    "href": "sessions/week1_lecture.html#examples-of-supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Examples of Supervised Learning",
    "text": "Examples of Supervised Learning\n\nSpam detection\nMedical diagnosis\nAd click prediction"
  },
  {
    "objectID": "sessions/week1_lecture.html#unsupervised-learning",
    "href": "sessions/week1_lecture.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\\[\nx_i \\sim p(x) \\text{ i.i.d.}\n\\]\nLearn about the distribution \\(p\\):\n\nClustering\nDimensionality reduction\nTopic modeling\nOutlier detection"
  },
  {
    "objectID": "sessions/week1_lecture.html#other-types-of-learning",
    "href": "sessions/week1_lecture.html#other-types-of-learning",
    "title": "Introduction to machine learning",
    "section": "Other Types of Learning",
    "text": "Other Types of Learning\n\nSemi-supervised\nActive Learning\nForecasting\nTransfer learning\nIf you understand supervised/unsupervised/reinforcement learning, others are easier to grasp"
  },
  {
    "objectID": "sessions/week1_lecture.html#dont-read-a-book-by-its-name",
    "href": "sessions/week1_lecture.html#dont-read-a-book-by-its-name",
    "title": "Introduction to machine learning",
    "section": "Don’t read a book by its name",
    "text": "Don’t read a book by its name\n\nClassifications of neighbourhoods (e.g. London output area classification) that are actually clustering\nAnomaly detection methods can be unsupervised or supervised learning\nForecasting can be done with lagged features (via supervised learning), or with time series data (via time series analysis), or both"
  },
  {
    "objectID": "sessions/week1_lecture.html#what-does-llm-belong-to",
    "href": "sessions/week1_lecture.html#what-does-llm-belong-to",
    "title": "Introduction to machine learning",
    "section": "What does LLM belong to?",
    "text": "What does LLM belong to?\n\n\n\nImage Credit: medium.com"
  },
  {
    "objectID": "sessions/week1_lecture.html#classification-vs.-regression",
    "href": "sessions/week1_lecture.html#classification-vs.-regression",
    "title": "Introduction to machine learning",
    "section": "Classification vs. Regression",
    "text": "Classification vs. Regression\n\n\nClassification\n\nTarget \\(y\\) is discrete\nExample: Is this patient sick?\n\n\nRegression\n\nTarget \\(y\\) is continuous\nExample: How long to recover?"
  },
  {
    "objectID": "sessions/week1_lecture.html#relationship-to-statistics",
    "href": "sessions/week1_lecture.html#relationship-to-statistics",
    "title": "Introduction to machine learning",
    "section": "Relationship to Statistics",
    "text": "Relationship to Statistics\n\n\nStatistics\n\nModel first\nEstimation emphasis\nYes/no questions\nWith many assumptions, need to test\nInterpretation is key\ne.g. Does smoking lead to lung cancer?\n\n\nMachine Learning\n\nData first\nPrediction emphasis\nFuture predictions\nFew assumptions (but not assumption-free)\nInterpretation isn’t primary\ne.g. Predict tomorrow’s weather"
  },
  {
    "objectID": "sessions/week1_lecture.html#following-statements-are-not-recommended",
    "href": "sessions/week1_lecture.html#following-statements-are-not-recommended",
    "title": "Introduction to machine learning",
    "section": "Following statements are not recommended",
    "text": "Following statements are not recommended\n\n“Linear regression is not needed anymore because of machine learning.”\n“Machine learning is just a fad; statistics is more important.”\n“Machine learning models are black boxes; we can’t interpret them at all.”\n“Machine learning models are as interpretable as linear models.”\n“Machine learning models have no assumptions.”"
  },
  {
    "objectID": "sessions/week1_lecture.html#guiding-principles-goal-considerations",
    "href": "sessions/week1_lecture.html#guiding-principles-goal-considerations",
    "title": "Introduction to machine learning",
    "section": "Guiding Principles: Goal Considerations",
    "text": "Guiding Principles: Goal Considerations\n\nDefine the goal clearly\nDefine how to measure success\nThink about context and baseline\nAsk: what’s the benefit?"
  },
  {
    "objectID": "sessions/week1_lecture.html#thinking-in-context",
    "href": "sessions/week1_lecture.html#thinking-in-context",
    "title": "Introduction to machine learning",
    "section": "Thinking in Context",
    "text": "Thinking in Context\n\nWhat do you want to achieve?\nWhat’s the baseline and its performance?\nWhat improvement over baseline do you need?"
  },
  {
    "objectID": "sessions/week1_lecture.html#good-and-bad-substitutes",
    "href": "sessions/week1_lecture.html#good-and-bad-substitutes",
    "title": "Introduction to machine learning",
    "section": "Good and Bad Substitutes",
    "text": "Good and Bad Substitutes\n\nChoose metrics carefully\nSubstitute metrics can be misleading\nOptimize for the right goal\nUnderstand side effects"
  },
  {
    "objectID": "sessions/week1_lecture.html#communicating-results",
    "href": "sessions/week1_lecture.html#communicating-results",
    "title": "Introduction to machine learning",
    "section": "Communicating Results",
    "text": "Communicating Results\n\nExplain why your approach works\nCommunicate uncertainty\nShow impact and limitations\nConvince stakeholders"
  },
  {
    "objectID": "sessions/week1_lecture.html#explainable-results",
    "href": "sessions/week1_lecture.html#explainable-results",
    "title": "Introduction to machine learning",
    "section": "Explainable Results",
    "text": "Explainable Results\n\n\n\n\nUsers want to know why recommendations are made\nExplainability improves engagement\nImportant for trust and transparency"
  },
  {
    "objectID": "sessions/week1_lecture.html#ethical-considerations",
    "href": "sessions/week1_lecture.html#ethical-considerations",
    "title": "Introduction to machine learning",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\n\n\n\n\nBias in risk assessments\nFairness in automated decisions\nTransparency and accountability"
  },
  {
    "objectID": "sessions/week1_lecture.html#ethics-its-in-the-application",
    "href": "sessions/week1_lecture.html#ethics-its-in-the-application",
    "title": "Introduction to machine learning",
    "section": "Ethics: It’s in the Application!",
    "text": "Ethics: It’s in the Application!\n\nUnderstand biases in your system\nConsider the impact of predictions\nUse algorithms responsibly\nSame algorithm, different outcomes"
  },
  {
    "objectID": "sessions/week1_lecture.html#data-and-data-collection",
    "href": "sessions/week1_lecture.html#data-and-data-collection",
    "title": "Introduction to machine learning",
    "section": "Data and Data Collection",
    "text": "Data and Data Collection\n\nCritical component of ML\nMore data usually helps (if from right source)\nConsider marginal cost vs. marginal benefit"
  },
  {
    "objectID": "sessions/week1_lecture.html#free-vs-expensive-data",
    "href": "sessions/week1_lecture.html#free-vs-expensive-data",
    "title": "Introduction to machine learning",
    "section": "Free vs Expensive Data",
    "text": "Free vs Expensive Data\n\n\nFree Data\n\nOpen data from gov and census, often aggregated (e.g. ONS, London Fire Brigade, NASA)\nOpen data from companies (e.g. Google Street View) (Read the license first)\nSynthetic data\nWeb scraping (be careful with legality and ethics)\n\n\nExpensive Data\n\nIndividual data (e.g. health records)\nMobile phone data (high cost)\nUser survey\nHigh-resolution imagery (e.g. satellite, aerial, drone)"
  },
  {
    "objectID": "sessions/week1_lecture.html#big-data-considerations",
    "href": "sessions/week1_lecture.html#big-data-considerations",
    "title": "Introduction to machine learning",
    "section": "Big Data Considerations",
    "text": "Big Data Considerations\n\nMore data can be more expensive to work with\nSubsample to RAM when possible (512GB available in cloud)\nRuntime and analyst time matter\nAlways try with a small sample first"
  },
  {
    "objectID": "sessions/week1_lecture.html#theorem-garbage-in-garbage-out-gigo",
    "href": "sessions/week1_lecture.html#theorem-garbage-in-garbage-out-gigo",
    "title": "Introduction to machine learning",
    "section": "Theorem: Garbage in, garbage out (GIGO)",
    "text": "Theorem: Garbage in, garbage out (GIGO)\n\nGreat algorithms + bad data = bad results\n\n\n\n\nModel performance is constrained by data quality.\nBiased, noisy, or incomplete data leads to misleading predictions.\n\n\n\n\n\nImage Credit: x.com/xschelling/status/954936528555429888"
  },
  {
    "objectID": "sessions/week1_lecture.html#good-data-large-size-high-quality.",
    "href": "sessions/week1_lecture.html#good-data-large-size-high-quality.",
    "title": "Introduction to machine learning",
    "section": "Good data = large size + high quality.",
    "text": "Good data = large size + high quality.\n\n\n\nSufficient sample size to capture variability in the problem\n\nHigh-quality labels and accurate measurements.\n\nRepresentative of the population and application context.\n\n\n\n\n\nImage Credit: Internet"
  },
  {
    "objectID": "sessions/week1_lecture.html#data-size-and-performance",
    "href": "sessions/week1_lecture.html#data-size-and-performance",
    "title": "Introduction to machine learning",
    "section": "Data size and performance",
    "text": "Data size and performance\n\n\nThe performance of ML/DL increases rapidly with the size of the data:\n\nLarge neural nets benefit the most from big data.\n\nMedium and small neural nets also improve with more data.\n\nTraditional ML algorithms (e.g. random forest, SVM) may saturate earlier.\n\n\n\n\n\nPerformance vs data size for ML/DL models"
  },
  {
    "objectID": "sessions/week1_lecture.html#feature-engineering-eda-90-time",
    "href": "sessions/week1_lecture.html#feature-engineering-eda-90-time",
    "title": "Introduction to machine learning",
    "section": "Feature engineering & EDA (~90% time)",
    "text": "Feature engineering & EDA (~90% time)\n\nMostly manual rather than automated (no automated methods for EDA)\nDomain knowledge from expertise is desirable\nUsing visualisation to identify patterns & data relationship\nRemoving noisy or erroneous data\n\nDealing with missing data\n\nGenerating new features by combining existing ones"
  },
  {
    "objectID": "sessions/week1_lecture.html#example-representation-of-geospatial-locations-in-ml-models",
    "href": "sessions/week1_lecture.html#example-representation-of-geospatial-locations-in-ml-models",
    "title": "Introduction to machine learning",
    "section": "Example: Representation of geospatial locations in ML models",
    "text": "Example: Representation of geospatial locations in ML models\n\nRaw coordinates (e.g. long/lat) may not be directly useful\nNeed to engineer features that capture spatial relationships\nLong/lat\nDistance to POIs (train stations/schools).\nUsing adjacency matrix between spatial units"
  },
  {
    "objectID": "sessions/week2.html",
    "href": "sessions/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "This week will introduce framework and evaluation metrics of supervised learning.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "2. Supervised Learning Metrics"
    ]
  },
  {
    "objectID": "sessions/week2.html#introduction",
    "href": "sessions/week2.html#introduction",
    "title": "Week 2",
    "section": "",
    "text": "This week will introduce framework and evaluation metrics of supervised learning.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "2. Supervised Learning Metrics"
    ]
  },
  {
    "objectID": "sessions/week2.html#learning-objectives",
    "href": "sessions/week2.html#learning-objectives",
    "title": "Week 2",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the framework of supervised learning.\nUnderstand the common metrics for regression and classification.\nAppreciate the accuracy paradox and its implications.\nKnow how to pick the right metric for your problem.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "2. Supervised Learning Metrics"
    ]
  },
  {
    "objectID": "sessions/week2.html#lecture",
    "href": "sessions/week2.html#lecture",
    "title": "Week 2",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 1: Supervised Learning",
      "2. Supervised Learning Metrics"
    ]
  },
  {
    "objectID": "sessions/week2.html#quiz",
    "href": "sessions/week2.html#quiz",
    "title": "Week 2",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 1: Supervised Learning",
      "2. Supervised Learning Metrics"
    ]
  },
  {
    "objectID": "sessions/week2.html#practical",
    "href": "sessions/week2.html#practical",
    "title": "Week 2",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 1: Supervised Learning",
      "2. Supervised Learning Metrics"
    ]
  },
  {
    "objectID": "sessions/week2.html#further-resources",
    "href": "sessions/week2.html#further-resources",
    "title": "Week 2",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 1: Supervised Learning",
      "2. Supervised Learning Metrics"
    ]
  },
  {
    "objectID": "sessions/week4.html",
    "href": "sessions/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "This week will introduce tree-based methods for supervised learning, including decision trees, random forests, and gradient boosting.",
    "crumbs": [
      "Part 2: Methods",
      "4. Tree-based Methods"
    ]
  },
  {
    "objectID": "sessions/week4.html#introduction",
    "href": "sessions/week4.html#introduction",
    "title": "Week 4",
    "section": "",
    "text": "This week will introduce tree-based methods for supervised learning, including decision trees, random forests, and gradient boosting.",
    "crumbs": [
      "Part 2: Methods",
      "4. Tree-based Methods"
    ]
  },
  {
    "objectID": "sessions/week4.html#learning-objectives",
    "href": "sessions/week4.html#learning-objectives",
    "title": "Week 4",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the design and training of decision trees.\nUnderstand the principle of ensemble methods, including bagging and boosting.\nUnderstand the design and strengths of random forests and gradient boosting machines.\nCan apply tree-based methods from proper libraries (random forest from sklearn and XGBoot from XGBoost).",
    "crumbs": [
      "Part 2: Methods",
      "4. Tree-based Methods"
    ]
  },
  {
    "objectID": "sessions/week4.html#lecture",
    "href": "sessions/week4.html#lecture",
    "title": "Week 4",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Methods",
      "4. Tree-based Methods"
    ]
  },
  {
    "objectID": "sessions/week4.html#quiz",
    "href": "sessions/week4.html#quiz",
    "title": "Week 4",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Methods",
      "4. Tree-based Methods"
    ]
  },
  {
    "objectID": "sessions/week4.html#practical",
    "href": "sessions/week4.html#practical",
    "title": "Week 4",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Methods",
      "4. Tree-based Methods"
    ]
  },
  {
    "objectID": "sessions/week4.html#further-resources",
    "href": "sessions/week4.html#further-resources",
    "title": "Week 4",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 2: Methods",
      "4. Tree-based Methods"
    ]
  },
  {
    "objectID": "sessions/week6.html",
    "href": "sessions/week6.html",
    "title": "Week 6",
    "section": "",
    "text": "This week, we’ll explore how to generalise neural networks to graph-structured data and introduce the classic types of graph neural networks.",
    "crumbs": [
      "Part 2: Methods",
      "6. Graph Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week6.html#introduction",
    "href": "sessions/week6.html#introduction",
    "title": "Week 6",
    "section": "",
    "text": "This week, we’ll explore how to generalise neural networks to graph-structured data and introduce the classic types of graph neural networks.",
    "crumbs": [
      "Part 2: Methods",
      "6. Graph Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week6.html#learning-objectives",
    "href": "sessions/week6.html#learning-objectives",
    "title": "Week 6",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the design and training of graph neural networks.\nUnderstand the common types of graph neural networks.\nCan apply graph neural networks to real-world graph-structured data.",
    "crumbs": [
      "Part 2: Methods",
      "6. Graph Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week6.html#lecture",
    "href": "sessions/week6.html#lecture",
    "title": "Week 6",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 2: Methods",
      "6. Graph Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week6.html#quiz",
    "href": "sessions/week6.html#quiz",
    "title": "Week 6",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 2: Methods",
      "6. Graph Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week6.html#practical",
    "href": "sessions/week6.html#practical",
    "title": "Week 6",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 2: Methods",
      "6. Graph Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week6.html#further-resources",
    "href": "sessions/week6.html#further-resources",
    "title": "Week 6",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 2: Methods",
      "6. Graph Neural Networks"
    ]
  },
  {
    "objectID": "sessions/week8.html",
    "href": "sessions/week8.html",
    "title": "Week 8",
    "section": "",
    "text": "This week will introduce how to deal with imbalanced data in machine learning classifcation tasks.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "8. Imbalanced Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#introduction",
    "href": "sessions/week8.html#introduction",
    "title": "Week 8",
    "section": "",
    "text": "This week will introduce how to deal with imbalanced data in machine learning classifcation tasks.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "8. Imbalanced Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#learning-objectives",
    "href": "sessions/week8.html#learning-objectives",
    "title": "Week 8",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nUnderstand the types and issue of imbalanced data.\nUnderstand the classic methods to handle imbalanced data.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "8. Imbalanced Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#lecture",
    "href": "sessions/week8.html#lecture",
    "title": "Week 8",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture",
    "crumbs": [
      "Part 3: Advanced Topics",
      "8. Imbalanced Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#quiz",
    "href": "sessions/week8.html#quiz",
    "title": "Week 8",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page.",
    "crumbs": [
      "Part 3: Advanced Topics",
      "8. Imbalanced Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#practical",
    "href": "sessions/week8.html#practical",
    "title": "Week 8",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload",
    "crumbs": [
      "Part 3: Advanced Topics",
      "8. Imbalanced Data"
    ]
  },
  {
    "objectID": "sessions/week8.html#further-resources",
    "href": "sessions/week8.html#further-resources",
    "title": "Week 8",
    "section": "Further resources",
    "text": "Further resources",
    "crumbs": [
      "Part 3: Advanced Topics",
      "8. Imbalanced Data"
    ]
  },
  {
    "objectID": "sessions/weekX.html",
    "href": "sessions/weekX.html",
    "title": "Week XXX",
    "section": "",
    "text": "This week will introduce XXX."
  },
  {
    "objectID": "sessions/weekX.html#introduction",
    "href": "sessions/weekX.html#introduction",
    "title": "Week XXX",
    "section": "",
    "text": "This week will introduce XXX."
  },
  {
    "objectID": "sessions/weekX.html#learning-objectives",
    "href": "sessions/weekX.html#learning-objectives",
    "title": "Week XXX",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this week, you will be able to:\n\nXXX\nXXX\nXXX"
  },
  {
    "objectID": "sessions/weekX.html#lecture",
    "href": "sessions/weekX.html#lecture",
    "title": "Week XXX",
    "section": "Lecture",
    "text": "Lecture\nTo access the lecture notes: Lecture"
  },
  {
    "objectID": "sessions/weekX.html#quiz",
    "href": "sessions/weekX.html#quiz",
    "title": "Week XXX",
    "section": "Quiz",
    "text": "Quiz\nTo access the quiz on Moodle, please check Moodle page."
  },
  {
    "objectID": "sessions/weekX.html#practical",
    "href": "sessions/weekX.html#practical",
    "title": "Week XXX",
    "section": "Practical",
    "text": "Practical\n\n\n\n\n\n\nNote\n\n\n\nTo save a copy of notebook to your own GitHub Repo: follow the GitHub link, click on Raw and then Save File As... to save it to your own computer. Make sure to change the extension from .ipynb.txt (which will probably be the default) to .ipynbbefore adding the file to your GitHub repository.\n\n\nTo access the practical:\n\nPreview\nDownload"
  },
  {
    "objectID": "sessions/weekX.html#further-resources",
    "href": "sessions/weekX.html#further-resources",
    "title": "Week XXX",
    "section": "Further resources",
    "text": "Further resources"
  },
  {
    "objectID": "sessions/weekX_practical.html",
    "href": "sessions/weekX_practical.html",
    "title": "Practical XXX: XXX",
    "section": "",
    "text": "This week is focussed on XXX."
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-a-callout-note",
    "href": "sessions/weekX_practical.html#to-add-a-callout-note",
    "title": "Practical XXX: XXX",
    "section": "To add a callout-note",
    "text": "To add a callout-note\n\n\n\n\n\n\nNote\n\n\n\nSuggestions for a Better Learning Experience:\n\nXXX"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-python-code-without-running-them",
    "href": "sessions/weekX_practical.html#to-add-python-code-without-running-them",
    "title": "Practical XXX: XXX",
    "section": "To add Python code without running them …",
    "text": "To add Python code without running them …\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-and-run-python-code",
    "href": "sessions/weekX_practical.html#to-add-and-run-python-code",
    "title": "Practical XXX: XXX",
    "section": "To add and run Python code",
    "text": "To add and run Python code\n\nimport pandas as pd\n\n# Read CSV file, skipping first 5 rows, using row 6 as header, and handling comma as thousands separator\ndf_pop = pd.read_csv(\n    'L1_data/UK_census_population.csv',\n    skiprows=5,        # Skip first 5 rows. Wnhy?\n    thousands=',',     # Interpret commas as thousands separators\n    header=0           # After skipping, the first row becomes the header\n)\n\nprint(df_pop.head())\n\n   Area code          Area name Area type  Population 2011  Population 2021  \\\n0  K04000001  England and Wales  National       56075912.0       59597542.0   \n1  E92000001            England   Country       53012456.0       56490048.0   \n2  W92000004              Wales   Country        3063456.0        3107494.0   \n3  E12000001         North East    Region        2596886.0        2647013.0   \n4  E12000002         North West    Region        7052177.0        7417397.0   \n\n   Percentage change  \n0                6.3  \n1                6.6  \n2                1.4  \n3                1.9  \n4                5.2"
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-a-photo---replace-the-path.-using-relative-path-is-also-okay.",
    "href": "sessions/weekX_practical.html#to-add-a-photo---replace-the-path.-using-relative-path-is-also-okay.",
    "title": "Practical XXX: XXX",
    "section": "To add a photo - replace the path. Using relative path is also okay.",
    "text": "To add a photo - replace the path. Using relative path is also okay."
  },
  {
    "objectID": "sessions/weekX_practical.html#to-add-some-questions",
    "href": "sessions/weekX_practical.html#to-add-some-questions",
    "title": "Practical XXX: XXX",
    "section": "To add some “questions”",
    "text": "To add some “questions”\nThe qmd file will be rendered as two files in sessions folder, including a html and ipynb format. The html file will contain both question and answer, while the ipynb file will contain only the question.\nFor the effect, please check HTML and ipynb.\n\nQuestionAnswerAnswer\n\n\nif ??\n    ??\nelse:\n    ??\n\n\n\n\n\nif 'Moscow' in ['Moscow', 'Beijing']:\n    print(\"Moscow is in the cities list.\")\nelse:\n    print(\"Moscow is not in the cities list.\")\nMoscow is in the cities list."
  },
  {
    "objectID": "sessions/weekX_practical.html#youre-done",
    "href": "sessions/weekX_practical.html#youre-done",
    "title": "Practical XXX: XXX",
    "section": "You’re Done!",
    "text": "You’re Done!\nCongratulations on completing the first QM practical session! If you are still working on it, take you time.\nDon’t worry about understanding every detail of the Python code — what matters most is knowing which functions to use for a specific task, like checking minimum and maximum values or generating boxplots, and knowing how to debug when it goes wrong. Remember, practice makes perfect."
  }
]