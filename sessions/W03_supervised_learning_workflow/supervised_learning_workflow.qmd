---
title: "Supervised learning workflow"
subtitle: "Model training and selection"
author: 
  - name: "Huanfa Chen"
email: "huanfa.chen@ucl.ac.uk"
date-as-string: "13/12/2025"
from: markdown+emoji
format:
  revealjs:
    transition: none
    slide-number: TRUE
    preview-links: auto
---
## Last week
- Framework of supervised learning
- Evaluation metrics for regression and classification

## Objectives of this week
1. Understand different workflow of supervised learning.
2. Understand train-test split and train-validation-test split.
3. Understand cross validation and its extensions
4. Know when to use different model evaluation methods.

## Supervised Learning

$$
\begin{aligned}
(x_i, y_i) &\sim p(x, y) \text{ i.i.d.} \\
x_i &\in \mathbb{R}^n \\
y_i &\in 
\begin{cases}
\mathbb{R}, & \text{(regression)} \\
\mathcal{Y} \text{ (finite set)}, & \text{(classification)}
\end{cases} \\
\text{learn } f(x_i) &\approx y_i \\
\text{such that } f(x) &\approx y
\end{aligned}
$$

## Challenges of training supervised models

1. To select evaluation metrics (so that the model solves the right problem)
2. [To design workflow (so that the model generalises well and avoids overfitting)]{style="color:blue"}
- A robust workflow leads to an unbiased evaluation of the model's generalisation performance
- It will select effective model hyperparameters (to avoid overfitting)

## Various workflows

1. Train-test split
2. Train-validation-test split
3. Cross-validation + test set

# Train-Test split

## Train-Test Split (usually 75/25)

<div style="text-align:center;">
  <img src="images/train_test_split_new.png" alt="Train-test split" style="max-width:100%;">
</div>

## Why Train-Test Split?

::::: columns
::: {.column width="50%"}
### Statistics

-   No train-test split
-   Train and evaluate on whole data
-   Estimation is key
-   Low model complexity
:::

::: {.column width="50%"}
### Machine Learning

-   Train-test split
-   Train on part, evaluate on held-out part
-   Prediction (Generalisation) is key 
-   High model complexity; overfitting risk 
:::
:::::

## Example - K Nearest Neighbors (KNN)

<div style="text-align:center;">
  <img src="images/knn_boundary_test_points.png" alt="KNN boundary with test points" style="max-width:40%;">
</div>

- Predict label of closest training sample
- $f(x) = y_i,\; i = \operatorname*{argmin}_j \lVert x_j - x \rVert$

## Nearest Neighbors (k=1)

<div style="text-align:center;">
  <img src="images/knn_boundary_k1.png" alt="KNN boundary k=1" style="max-width:40%;">
</div>

- Memorises training samples
- Sensitive/Complex decision boundary

# KNN with scikit-learn

```{python}
#| echo: false
from sklearn.datasets import make_blobs
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
import pandas as pd

%matplotlib inline
plt.rcParams['image.cmap'] = "bwr"
plt.rcParams['figure.dpi'] = "300"
plt.rcParams['savefig.bbox'] = "tight"

X, y = make_blobs(centers=2, cluster_std=2, random_state=0, n_samples=50)
```

```{python}
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)
print("accuracy:", knn.score(X_test, y_test))
y_pred = knn.predict(X_test)
```

## Influence of n_neighbors (k=3)

- Larger k → smoother boundary
- Smaller k → complex boundary

<div style="text-align:center;">
  <img src="images/knn_boundary_k3.png" alt="KNN boundary k=3" style="max-width:50%;">
</div>

# Influence of n_neighbors

- Trade-off between complexity and generalisation
- Smaller k → higher model complexity

<div style="text-align:center;">
  <img src="images/knn_boundary_varying_k.png" alt="KNN boundary varying k" style="max-width:45%;">
</div>

# Model Complexity

<div style="text-align:center;">
  <img src="images/knn_model_complexity.png" alt="KNN model complexity" style="max-width:75%;">
</div>

- Train score ↓ as k increases
- Test score peaks at intermediate k

## Overfitting vs Underfitting

- Performance on training data generally increases with model complexity
<div style="text-align:center;">
  <img src="images/overfitting_underfitting_cartoon_train.png" alt="Overfitting vs Underfitting (train)" style="max-width:80%;">
</div>

## Overfitting vs Underfitting

- Performance on testing data firstly increases with model complexity, and then decreases *due to overfitting*
<div style="text-align:center;">
  <img src="images/overfitting_underfitting_cartoon_generalization.png" alt="Overfitting vs Underfitting (generalization)" style="max-width:80%;">
</div>

## Overfitting vs Underfitting

- The optimal hyperparameter achieves balance between underfitting and overfitting

<!-- ![](images/overfitting_underfitting_cartoon_full.png){.r-stretch} -->
<div style="text-align:center;">
  <img src="images/overfitting_underfitting_cartoon_full.png" alt="Overfitting vs Underfitting (full)" style="max-width:80%;">
</div>

## So far: Happy ending?

- Report: best k=19, test accuracy=0.77
- Good for choosing k
- [But the test accuray is overly optimistic and biased]{style="color:blue"}
- Problem: test set used twice, for both choosing k and final evaluation

## Summary of train-test split

- The idea is to train the model on the training set, and evaluate it on the test set.
- But, problematic to use the test set for both hyperparameter tuning and final evaluation.
- If we need to choose model hyperparameters, we need a separate validation set (or cross-validation).

# Train-validation-test split

# Idea

- Training → Validation (hyperparams tuning) → Test (final eval)

<div style="text-align:center;">
  <img src="images/train_test_validation_split.png" alt="Train/validation/test split" style="max-width:100%;">
</div>

## Why using three-hold split?

- It separates hyperparameter tuning (using validation set) and final evaluation (using test set)
- Ensures test set is only used for final evaluation
- Interesting reading [Preventing Overfitting in cross-validation - Ng 1997](http://robotics.stanford.edu/~ang/papers/cv-final.pdf)

<div style="text-align:center;">
  <img src="images/overfitting_validation_set_1.png" alt="Overfitting validation 1" style="max-width:80%;">
</div>



## Overfitting the Validation Set

<div style="text-align:center;">
  <img src="images/overfitting_validation_set_2.png" alt="Overfitting validation 2" style="max-width:80%;">
</div>

# Overfitting the Validation Set

<div style="text-align:center;">
  <img src="images/overfitting_validation_set_3.png" alt="Overfitting validation 3" style="max-width:80%;">
</div>

# Overfitting the Validation Set

<div style="text-align:center;">
  <img src="images/overfitting_validation_set_4.png" alt="Overfitting validation 4" style="max-width:80%;">
</div>

## Threefold Split (Code)

```python
X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=0)
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=0)

val_scores = []
neighbors = np.arange(1, 15, 2)
for i in neighbors:
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    val_scores.append(knn.score(X_val, y_val))
print(f"best validation score: {np.max(val_scores):.3}\n")
best_n_neighbors = neighbors[np.argmax(val_scores)]
print(f"best n_neighbors:{best_n_neighbors}\n")

knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
knn.fit(X_trainval, y_trainval)
print(f"test-set score: {knn.score(X_test, y_test):.3f}")
```

- best validation score: 0.991
- best n_neighbors: 11
- test-set score: 0.951

::: {.notes}
Here is an implementation of the three-fold split for selecting the
number of neighbors.
For each number of neighbors that we want to try, we build a model on
the training set, and evaluate it on the validation set.
We then pick the best validation set score, here that’s 99.1%, achieved
when using 11 neighbors.
We then retrain the model with this parameter, and evaluate on the test set.
The retraining step is somewhat optional. We could also just use the best
model. But retraining allows us to make better use of all the data.

Still, our results depend on how exactly we split the datasets.
So how can we make this more robust?
:::

## New problem with threefold split

- Fixed train-val-test split: results depend on split
- High variance in best k and test score, not robust

```{python}
#| echo: false
import pandas as pd

results_data = []

for random_seed in range(6):
    X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, random_state=random_seed)
    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=random_seed)

    val_scores = []
    neighbors = np.arange(1, 15, 2)
    for i in neighbors:
        knn = KNeighborsClassifier(n_neighbors=i)
        knn.fit(X_train, y_train)
        val_scores.append(knn.score(X_val, y_val))
    
    best_val_score = np.max(val_scores)
    best_n_neighbors = neighbors[np.argmax(val_scores)]
    
    knn = KNeighborsClassifier(n_neighbors=best_n_neighbors)
    knn.fit(X_trainval, y_trainval)
    test_score = knn.score(X_test, y_test)
    
    results_data.append({
        'random_seed': random_seed,
        'best_validation_score': best_val_score,
        'best_k': best_n_neighbors,
        'test_set_score': test_score
    })

results_df = pd.DataFrame(results_data)
print(results_df)
```

<!-- | random_seed | best_validation_score | best_k | test_set_score |
|-------------|-----------------------|--------|----------------|
| 0           | 0.7                   | 5      | 0.85           |
| 1           | 0.7                   | 1      | 0.54           |
| 2           | 1.0                   | 13     | 0.69           |
| 3           | 0.7                   | 1      | 0.85           |
| 4           | 0.9                   | 5      | 0.77           |
| 5           | 0.8                   | 11     | 0.77           | -->


# Cross-Validation + Test Set

## Idea

<div style="text-align:center;">
  <img src="images/grid_search_cross_validation_new.png" alt="Grid search with CV and test set" style="max-width:105%;">
</div>

# Grid-Search with CV (Code)

```{python}
#| echo: true
from sklearn.model_selection import cross_val_score

X_train, X_test, y_train, y_test = train_test_split(X, y)

cross_val_scores = []
for i in neighbors:
    knn = KNeighborsClassifier(n_neighbors=i)
    scores = cross_val_score(knn, X_train, y_train, cv=10)
    cross_val_scores.append(np.mean(scores))

best_n = neighbors[np.argmax(cross_val_scores)]
knn = KNeighborsClassifier(n_neighbors=best_n)
knn.fit(X_train, y_train)
print(f"test-set score: {knn.score(X_test, y_test):.3f}")
```

# Grid-Search Workflow

<div style="text-align:center;">
  <img src="images/gridsearch_workflow.png" alt="Grid search workflow" style="max-width:80%;">
</div>

# GridSearchCV (Code)

```{python}
#| echo: true
from sklearn.model_selection import GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)
param_grid = {'n_neighbors':  np.arange(1, 30, 2)}

grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=10,
                    return_train_score=True)

grid.fit(X_train, y_train)
print("best parameters:", grid.best_params_)
print(f"test-set score: {grid.score(X_test, y_test):.3f}")
```

# GridSearchCV Results

```{python}
#| echo: true
import pandas as pd
results = pd.DataFrame(grid.cv_results_)
results.params.head()
```

## n_neighbors Search Results

<div style="text-align:center;">
  <img src="images/grid_search_n_neighbors.png" alt="Grid search n_neighbors" style="max-width:70%;">
</div>

## How many folds in CV?

- Recommend to run 5-fold (*default in sklearn*) or 10-fold CV
- #folds represents a trade-off between computing time and performance estimate quality
- More folds → more training data per fold → better generlisation performance estimate, but longer computing time
- Computing time is key for large datasets and complex models.
- Advice: start with train model without CV, get computing time T, then estimate CV time as approximately ```T × #folds```.

# Should data be shuffled in CV?

- In sklearn, ```KFold``` does not shuffle data and is subject to data ordering
- Recommend to set ```shuffle=True``` in ```KFold```, or using ```ShuffleSplit``` function

<div style="text-align:center;">
  <img src="images/shuffle_split_cv.png" alt="Shuffle split" style="max-width:100%;">
</div>

# CV extensions

- **StratifiedKFold**: for multiclass classification or imbalanced data
- **GroupKFold**: for grouped data
- **TimeSeriesSplit**: for time series data

## Stratified CV

- Standard CV leads to folds with different class distributions

<div style="text-align:center;">
  <img src="images/kfold_cv.png" alt="KFold CV" style="max-width:70%;">
</div>

## Stratified CV: for multiclass classification or imbalanced data

- Preserve class distribution in each fold

<div style="text-align:center;">
  <img src="images/stratified_cv.png" alt="Stratified CV" style="max-width:70%;">
</div>


# Stratified CV (Code)

```python
#| echo: true
dc = DummyClassifier('most_frequent')
skf = StratifiedKFold(n_splits=5, shuffle=True)
np.mean(cross_val_score(dc, X, y, cv=skf))
```

## CV for grouped data?

- CV is more complicated when data are grouped
- Data points within a group are correlated (e.g., city, patient, user)
- e.g. The task is to if a patient has a disease based on medical records from 9 cities 
- How CV should be done depends on the application scenario

## Scenario 1: To predict new data from any existing cities (same distribution as training data)

- Standard CV (e.g. KFold, RepeatedKFold) can be used
- Group information can be ignored

## Scenario 2: To predict new data from unknown cities (not i.i.d.)

- GroupKFold should be used; ensure each group is contained in exactly one fold (either train or test)
- Data from 9 cities; 5-fold CV; GroupKFold as below.

<div style="text-align:center;">
  <img src="images/group_kfold.png" alt="Group KFold" style="max-width:60%;">
</div>

# CV for time series?

- Data is collected over time and is not i.i.d.; future data depends on past data

<div style="text-align:center;">
  <img src="images/time_series1.png" alt="Time series 1" style="max-width:60%;">
</div>

## Standard train-test split or CV not suitable for time series

<div style="text-align:center;">
  <img src="images/time_series2.png" alt="Time series 2" style="max-width:70%;">
</div>

## Train-Test Split for Time Series

- Using past data to train, future data to test

<div style="text-align:center;">
  <img src="images/time_series3.png" alt="Time series 3" style="max-width:65%;">
</div>

## TimeSeriesSplit

<div style="text-align:center;">
  <img src="images/time_series_walk_forward_cv.png" alt="Walk-forward CV" style="max-width:80%;">
</div>

## Time Series CV

<div style="text-align:center;">
  <img src="images/time_series_cv.png" alt="Time series CV" style="max-width:80%;">
</div>

<!-- # Using CV Generators (Code)

```python
from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, RepeatedStratifiedKFold
kfold = KFold(n_splits=5)
skfold = StratifiedKFold(n_splits=5, shuffle=True)
ss = ShuffleSplit(n_splits=20, train_size=.4, test_size=.3)
rs = RepeatedStratifiedKFold(n_splits=5, n_repeats=10)

print(cross_val_score(KNeighborsClassifier(), X, y, cv=kfold))
print(cross_val_score(KNeighborsClassifier(), X, y, cv=skfold))
print(cross_val_score(KNeighborsClassifier(), X, y, cv=ss))
print(cross_val_score(KNeighborsClassifier(), X, y, cv=rs))
```

# cross_validate (Code)

```python
from sklearn.model_selection import cross_validate
res = cross_validate(KNeighborsClassifier(), X, y, return_train_score=True,
                     scoring=["accuracy", "roc_auc"])
res_df = pd.DataFrame(res)
res_df.head()
``` -->

## Overview

We've covered:

- Train-test split, threefold split (not recommended)
- **Cross-validation and test set** is recommended
- In cross-validation, recommended to use 5-fold or 10-fold CV with shuffling
- Stratified CV (StratifiedKFold) for multiple classes and imbalanced data
- CV for grouped data and time series data

## Questions?
