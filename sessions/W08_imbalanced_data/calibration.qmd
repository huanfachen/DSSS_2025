---
title: "Calibration"
# subtitle: ""
author: 
  - name: "Huanfa Chen"
email: "huanfa.chen@ucl.ac.uk"
date-as-string: "13/12/2025"
from: markdown+emoji
format:
  revealjs:
    transition: none
    slide-number: TRUE
    preview-links: auto
---

## Calibration
- Probabilities can be more informative than labels
- Example: "no cancer" vs "40% likely to have cancer"
- Goal: accurate probability estimates from any classifier

## Calibration Curve (Reliability Diagram)
:::: columns
::: {.column width="50%"}
<div style="text-align:center;">
  <img src="images/prob_table.png" style="max-width:100%;">
</div>
:::
::: {.column width="50%"}
<div style="text-align:center;">
  <img src="images/calib_curve.png" style="max-width:100%;">
</div>
:::
::::
- For binary classification
- A model can be calibrated yet inaccurate
- Bin predictions, plot fraction positive per bin
- Does not require ground-truth probabilities

## calibration_curve with sklearn
:::: columns
::: {.column width="50%"}
- Using subsample of covertype dataset
- LogisticRegressionCV baseline
```python
lr = LogisticRegressionCV().fit(X_train, y_train)
probs = lr.predict_proba(X_test)[:, 1]
prob_true, prob_pred = calibration_curve(y_test, probs, n_bins=5)
```
:::
::: {.column width="50%"}
<div style="text-align:center;">
  <img src="images/predprob_positive.png" style="max-width:70%;">
</div>
:::
::::

## Influence of Number of Bins
<div style="text-align:center;">
  <img src="images/influence_bins.png" style="max-width:100%;">
</div>
- More bins give resolution but add noise
- Large datasets tolerate more bins; small datasets get noisy

## Comparing Models
<div style="text-align:center;">
  <img src="images/calib_curve_models.png" style="max-width:90%;">
</div>
- Logistic regression well calibrated
- Trees often overconfident; forests underconfident

## Brier Score (Binary)
- Mean squared error of probability estimate
$$BS = \frac{\sum_{i=1}^{n} (\hat{p}(y_i)-y_i)^2}{n}$$
<div style="text-align:center;">
  <img src="images/models_bscore.png" style="max-width:70%;">
</div>
- Smaller is better; mixes accuracy and calibration

## Fixing It: Calibrating a Classifier
- Learn mapping from model scores to better probabilities
- Usually 1d model; works even without native probabilities
- Train calibration on hold-out or via cross-validation

## Platt Scaling
- Use logistic sigmoid as calibration function
- Works well for SVMs
$$f_{platt} = \frac{1}{1 + \exp(-w s(x) - b)}$$

## Isotonic Regression
- Flexible monotone step-function calibrator
- Minimizes MSE under monotonicity
<div style="text-align:center;">
  <img src="images/isotonic_regression.png" style="max-width:40%;">
</div>

## Building the Model
- Using the training set for calibration overfits
- Use hold-out set or cross-validation for unbiased probabilities

## Fitting the Calibration Model
<div style="text-align:center;">
  <img src="images/calibration_val_scores.png" style="max-width:70%;">
</div>

## Fitting the Calibration Model (Fitted Curves)
<div style="text-align:center;">
  <img src="images/calibration_val_scores_fitted.png" style="max-width:70%;">
</div>

## CalibratedClassifierCV
:::: columns
::: {.column width="50%"}
- Calibrate a pretrained model on validation data
```python
rf = RandomForestClassifier().fit(X_train_sub, y_train_sub)
scores = rf.predict_proba(X_test)[:, 1]
plot_calibration_curve(y_test, scores, n_bins=20)
```
:::
::: {.column width="50%"}
<div style="text-align:center;">
  <img src="images/random_forest.png" style="max-width:30%;">
</div>
:::
::::

## Calibration on Random Forest
:::: columns
::: {.column width="50%"}
```python
cal_rf = CalibratedClassifierCV(rf, cv="prefit", method="sigmoid")
cal_rf.fit(X_val, y_val)
scores_sigm = cal_rf.predict_proba(X_test)[:, 1]

cal_rf_iso = CalibratedClassifierCV(rf, cv="prefit", method="isotonic")
cal_rf_iso.fit(X_val, y_val)
scores_iso = cal_rf_iso.predict_proba(X_test)[:, 1]
```
:::
::: {.column width="50%"}
<div style="text-align:center;">
  <img src="images/types_calib.png" style="max-width:90%;">
</div>
:::
::::
- Hold-out calibration with sigmoid or isotonic

## Cross-Validated Calibration
:::: columns
::: {.column width="50%"}
```python
cal_rf_iso_cv = CalibratedClassifierCV(rf, method="isotonic")
cal_rf_iso_cv.fit(X_train, y_train)
scores_iso_cv = cal_rf_iso_cv.predict_proba(X_test)[:, 1]
```
:::
::: {.column width="50%"}
<div style="text-align:center;">
  <img src="images/types_calib_cv.png" style="max-width:90%;">
</div>
:::
::::
- Uses CV folds; averages calibrated models; more trees, better calibration

## Multi-Class Calibration
<div style="text-align:center;">
  <img src="images/multi_class_calibration.png" style="max-width:100%;">
</div>
- Calibrate per class then renormalize

<!-- ## Recap on Imbalanced Data
- Classification often has asymmetric costs or data imbalance
- Need metrics and models that respect imbalance

## Two Sources of Imbalance
- Asymmetric cost between errors
- Asymmetric data prevalence

## Why Do We Care?
- Real-world costs rarely symmetric
- Data often heavily imbalanced; rare event detection common

## Changing Thresholds
- Adjust probability threshold to trade precision/recall
```python
y_pred = lr.predict_proba(X_test)[:, 1] > 0.85
classification_report(y_test, y_pred)
```
- Choose threshold to minimize given cost

## ROC Curve
<div style="text-align:center;">
  <img src="images/roc_svc_rf_curve.png" style="max-width:85%;">
</div>
- Evaluates all thresholds via TPR vs FPR

## Remedies for the Model
- Beyond thresholding: modify data or training to address imbalance

## Mammography Data
:::: columns
::: {.column width="50%"}
```python
data = fetch_openml("mammography", as_frame=True)
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(
    X, y == "1", random_state=0)
```
:::
::: {.column width="50%"}
<div style="text-align:center;">
  <img src="images/mammography_data.png" style="max-width:100%;">
</div>
:::
::::
- Imbalanced dataset: 260 positive of 11183 samples

## Mammography Baselines
- LogisticRegression CV=10: ROC AUC 0.920, AP 0.630
- RandomForest CV=10: ROC AUC 0.939, AP 0.722

## Basic Approaches
:::: columns
::: {.column width="50%"}
<div style="text-align:center;">
  <img src="images/basic_approaches.png" style="max-width:100%;">
</div>
:::
::: {.column width="50%"}
- Change the training procedure
- Modify data via sampling
:::
::::

## Scikit-learn vs Resampling
<div style="text-align:center;">
  <img src="images/pipeline.png" style="max-width:55%;">
</div>
- Standard pipelines transform X only; cannot resample y without extensions

## Imbalance-Learn
- Library: http://imbalanced-learn.org
- `pip install -U imbalanced-learn`
- Extends sklearn API with samplers and pipelines

## Sampler API
- `data_resampled, targets_resampled = sampler.sample(X, y)`
- `fit_sample` convenience to fit and sample
- In pipelines, sampling only occurs during `fit`
- Many samplers are binary-only; check multiclass support

## Random Undersampling
- Drop majority samples until balanced
- Very fast; dataset shrinks to ~2x minority
- Loses data but can still perform well
```python
rus = RandomUnderSampler(replacement=False)
X_sub, y_sub = rus.fit_sample(X_train, y_train)
```

## Random Undersampling Results
- LogisticRegression: ROC AUC 0.927, AP 0.527 (baseline 0.920, 0.630)
- RandomForest: ROC AUC 0.951, AP 0.629 (baseline 0.939, 0.722)
- Often as accurate with fraction of data; great for large datasets

## Random Oversampling
- Repeat minority samples until balanced
- Dataset grows; slower training
```python
ros = RandomOverSampler()
X_over, y_over = ros.fit_sample(X_train, y_train)
```

## Random Oversampling Results
- LogisticRegression: ROC AUC 0.917, AP 0.585
- RandomForest: ROC AUC 0.926, AP 0.715
- Performance similar to baseline; heavier compute

## Curves for LogReg
<div style="text-align:center;">
  <img src="images/curves_logreg.png" style="max-width:100%;">
</div>

## Curves for Random Forest
<div style="text-align:center;">
  <img src="images/curves_rf.png" style="max-width:100%;">
</div>

## Class-Weights
- Reweight loss instead of resampling
- Same effect as oversampling without data duplication
- Supported by most models

## Class-Weights in Linear Models
- Modify loss with per-class weight $c_{y_i}$
- Equivalent to repeating samples by class weight count

## Class-Weights in Trees
- Apply class weights in impurity (Gini or entropy)
- Use weighted votes for prediction

## Using Class-Weights
- LogisticRegression(class_weight="balanced"): ROC AUC 0.918, AP 0.587
- RandomForest(class_weight="balanced"): ROC AUC 0.917, AP 0.701

## Ensemble Resampling
- Random resampling separately per estimator in ensemble
- Example: Balanced bagging or balanced random forest
- Easy with imblearn; not yet in sklearn core

## Easy Ensemble with imblearn
:::: columns
::: {.column width="50%"}
```python
from imblearn.ensemble import BalancedBaggingClassifier
base = DecisionTreeClassifier(max_features="auto")
resampled_rf = BalancedBaggingClassifier(base_estimator=base, random_state=0)
```
:::
::: {.column width="50%"}
- Trains each tree on a different undersampled dataset
- ROC AUC 0.957, AP 0.654 (baseline RF 0.939, 0.722)
:::
::::
- As cheap as undersampling; strong results

## ROC vs PR Comparison
<div style="text-align:center;">
  <img src="images/roc_vs_pr.png" style="max-width:100%;">
</div>
- Easy ensemble performs well at higher recall and precision regions

## Synthetic Sample Generation
- SMOTE: Synthetic Minority Oversampling Technique
- Interview-friendly method; many variants exist

## SMOTE
- Add synthetic points for minority class
- For each minority sample: pick random neighbor, interpolate on line segment
- Leads to larger datasets; can combine with undersampling

## SMOTE Illustration
<div style="text-align:center;">
  <img src="images/smote_mammography.png" style="max-width:100%;">
</div>

## SMOTE Results
- LogisticRegression with SMOTE: ROC AUC 0.919, AP 0.585
- RandomForest with SMOTE: ROC AUC 0.946, AP 0.688
- Similar to baseline; tune k_neighbors for best AP

## SMOTE Tuning
<div style="text-align:center;">
  <img src="images/param_smote_k_neighbors.png" style="max-width:60%;">
</div>
- GridSearch over `smote__k_neighbors`; moderate impact on metrics

## SMOTE Curves
<div style="text-align:center;">
  <img src="images/smote_k_neighbors.png" style="max-width:100%;">
</div>

## ROC vs PR with SMOTE
<div style="text-align:center;">
  <img src="images/roc_vs_pr_smote.png" style="max-width:100%;">
</div>

## Summary
- Inspect both ROC AUC and average precision; review curves
- Undersampling is fast and can help
- Undersampling plus ensembles is powerful
- SMOTE adds synthetic samples; results vary by metric -->
